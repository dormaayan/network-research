{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV, StratifiedKFold, \\\n",
    "    cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \\\n",
    "    mean_absolute_error, make_scorer, brier_score_loss, roc_curve\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "\n",
    "__author__ = \"Dor Ma'ayan\"\n",
    "__email__ = \"grano@ifi.uzh.ch\"\n",
    "__license__ = \"MIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"complete-frame.csv\"\n",
    "CSV_MINER_PATH = \"testminereffectiveness.csv\"\n",
    "DATA_DIR = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_rename (row):\n",
    "    return row['path_test'].split('/')[len(row['path_test'].split('/')) - 1].split('.')[0]\n",
    "\n",
    "def load_frame():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['project', 'module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','NÂº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "def load_quartile(frame):\n",
    "    low, high = frame.mutation.quantile([0.25,0.75])\n",
    "    frame_low = frame.query('mutation<{low}'.format(low=low))\n",
    "    frame_high = frame.query('mutation>{high}'.format(high=high))\n",
    "    frame_low['mutation'] = 0\n",
    "    frame_high['mutation'] = 1\n",
    "    frame = pd.concat([frame_low, frame_high], ignore_index=True)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    return frame;\n",
    "\n",
    "\n",
    "def load_all_data(frame):\n",
    "    columns = [frame.no_mutations, frame.line_coverage, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "    \n",
    "\n",
    "def load_all_data_with_mine(frame):\n",
    "    columns = [frame.no_mutations, frame.line_coverage, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability,frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "def load_all_data_static(frame):\n",
    "    columns = [frame.no_mutations, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "    \n",
    "\n",
    "def load_all_data_with_mine_static(frame):\n",
    "    columns = [frame.no_mutations, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability,frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "def get_scoring():\n",
    "    \"\"\"Returns the scores to evaluate the model\"\"\"\n",
    "    return dict(accuracy=make_scorer(accuracy_score),\n",
    "                precision=make_scorer(precision_score),\n",
    "                recall=make_scorer(recall_score),\n",
    "                f1_score=make_scorer(f1_score),\n",
    "                roc_auc_scorer=make_scorer(roc_auc_score),\n",
    "                mean_absolute_error=make_scorer(mean_absolute_error),\n",
    "                brier_score=make_scorer(brier_score_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_frame(consider_coverage, my_data):\n",
    "    frame = load_frame()\n",
    "    frame = load_quartile(frame)\n",
    "    if consider_coverage and my_data:\n",
    "        return load_all_data_with_mine(frame)\n",
    "    if not consider_coverage and my_data:\n",
    "        return load_all_data_with_mine_static(frame)\n",
    "    if consider_coverage and not my_data:\n",
    "        return load_all_data(frame)\n",
    "    else:\n",
    "        return load_all_data_static(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', activation='linear', init_mode='uniform', dropout_rate=0.1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(83,)))\n",
    "    model.add(keras.layers.Dense(40, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(20, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(2, kernel_initializer=init_mode, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data\n",
      "Import: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performances:\n",
      "Accuracy\t 0.930\n",
      "Precision\t 0.914\n",
      "Recall\t 0.950\n",
      "F1 Score\t 0.931\n",
      "ROC AUC\t 0.930\n",
      "MAE\t 0.070\n",
      "Brier Score\t 0.070\n",
      "\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is:\n",
      "neural\n",
      "Saving the model on the entire set\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/Dor/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/backend/queues.py\", line 150, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/Users/Dor/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/backend/reduction.py\", line 247, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/Users/Dor/anaconda3/lib/python3.6/site-packages/joblib/externals/loky/backend/reduction.py\", line 240, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/Users/Dor/anaconda3/lib/python3.6/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 482, in dump\n    return Pickler.dump(self, obj)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 409, in dump\n    self.save(obj)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 852, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 781, in save_list\n    self._batch_appends(obj)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 808, in _batch_appends\n    save(tmp[0])\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 736, in save_tuple\n    save(element)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 852, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 781, in save_list\n    self._batch_appends(obj)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 808, in _batch_appends\n    save(tmp[0])\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 521, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 634, in save_reduce\n    save(state)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 476, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 821, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n    save(v)\n  File \"/Users/Dor/anaconda3/lib/python3.6/pickle.py\", line 496, in save\n    rv = reduce(self.proto)\nTypeError: can't pickle _thread._local objects\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f9944c257390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-f9944c257390>\u001b[0m in \u001b[0;36mclassification\u001b[0;34m(consider_coverage, my_data, n_inner, n_outer)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving the model on the entire set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}/model_{}_{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoverage_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "def classification(consider_coverage=True, my_data=True, n_inner=5, n_outer=10):\n",
    "    \"\"\"\n",
    "    Runs the entire process of classification and evaluation\n",
    "    :param consider_coverage: to include or not the line coverage as a feature\n",
    "    :param n_inner: number of folds for the inner cross fold validation\n",
    "    :param n_outer: number of folds for the outer cross fold validation\n",
    "    :param algorithm: select the algorithm to run; possible choices are 'svc', 'rfc', 'knn' and 'all'\n",
    "    Validate and save a ML model\n",
    "    \"\"\"\n",
    "    global data_x, data_y, coverage_suffix\n",
    "\n",
    "    # the suffix for saving the files\n",
    "    coverage_suffix = 'dynamic' if consider_coverage else 'static'\n",
    "    algorithm  = 'my_data' if my_data else ''\n",
    "\n",
    "    # Import the data\n",
    "    print('Importing data')\n",
    "    \n",
    "    data_x, data_y, number_of_features = import_frame(consider_coverage, my_data)\n",
    "    \n",
    "    data_x = data_x.values\n",
    "    data_y = data_y.values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_x)\n",
    "    data_x = scaler.transform(data_x)\n",
    "    \n",
    "    \n",
    "    #frame, metrics = import_frame(consider_coverage)\n",
    "    print('Import: DONE')\n",
    "\n",
    "    #X = frame[metrics]\n",
    "    #Y = frame['y']\n",
    "    #print('Running with {} metrics'.format(metrics))\n",
    "    pipe = Pipeline([('preprocessing', StandardScaler()),\n",
    "                     ('classifier', KerasClassifier(build_fn=create_model, verbose=0, epochs=2000))])\n",
    "\n",
    "    # Set up the algorithms to tune, train and evaluate\n",
    "    #param_grid = get_param_grid(algorithm, metrics)\n",
    "    \n",
    "    # define the grid search parameters\n",
    "    batch_size = [10] #, 20, 40, 60, 80, 100]\n",
    "    activation = ['relu']\n",
    "    optimizer = ['Adam']\n",
    "    #activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "    #optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "    #weight_constraint = [1, 2, 3, 4, 5]\n",
    "    dropout_rate = [0.25]\n",
    "    #dropout_rate = [0.2, 0.25, 0.3]\n",
    "    param_grid = dict(batch_size=batch_size, optimizer=optimizer, activation=activation, dropout_rate=dropout_rate)\n",
    "\n",
    "    inner_cv = StratifiedKFold(n_splits=n_inner, shuffle=True)\n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_outer, n_repeats=1)\n",
    "\n",
    "    model = KerasClassifier(build_fn=create_model, verbose=0, epochs=50)\n",
    "\n",
    "    early_stopping_monitor = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0003, patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "    # inner cross validation\n",
    "    grid = GridSearchCV(estimator=model, #instead of pipe\n",
    "                        param_grid=param_grid,\n",
    "                        cv=inner_cv,\n",
    "                        scoring=get_scoring(),\n",
    "                        refit='roc_auc_scorer',\n",
    "                        return_train_score=True,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    results = cross_validate(estimator=grid,\n",
    "                             cv=outer_cv,\n",
    "                             X=data_x,\n",
    "                             y=data_y,\n",
    "                             scoring=get_scoring(),\n",
    "                             return_train_score=True,\n",
    "                             verbose=1,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "    accuracy = results.get('test_accuracy').mean()\n",
    "    precision = results.get('test_precision').mean()\n",
    "    recall = results.get('test_recall').mean()\n",
    "    f1_score = results.get('test_f1_score').mean()\n",
    "    roc_auc = results.get('test_roc_auc_scorer').mean()\n",
    "    mae = results.get('test_mean_absolute_error').mean()\n",
    "    brier = results.get('test_brier_score').mean()\n",
    "\n",
    "    print('Performances:\\n'\n",
    "          'Accuracy\\t {:.3f}\\n'\n",
    "          'Precision\\t {:.3f}\\n'\n",
    "          'Recall\\t {:.3f}\\n'\n",
    "          'F1 Score\\t {:.3f}\\n'\n",
    "          'ROC AUC\\t {:.3f}\\n'\n",
    "          'MAE\\t {:.3f}\\n'\n",
    "          'Brier Score\\t {:.3f}\\n'.format(accuracy, precision, recall, f1_score, roc_auc, mae, brier))\n",
    "\n",
    "    # save performance metrics\n",
    "    metrics_res = pd.DataFrame({'accuracy': [accuracy],\n",
    "                                'precision': [precision],\n",
    "                                'recall': [recall],\n",
    "                                'f1_score': [f1_score],\n",
    "                                'ROC-AUC': [roc_auc],\n",
    "                                'MAE': [mae],\n",
    "                                'Brier': [brier]})\n",
    "\n",
    "    metrics_res.to_csv('{}/evaluation_{}_{}.csv'.format(DATA_DIR, coverage_suffix, algorithm), index=False)\n",
    "\n",
    "    grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "    model = 'neural' #grid.best_params_['classifier']\n",
    "    print('Best model is:\\n{}'.format(model))\n",
    "    model_string = open('{}/_model_{}_{}.txt'.format(DATA_DIR, coverage_suffix, algorithm), 'w')\n",
    "    model_string.write(str(model))\n",
    "    model_string.close()\n",
    "\n",
    "    print('Saving the model on the entire set')\n",
    "    grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "    joblib.dump(grid.best_estimator_, '{}/model_{}_{}.pkl'.format(DATA_DIR, coverage_suffix, algorithm), compress=1)\n",
    "\n",
    "\n",
    "    \n",
    "classification(n_inner=2, n_outer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', activation='linear', init_mode='uniform', dropout_rate=0.1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(82,)))\n",
    "    model.add(keras.layers.Dense(40, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(20, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(2, kernel_initializer=init_mode, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load dataset\n",
    "frame = load_frame()\n",
    "frame = load_quartile(frame)\n",
    "\n",
    "data_x, data_y, number_of_features = load_all_data_with_mine_static(frame)\n",
    "\n",
    "data_x = data_x.values\n",
    "data_y = data_y.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0003, patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=2000)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10] #, 20, 40, 60, 80, 100]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "#weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.2, 0.25, 0.3]\n",
    "param_grid = dict(batch_size=batch_size, optimizer=optimizer, activation=activation, dropout_rate=dropout_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=kfold, verbose=2)\n",
    "grid_result = grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
