{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import sklearn\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"complete-frame.csv\"\n",
    "CSV_MINER_PATH = \"testminereffectiveness.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_rename (row):\n",
    "    return row['path_test'].split('/')[len(row['path_test'].split('/')) - 1].split('.')[0]\n",
    "\n",
    "def load_frame():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['project', 'module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','Nº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "def load_frame_with_projects():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','Nº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def load_static_frame():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['project', 'module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','Nº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def load_quartile(frame):\n",
    "    low, high = frame.mutation.quantile([0.25,0.75])\n",
    "    frame_low = frame.query('mutation<{low}'.format(low=low))\n",
    "    frame_high = frame.query('mutation>{high}'.format(high=high))\n",
    "    frame_low['mutation'] = 0\n",
    "    frame_high['mutation'] = 1\n",
    "    frame = pd.concat([frame_low, frame_high], ignore_index=True)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    return frame;\n",
    "\n",
    "def load_meaningful_subset(frame):\n",
    "    columns = [frame.no_mutations,\n",
    "                         frame.line_coverage,\n",
    "                         frame.csm_FE,\n",
    "                         frame.CONNECTIVITY_prod,\n",
    "                         frame.CONNECTIVITY_test,\n",
    "                         frame.isEagerTest,\n",
    "                         frame.LOC_prod, frame.LOC_test, frame.WMC_prod,\n",
    "                         frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "                         frame.LCOM4_prod, frame.McCABE_prod,\n",
    "                         frame.RFC_prod, frame.MPC_prod,\n",
    "                         frame.RFC_test, frame.MPC_test,\n",
    "                         frame.LCOM1_test, frame.LCOM2_test,\n",
    "                         frame.LCOM4_test, frame.LCC_test,\n",
    "                         frame.LCC_test, frame.WMC_test,\n",
    "                         frame.McCABE_test, frame.NOP_prod]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "def load_meaningful_subset_static(frame):\n",
    "    columns = [frame.no_mutations,\n",
    "                         frame.csm_FE,\n",
    "                         frame.CONNECTIVITY_prod,\n",
    "                         frame.CONNECTIVITY_test,\n",
    "                         frame.isEagerTest,\n",
    "                         frame.LOC_prod, frame.LOC_test, frame.WMC_prod,\n",
    "                         frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "                         frame.LCOM4_prod, frame.McCABE_prod,\n",
    "                         frame.RFC_prod, frame.MPC_prod,\n",
    "                         frame.RFC_test, frame.MPC_test,\n",
    "                         frame.LCOM1_test, frame.LCOM2_test,\n",
    "                         frame.LCOM4_test, frame.LCC_test,\n",
    "                         frame.LCC_test, frame.WMC_test,\n",
    "                         frame.McCABE_test, frame.NOP_prod]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "def load_meaningful_subset_2(frame):\n",
    "    #columns = [frame.line_coverage, frame.isAssertionRoulette, frame.isMysteryGuest,\n",
    "    #   frame.isResourceOptimism, frame.isForTestersOnly, frame.COH_prod, frame.BUSWEIMER_prod,\n",
    "    #   frame.BUSWEIMER_test, frame.csm_LM, frame.prod_readability]\n",
    "    \n",
    "    [frame.line_coverage,\n",
    "    frame.COH_prod, frame.BUSWEIMER_prod, frame.csm_MC,\n",
    "       frame.prod_readability, frame.prod_readability]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_all_data(frame):\n",
    "    columns = [frame.no_mutations, frame.line_coverage, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "    \n",
    "\n",
    "def load_all_data_with_mine(frame):\n",
    "    columns = [frame.no_mutations, frame.line_coverage, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability,frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "def load_all_data_static(frame):\n",
    "    columns = [frame.no_mutations, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "    \n",
    "\n",
    "def load_all_data_with_mine_static(frame):\n",
    "    columns = [frame.no_mutations, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability,frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "def load_mine(frame):\n",
    "    columns = [frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters, frame.mutation]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid - Static with all their data + my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1008 candidates, totalling 10080 fits\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   5.6s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.4s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.4s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.9s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   5.0s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.4s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   7.3s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   5.0s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.9s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.9s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=RMSprop \n",
      "WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e9759df08b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 73\u001b[0;31m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2132\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    168\u001b[0m               \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m               reduction=losses_utils.ReductionV2.NONE)\n\u001b[0m\u001b[1;32m    171\u001b[0m           \u001b[0mloss_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     weighted_losses = tf_losses_utils.scale_losses_by_sample_weight(\n\u001b[0;32m--> 107\u001b[0;31m         losses, sample_weight)\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/util.py\u001b[0m in \u001b[0;36mscale_losses_by_sample_weight\u001b[0;34m(losses, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;31m# Broadcast weights if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_broadcast_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6699\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6701\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6702\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6703\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1773\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1774\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', activation='linear', init_mode='uniform', dropout_rate=0.1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(82,)))\n",
    "    model.add(keras.layers.Dense(40, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(20, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(2, kernel_initializer=init_mode, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load dataset\n",
    "frame = load_frame()\n",
    "frame = load_quartile(frame)\n",
    "\n",
    "data_x, data_y, number_of_features = load_all_data_with_mine_static(frame)\n",
    "\n",
    "data_x = data_x.values\n",
    "data_y = data_y.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0003, patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=2000)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "dropout_rate = [0.2, 0.25, 0.3]\n",
    "param_grid = dict(batch_size=batch_size, optimizer=optimizer, activation=activation, dropout_rate=dropout_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=kfold, verbose=2)\n",
    "grid_result = grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid - predict the exact mutation score value (Dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', activation='linear', init_mode='uniform', dropout_rate=0.1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(82,)))\n",
    "    model.add(keras.layers.Dense(40, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(20, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(2, kernel_initializer=init_mode, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load dataset\n",
    "frame = load_frame()\n",
    "frame = load_quartile(frame)\n",
    "\n",
    "data_x, data_y, number_of_features = load_all_data_with_mine(frame)\n",
    "\n",
    "data_x = data_x.values\n",
    "data_y = data_y.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0003, patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=2000)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "dropout_rate = [0.2, 0.25, 0.3]\n",
    "param_grid = dict(batch_size=batch_size, optimizer=optimizer, activation=activation, dropout_rate=dropout_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=kfold, verbose=2)\n",
    "grid_result = grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>isAssertionRoulette</th>\n",
       "      <th>isEagerTest</th>\n",
       "      <th>isLazyTest</th>\n",
       "      <th>isMysteryGuest</th>\n",
       "      <th>isSensitiveEquality</th>\n",
       "      <th>isResourceOptimism</th>\n",
       "      <th>isForTestersOnly</th>\n",
       "      <th>...</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960591</td>\n",
       "      <td>203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>11284.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>4113.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>69372.0</td>\n",
       "      <td>6332.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453172</td>\n",
       "      <td>331</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4461.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>70</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.296651</td>\n",
       "      <td>209</td>\n",
       "      <td>0.424779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3310.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>0.179487</td>\n",
       "      <td>117</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1779.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3397.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>0.967742</td>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6472.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6046.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>0.602273</td>\n",
       "      <td>88</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>7641.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>0.163793</td>\n",
       "      <td>116</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>6872.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5088.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutation  no_mutations  line_coverage  isAssertionRoulette  isEagerTest  \\\n",
       "0     0.960591           203       1.000000                    1            1   \n",
       "1     0.666667             6       1.000000                    1            0   \n",
       "2     0.453172           331       0.560606                    1            1   \n",
       "3     0.700000            70       0.848485                    1            1   \n",
       "4     0.296651           209       0.424779                    1            1   \n",
       "...        ...           ...            ...                  ...          ...   \n",
       "2240  0.179487           117       0.316667                    1            1   \n",
       "2241  1.000000             3       1.000000                    1            0   \n",
       "2242  0.967742            31       1.000000                    1            1   \n",
       "2243  0.602273            88       1.000000                    1            1   \n",
       "2244  0.163793           116       0.189189                    1            1   \n",
       "\n",
       "      isLazyTest  isMysteryGuest  isSensitiveEquality  isResourceOptimism  \\\n",
       "0              0               0                    1                   0   \n",
       "1              0               1                    0                   0   \n",
       "2              0               0                    1                   0   \n",
       "3              0               0                    0                   0   \n",
       "4              0               0                    0                   0   \n",
       "...          ...             ...                  ...                 ...   \n",
       "2240           0               0                    0                   0   \n",
       "2241           0               0                    0                   0   \n",
       "2242           0               0                    0                   0   \n",
       "2243           0               0                    0                   0   \n",
       "2244           0               0                    0                   0   \n",
       "\n",
       "      isForTestersOnly  ...  Mockito  BadApi     LOC  Expressions  Depth  \\\n",
       "0                    0  ...      0.0     0.0   757.0       1316.0    9.0   \n",
       "1                    0  ...      0.0     0.0  1520.0       4113.0   27.0   \n",
       "2                    0  ...      0.0     0.0   287.0        520.0   10.0   \n",
       "3                    0  ...      0.0     1.0   126.0        321.0   15.0   \n",
       "4                    0  ...      0.0    17.0   174.0        361.0    8.0   \n",
       "...                ...  ...      ...     ...     ...          ...    ...   \n",
       "2240                 0  ...      0.0     8.0   100.0        198.0    8.0   \n",
       "2241                 0  ...      0.0     0.0    43.0        154.0   25.0   \n",
       "2242                 0  ...      0.0     0.0   363.0        631.0   13.0   \n",
       "2243                 0  ...      0.0     0.0   338.0        546.0   18.0   \n",
       "2244                 0  ...      0.0    36.0   330.0        661.0    9.0   \n",
       "\n",
       "      Vocabulary  Understandability  BodySize  Dexterity  NonWhiteCharacters  \n",
       "0           81.0            11284.0    1410.0        3.0             12516.0  \n",
       "1          240.0            69372.0    6332.0        2.0             31106.0  \n",
       "2           44.0             4461.0     666.0        3.0              3026.0  \n",
       "3           45.0             3206.0     427.0        2.0              2359.0  \n",
       "4           42.0             3310.0     451.0        3.0              2828.0  \n",
       "...          ...                ...       ...        ...                 ...  \n",
       "2240        25.0             1779.0     254.0        3.0              1591.0  \n",
       "2241        41.0             3397.0     269.0        2.0              1307.0  \n",
       "2242        53.0             6472.0     879.0        5.0              6046.0  \n",
       "2243        52.0             7641.0     838.0        2.0              4274.0  \n",
       "2244        67.0             6872.0     847.0        3.0              5088.0  \n",
       "\n",
       "[2243 rows x 84 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      no_mutations  line_coverage  isAssertionRoulette  isEagerTest  \\\n",
       " 0              176           0.95                    0            0   \n",
       " 1              165           1.00                    0            0   \n",
       " 2              147           0.68                    1            1   \n",
       " 3                6           1.00                    1            0   \n",
       " 4             1187           1.00                    0            0   \n",
       " ...            ...            ...                  ...          ...   \n",
       " 2240           164           0.94                    0            1   \n",
       " 2241           100           0.78                    1            0   \n",
       " 2242          4379           0.09                    1            1   \n",
       " 2243           255           0.86                    0            1   \n",
       " 2244            23           1.00                    1            1   \n",
       " \n",
       "       isLazyTest  isMysteryGuest  isSensitiveEquality  isResourceOptimism  \\\n",
       " 0              0               0                    0                   0   \n",
       " 1              0               0                    0                   0   \n",
       " 2              0               0                    0                   0   \n",
       " 3              0               0                    0                   0   \n",
       " 4              0               0                    0                   0   \n",
       " ...          ...             ...                  ...                 ...   \n",
       " 2240           0               0                    0                   0   \n",
       " 2241           0               0                    1                   0   \n",
       " 2242           0               0                    1                   0   \n",
       " 2243           0               0                    1                   0   \n",
       " 2244           0               0                    0                   0   \n",
       " \n",
       "       isForTestersOnly  isIndirectTesting  ...  Mockito  BadApi    LOC  \\\n",
       " 0                    0                  0  ...      0.0     0.0   79.0   \n",
       " 1                    0                  0  ...      0.0     0.0  198.0   \n",
       " 2                    0                  0  ...      0.0    15.0  127.0   \n",
       " 3                    0                  0  ...      0.0     0.0  199.0   \n",
       " 4                    0                  0  ...      0.0     0.0  464.0   \n",
       " ...                ...                ...  ...      ...     ...    ...   \n",
       " 2240                 0                  0  ...      0.0     0.0  117.0   \n",
       " 2241                 0                  0  ...      0.0     0.0   76.0   \n",
       " 2242                 0                  0  ...      0.0     0.0   65.0   \n",
       " 2243                 0                  0  ...      0.0     0.0   86.0   \n",
       " 2244                 0                  0  ...      0.0     0.0  443.0   \n",
       " \n",
       "       Expressions  Depth  Vocabulary  Understandability  BodySize  Dexterity  \\\n",
       " 0           152.0    9.0        25.0             1353.0     189.0        3.0   \n",
       " 1           440.0    8.0        81.0             5152.0     377.0        4.0   \n",
       " 2           270.0    8.0        37.0             2448.0     337.0        3.0   \n",
       " 3           475.0   18.0        92.0             6044.0     593.0        2.0   \n",
       " 4          1078.0   15.0       136.0            12279.0    1527.0        3.0   \n",
       " ...           ...    ...         ...                ...       ...        ...   \n",
       " 2240        373.0    8.0        19.0             2925.0     400.0        4.0   \n",
       " 2241        214.0   12.0        44.0             1916.0     270.0        2.0   \n",
       " 2242        114.0    8.0        17.0              916.0     142.0        3.0   \n",
       " 2243        226.0    9.0        40.0             2024.0     277.0        2.0   \n",
       " 2244        915.0   20.0       114.0            12422.0    1356.0        2.0   \n",
       " \n",
       "       NonWhiteCharacters  \n",
       " 0                 1600.0  \n",
       " 1                 6105.0  \n",
       " 2                 1945.0  \n",
       " 3                 3702.0  \n",
       " 4                12403.0  \n",
       " ...                  ...  \n",
       " 2240              3032.0  \n",
       " 2241              1698.0  \n",
       " 2242               892.0  \n",
       " 2243              1812.0  \n",
       " 2244              7155.0  \n",
       " \n",
       " [2243 rows x 83 columns],       mutation\n",
       " 0     0.795455\n",
       " 1     0.945455\n",
       " 2     0.394558\n",
       " 3     0.833333\n",
       " 4     0.213142\n",
       " ...        ...\n",
       " 2240  0.902439\n",
       " 2241  0.600000\n",
       " 2242  0.040877\n",
       " 2243  0.654902\n",
       " 2244  1.000000\n",
       " \n",
       " [2243 rows x 1 columns], 83)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_all_data_with_mine(load_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1570, 83)\n",
      "Train on 1570 samples, validate on 336 samples\n",
      "Epoch 1/1000\n",
      "1570/1570 [==============================] - 2s 2ms/sample - loss: 0.1349 - mae: 0.2682 - val_loss: 0.0868 - val_mae: 0.2144\n",
      "Epoch 2/1000\n",
      "1570/1570 [==============================] - 0s 170us/sample - loss: 0.0418 - mae: 0.1597 - val_loss: 0.0814 - val_mae: 0.1732\n",
      "Epoch 3/1000\n",
      "1570/1570 [==============================] - 0s 159us/sample - loss: 0.0283 - mae: 0.1293 - val_loss: 0.0675 - val_mae: 0.1617\n",
      "Epoch 4/1000\n",
      "1570/1570 [==============================] - 0s 146us/sample - loss: 0.0208 - mae: 0.1101 - val_loss: 0.0648 - val_mae: 0.1577\n",
      "Epoch 5/1000\n",
      "1570/1570 [==============================] - 0s 148us/sample - loss: 0.0173 - mae: 0.0992 - val_loss: 0.0550 - val_mae: 0.1491\n",
      "Epoch 6/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0150 - mae: 0.0905 - val_loss: 0.0533 - val_mae: 0.1465\n",
      "Epoch 7/1000\n",
      "1570/1570 [==============================] - 0s 144us/sample - loss: 0.0130 - mae: 0.0839 - val_loss: 0.0574 - val_mae: 0.1484\n",
      "Epoch 8/1000\n",
      "1570/1570 [==============================] - 0s 155us/sample - loss: 0.0122 - mae: 0.0812 - val_loss: 0.0565 - val_mae: 0.1424\n",
      "Epoch 9/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0161 - mae: 0.0846 - val_loss: 0.0431 - val_mae: 0.1445\n",
      "Epoch 10/1000\n",
      "1570/1570 [==============================] - 0s 144us/sample - loss: 0.0116 - mae: 0.0783 - val_loss: 0.0444 - val_mae: 0.1358\n",
      "Epoch 11/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0093 - mae: 0.0710 - val_loss: 0.0442 - val_mae: 0.1364\n",
      "Epoch 12/1000\n",
      "1570/1570 [==============================] - 0s 142us/sample - loss: 0.0086 - mae: 0.0677 - val_loss: 0.0445 - val_mae: 0.1360\n",
      "Epoch 13/1000\n",
      "1570/1570 [==============================] - 0s 138us/sample - loss: 0.0102 - mae: 0.0694 - val_loss: 0.0463 - val_mae: 0.1382\n",
      "Epoch 14/1000\n",
      "1570/1570 [==============================] - 0s 140us/sample - loss: 0.0081 - mae: 0.0646 - val_loss: 0.0451 - val_mae: 0.1378\n",
      "Epoch 15/1000\n",
      "1570/1570 [==============================] - 0s 147us/sample - loss: 0.0066 - mae: 0.0590 - val_loss: 0.0442 - val_mae: 0.1334\n",
      "Epoch 16/1000\n",
      "1570/1570 [==============================] - 0s 147us/sample - loss: 0.0057 - mae: 0.0558 - val_loss: 0.0402 - val_mae: 0.1317\n",
      "Epoch 17/1000\n",
      "1570/1570 [==============================] - 0s 134us/sample - loss: 0.0055 - mae: 0.0534 - val_loss: 0.0474 - val_mae: 0.1344\n",
      "Epoch 18/1000\n",
      "1570/1570 [==============================] - 0s 138us/sample - loss: 0.0055 - mae: 0.0539 - val_loss: 0.0393 - val_mae: 0.1307\n",
      "Epoch 19/1000\n",
      "1570/1570 [==============================] - 0s 135us/sample - loss: 0.0085 - mae: 0.0542 - val_loss: 0.0450 - val_mae: 0.1360\n",
      "Epoch 20/1000\n",
      "1570/1570 [==============================] - 0s 139us/sample - loss: 0.0072 - mae: 0.0575 - val_loss: 0.0404 - val_mae: 0.1372\n",
      "Epoch 21/1000\n",
      "1570/1570 [==============================] - 0s 147us/sample - loss: 0.0060 - mae: 0.0528 - val_loss: 0.0447 - val_mae: 0.1343\n",
      "Epoch 22/1000\n",
      "1570/1570 [==============================] - 0s 147us/sample - loss: 0.0047 - mae: 0.0492 - val_loss: 0.0400 - val_mae: 0.1295\n",
      "Epoch 23/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0044 - mae: 0.0475 - val_loss: 0.0380 - val_mae: 0.1337\n",
      "Epoch 24/1000\n",
      "1570/1570 [==============================] - 0s 140us/sample - loss: 0.0043 - mae: 0.0479 - val_loss: 0.0390 - val_mae: 0.1291\n",
      "Epoch 25/1000\n",
      "1570/1570 [==============================] - 0s 140us/sample - loss: 0.0036 - mae: 0.0433 - val_loss: 0.0401 - val_mae: 0.1296\n",
      "Epoch 26/1000\n",
      "1570/1570 [==============================] - 0s 150us/sample - loss: 0.0037 - mae: 0.0427 - val_loss: 0.0372 - val_mae: 0.1293\n",
      "Epoch 27/1000\n",
      "1570/1570 [==============================] - 0s 139us/sample - loss: 0.0051 - mae: 0.0483 - val_loss: 0.0459 - val_mae: 0.1342\n",
      "Epoch 28/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0035 - mae: 0.0432 - val_loss: 0.0396 - val_mae: 0.1297\n",
      "Epoch 29/1000\n",
      "1570/1570 [==============================] - 0s 155us/sample - loss: 0.0031 - mae: 0.0395 - val_loss: 0.0471 - val_mae: 0.1327\n",
      "Epoch 30/1000\n",
      "1570/1570 [==============================] - 0s 147us/sample - loss: 0.0032 - mae: 0.0382 - val_loss: 0.0403 - val_mae: 0.1308\n",
      "Epoch 31/1000\n",
      "1570/1570 [==============================] - 0s 142us/sample - loss: 0.0037 - mae: 0.0403 - val_loss: 0.0375 - val_mae: 0.1345\n",
      "Epoch 32/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0037 - mae: 0.0412 - val_loss: 0.0398 - val_mae: 0.1335\n",
      "Epoch 33/1000\n",
      "1570/1570 [==============================] - 0s 153us/sample - loss: 0.0031 - mae: 0.0386 - val_loss: 0.0369 - val_mae: 0.1311\n",
      "Epoch 34/1000\n",
      "1570/1570 [==============================] - 0s 152us/sample - loss: 0.0031 - mae: 0.0396 - val_loss: 0.0346 - val_mae: 0.1273\n",
      "Epoch 35/1000\n",
      "1570/1570 [==============================] - 0s 163us/sample - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0420 - val_mae: 0.1343\n",
      "Epoch 36/1000\n",
      "1570/1570 [==============================] - 0s 145us/sample - loss: 0.0041 - mae: 0.0462 - val_loss: 0.0366 - val_mae: 0.1325\n",
      "Epoch 37/1000\n",
      "1570/1570 [==============================] - 0s 142us/sample - loss: 0.0031 - mae: 0.0406 - val_loss: 0.0407 - val_mae: 0.1339\n",
      "Epoch 38/1000\n",
      "1570/1570 [==============================] - 0s 145us/sample - loss: 0.0059 - mae: 0.0523 - val_loss: 0.0473 - val_mae: 0.1381\n",
      "Epoch 39/1000\n",
      "1570/1570 [==============================] - 0s 154us/sample - loss: 0.0068 - mae: 0.0579 - val_loss: 0.0373 - val_mae: 0.1338\n",
      "Epoch 40/1000\n",
      "1570/1570 [==============================] - 0s 173us/sample - loss: 0.0045 - mae: 0.0462 - val_loss: 0.0458 - val_mae: 0.1395\n",
      "Epoch 41/1000\n",
      "1570/1570 [==============================] - 0s 178us/sample - loss: 0.0042 - mae: 0.0464 - val_loss: 0.0354 - val_mae: 0.1296\n",
      "Epoch 42/1000\n",
      "1570/1570 [==============================] - 0s 183us/sample - loss: 0.0035 - mae: 0.0415 - val_loss: 0.0376 - val_mae: 0.1284\n",
      "Epoch 43/1000\n",
      "1570/1570 [==============================] - 0s 154us/sample - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0362 - val_mae: 0.1266\n",
      "Epoch 44/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0025 - mae: 0.0351 - val_loss: 0.0381 - val_mae: 0.1292\n",
      "Epoch 45/1000\n",
      "1570/1570 [==============================] - 0s 160us/sample - loss: 0.0021 - mae: 0.0313 - val_loss: 0.0409 - val_mae: 0.1300\n",
      "Epoch 46/1000\n",
      "1570/1570 [==============================] - 0s 171us/sample - loss: 0.0024 - mae: 0.0329 - val_loss: 0.0444 - val_mae: 0.1320\n",
      "Epoch 47/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0025 - mae: 0.0347 - val_loss: 0.0379 - val_mae: 0.1283\n",
      "Epoch 48/1000\n",
      "1570/1570 [==============================] - 0s 144us/sample - loss: 0.0017 - mae: 0.0292 - val_loss: 0.0402 - val_mae: 0.1271\n",
      "Epoch 49/1000\n",
      "1570/1570 [==============================] - 0s 142us/sample - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0416 - val_mae: 0.1293\n",
      "Epoch 50/1000\n",
      "1570/1570 [==============================] - 0s 145us/sample - loss: 0.0019 - mae: 0.0312 - val_loss: 0.0414 - val_mae: 0.1304\n",
      "Epoch 51/1000\n",
      "1570/1570 [==============================] - 0s 148us/sample - loss: 0.0018 - mae: 0.0291 - val_loss: 0.0429 - val_mae: 0.1299\n",
      "Epoch 52/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 0.0021 - mae: 0.0315 - val_loss: 0.0419 - val_mae: 0.1298\n",
      "Epoch 53/1000\n",
      "1570/1570 [==============================] - 0s 173us/sample - loss: 0.0018 - mae: 0.0297 - val_loss: 0.0381 - val_mae: 0.1276\n",
      "Epoch 54/1000\n",
      "1570/1570 [==============================] - 0s 162us/sample - loss: 0.0017 - mae: 0.0291 - val_loss: 0.0381 - val_mae: 0.1280\n",
      "Epoch 55/1000\n",
      "1570/1570 [==============================] - 0s 160us/sample - loss: 0.0014 - mae: 0.0257 - val_loss: 0.0426 - val_mae: 0.1298\n",
      "Epoch 56/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0021 - mae: 0.0332 - val_loss: 0.0398 - val_mae: 0.1288\n",
      "Epoch 57/1000\n",
      "1570/1570 [==============================] - 0s 145us/sample - loss: 0.0018 - mae: 0.0295 - val_loss: 0.0385 - val_mae: 0.1275\n",
      "Epoch 58/1000\n",
      "1570/1570 [==============================] - 0s 150us/sample - loss: 0.0021 - mae: 0.0299 - val_loss: 0.0382 - val_mae: 0.1295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "1570/1570 [==============================] - 0s 156us/sample - loss: 0.0023 - mae: 0.0306 - val_loss: 0.0413 - val_mae: 0.1284\n",
      "Epoch 60/1000\n",
      "1570/1570 [==============================] - 0s 156us/sample - loss: 0.0021 - mae: 0.0286 - val_loss: 0.0413 - val_mae: 0.1269\n",
      "Epoch 61/1000\n",
      "1570/1570 [==============================] - 0s 152us/sample - loss: 0.0018 - mae: 0.0298 - val_loss: 0.0382 - val_mae: 0.1267\n",
      "Epoch 62/1000\n",
      "1570/1570 [==============================] - 0s 150us/sample - loss: 0.0014 - mae: 0.0271 - val_loss: 0.0421 - val_mae: 0.1290\n",
      "Epoch 63/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0016 - mae: 0.0289 - val_loss: 0.0365 - val_mae: 0.1272\n",
      "Epoch 64/1000\n",
      "1570/1570 [==============================] - 0s 149us/sample - loss: 0.0014 - mae: 0.0262 - val_loss: 0.0416 - val_mae: 0.1258\n",
      "Epoch 65/1000\n",
      "1570/1570 [==============================] - 0s 161us/sample - loss: 0.0010 - mae: 0.0230 - val_loss: 0.0362 - val_mae: 0.1251\n",
      "Epoch 66/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0405 - val_mae: 0.1276\n",
      "Epoch 67/1000\n",
      "1570/1570 [==============================] - 0s 147us/sample - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0344 - val_mae: 0.1252\n",
      "Epoch 68/1000\n",
      "1570/1570 [==============================] - 0s 152us/sample - loss: 0.0015 - mae: 0.0263 - val_loss: 0.0375 - val_mae: 0.1277\n",
      "Epoch 69/1000\n",
      "1570/1570 [==============================] - 0s 153us/sample - loss: 0.0011 - mae: 0.0240 - val_loss: 0.0377 - val_mae: 0.1244\n",
      "Epoch 70/1000\n",
      "1570/1570 [==============================] - 0s 152us/sample - loss: 9.2982e-04 - mae: 0.0217 - val_loss: 0.0386 - val_mae: 0.1243\n",
      "Epoch 71/1000\n",
      "1570/1570 [==============================] - 0s 166us/sample - loss: 7.9402e-04 - mae: 0.0200 - val_loss: 0.0381 - val_mae: 0.1250\n",
      "Epoch 72/1000\n",
      "1570/1570 [==============================] - 0s 166us/sample - loss: 7.8576e-04 - mae: 0.0198 - val_loss: 0.0387 - val_mae: 0.1254\n",
      "Epoch 73/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 8.2911e-04 - mae: 0.0200 - val_loss: 0.0379 - val_mae: 0.1247\n",
      "Epoch 74/1000\n",
      "1570/1570 [==============================] - 0s 161us/sample - loss: 0.0010 - mae: 0.0229 - val_loss: 0.0377 - val_mae: 0.1253\n",
      "Epoch 75/1000\n",
      "1570/1570 [==============================] - 0s 155us/sample - loss: 0.0012 - mae: 0.0253 - val_loss: 0.0347 - val_mae: 0.1252\n",
      "Epoch 76/1000\n",
      "1570/1570 [==============================] - 0s 151us/sample - loss: 0.0021 - mae: 0.0315 - val_loss: 0.0391 - val_mae: 0.1283\n",
      "Epoch 77/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 0.0024 - mae: 0.0352 - val_loss: 0.0369 - val_mae: 0.1296\n",
      "Epoch 78/1000\n",
      "1570/1570 [==============================] - 0s 174us/sample - loss: 0.0054 - mae: 0.0539 - val_loss: 0.0359 - val_mae: 0.1281\n",
      "Epoch 79/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 0.0047 - mae: 0.0512 - val_loss: 0.0345 - val_mae: 0.1254\n",
      "Epoch 80/1000\n",
      "1570/1570 [==============================] - 0s 160us/sample - loss: 0.0045 - mae: 0.0476 - val_loss: 0.0326 - val_mae: 0.1250\n",
      "Epoch 81/1000\n",
      "1570/1570 [==============================] - 0s 151us/sample - loss: 0.0038 - mae: 0.0441 - val_loss: 0.0354 - val_mae: 0.1256\n",
      "Epoch 82/1000\n",
      "1570/1570 [==============================] - 0s 153us/sample - loss: 0.0031 - mae: 0.0399 - val_loss: 0.0388 - val_mae: 0.1275\n",
      "Epoch 83/1000\n",
      "1570/1570 [==============================] - 0s 159us/sample - loss: 0.0022 - mae: 0.0349 - val_loss: 0.0322 - val_mae: 0.1195\n",
      "Epoch 84/1000\n",
      "1570/1570 [==============================] - 0s 165us/sample - loss: 0.0022 - mae: 0.0332 - val_loss: 0.0314 - val_mae: 0.1213\n",
      "Epoch 85/1000\n",
      "1570/1570 [==============================] - 0s 152us/sample - loss: 0.0016 - mae: 0.0283 - val_loss: 0.0343 - val_mae: 0.1183\n",
      "Epoch 86/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0022 - mae: 0.0347 - val_loss: 0.0326 - val_mae: 0.1222\n",
      "Epoch 87/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0316 - val_mae: 0.1193\n",
      "Epoch 88/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0017 - mae: 0.0304 - val_loss: 0.0326 - val_mae: 0.1214\n",
      "Epoch 89/1000\n",
      "1570/1570 [==============================] - 0s 165us/sample - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0345 - val_mae: 0.1207\n",
      "Epoch 90/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0346 - val_mae: 0.1228\n",
      "Epoch 91/1000\n",
      "1570/1570 [==============================] - 0s 154us/sample - loss: 9.6177e-04 - mae: 0.0214 - val_loss: 0.0336 - val_mae: 0.1198\n",
      "Epoch 92/1000\n",
      "1570/1570 [==============================] - 0s 153us/sample - loss: 9.4717e-04 - mae: 0.0217 - val_loss: 0.0319 - val_mae: 0.1179\n",
      "Epoch 93/1000\n",
      "1570/1570 [==============================] - 0s 153us/sample - loss: 8.7556e-04 - mae: 0.0209 - val_loss: 0.0332 - val_mae: 0.1191\n",
      "Epoch 94/1000\n",
      "1570/1570 [==============================] - 0s 153us/sample - loss: 7.7202e-04 - mae: 0.0197 - val_loss: 0.0349 - val_mae: 0.1214\n",
      "Epoch 95/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0327 - val_mae: 0.1193\n",
      "Epoch 96/1000\n",
      "1570/1570 [==============================] - 0s 171us/sample - loss: 7.3873e-04 - mae: 0.0199 - val_loss: 0.0338 - val_mae: 0.1213\n",
      "Epoch 97/1000\n",
      "1570/1570 [==============================] - 0s 159us/sample - loss: 6.7132e-04 - mae: 0.0186 - val_loss: 0.0358 - val_mae: 0.1224\n",
      "Epoch 98/1000\n",
      "1570/1570 [==============================] - 0s 154us/sample - loss: 6.6312e-04 - mae: 0.0187 - val_loss: 0.0348 - val_mae: 0.1195\n",
      "Epoch 99/1000\n",
      "1570/1570 [==============================] - 0s 154us/sample - loss: 7.4516e-04 - mae: 0.0195 - val_loss: 0.0329 - val_mae: 0.1210\n",
      "Epoch 100/1000\n",
      "1570/1570 [==============================] - 0s 152us/sample - loss: 8.6268e-04 - mae: 0.0206 - val_loss: 0.0342 - val_mae: 0.1204\n",
      "Epoch 101/1000\n",
      "1570/1570 [==============================] - 0s 159us/sample - loss: 0.0011 - mae: 0.0234 - val_loss: 0.0331 - val_mae: 0.1226\n",
      "Epoch 102/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 0.0038 - mae: 0.0443 - val_loss: 0.0368 - val_mae: 0.1192\n",
      "Epoch 103/1000\n",
      "1570/1570 [==============================] - 0s 159us/sample - loss: 0.0026 - mae: 0.0369 - val_loss: 0.0329 - val_mae: 0.1197\n",
      "Epoch 104/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0027 - mae: 0.0349 - val_loss: 0.0305 - val_mae: 0.1180\n",
      "Epoch 105/1000\n",
      "1570/1570 [==============================] - 0s 150us/sample - loss: 0.0024 - mae: 0.0338 - val_loss: 0.0318 - val_mae: 0.1165\n",
      "Epoch 106/1000\n",
      "1570/1570 [==============================] - 0s 156us/sample - loss: 0.0025 - mae: 0.0341 - val_loss: 0.0378 - val_mae: 0.1280\n",
      "Epoch 107/1000\n",
      "1570/1570 [==============================] - 0s 163us/sample - loss: 0.0035 - mae: 0.0435 - val_loss: 0.0386 - val_mae: 0.1231\n",
      "Epoch 108/1000\n",
      "1570/1570 [==============================] - 0s 170us/sample - loss: 0.0022 - mae: 0.0344 - val_loss: 0.0359 - val_mae: 0.1221\n",
      "Epoch 109/1000\n",
      "1570/1570 [==============================] - 0s 162us/sample - loss: 0.0015 - mae: 0.0279 - val_loss: 0.0347 - val_mae: 0.1227\n",
      "Epoch 110/1000\n",
      "1570/1570 [==============================] - 0s 164us/sample - loss: 0.0015 - mae: 0.0275 - val_loss: 0.0363 - val_mae: 0.1203\n",
      "Epoch 111/1000\n",
      "1570/1570 [==============================] - 0s 155us/sample - loss: 0.0018 - mae: 0.0297 - val_loss: 0.0350 - val_mae: 0.1219\n",
      "Epoch 112/1000\n",
      "1570/1570 [==============================] - 0s 151us/sample - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0369 - val_mae: 0.1197\n",
      "Epoch 113/1000\n",
      "1570/1570 [==============================] - 0s 164us/sample - loss: 9.4239e-04 - mae: 0.0217 - val_loss: 0.0347 - val_mae: 0.1178\n",
      "Epoch 114/1000\n",
      "1570/1570 [==============================] - 0s 189us/sample - loss: 8.8014e-04 - mae: 0.0207 - val_loss: 0.0359 - val_mae: 0.1192\n",
      "Epoch 115/1000\n",
      "1570/1570 [==============================] - 0s 154us/sample - loss: 9.3292e-04 - mae: 0.0211 - val_loss: 0.0366 - val_mae: 0.1178\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570/1570 [==============================] - 0s 157us/sample - loss: 8.0084e-04 - mae: 0.0197 - val_loss: 0.0373 - val_mae: 0.1207\n",
      "Epoch 117/1000\n",
      "1570/1570 [==============================] - 0s 162us/sample - loss: 0.0022 - mae: 0.0337 - val_loss: 0.0363 - val_mae: 0.1199\n",
      "Epoch 118/1000\n",
      "1570/1570 [==============================] - 0s 160us/sample - loss: 0.0015 - mae: 0.0285 - val_loss: 0.0380 - val_mae: 0.1192\n",
      "Epoch 119/1000\n",
      "1570/1570 [==============================] - 0s 169us/sample - loss: 0.0012 - mae: 0.0256 - val_loss: 0.0352 - val_mae: 0.1189\n",
      "Epoch 120/1000\n",
      "1570/1570 [==============================] - 0s 168us/sample - loss: 0.0014 - mae: 0.0269 - val_loss: 0.0349 - val_mae: 0.1218\n",
      "Epoch 121/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 0.0016 - mae: 0.0271 - val_loss: 0.0345 - val_mae: 0.1176\n",
      "Epoch 122/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 0.0012 - mae: 0.0248 - val_loss: 0.0366 - val_mae: 0.1182\n",
      "Epoch 123/1000\n",
      "1570/1570 [==============================] - 0s 155us/sample - loss: 8.5278e-04 - mae: 0.0210 - val_loss: 0.0372 - val_mae: 0.1199\n",
      "Epoch 124/1000\n",
      "1570/1570 [==============================] - 0s 161us/sample - loss: 0.0014 - mae: 0.0265 - val_loss: 0.0367 - val_mae: 0.1206\n",
      "Epoch 125/1000\n",
      "1570/1570 [==============================] - 0s 186us/sample - loss: 8.3082e-04 - mae: 0.0212 - val_loss: 0.0396 - val_mae: 0.1208\n",
      "Epoch 126/1000\n",
      "1570/1570 [==============================] - 0s 173us/sample - loss: 0.0018 - mae: 0.0278 - val_loss: 0.0339 - val_mae: 0.1218\n",
      "Epoch 127/1000\n",
      "1570/1570 [==============================] - 0s 159us/sample - loss: 0.0024 - mae: 0.0351 - val_loss: 0.0378 - val_mae: 0.1205\n",
      "Epoch 128/1000\n",
      "1570/1570 [==============================] - 0s 165us/sample - loss: 0.0025 - mae: 0.0361 - val_loss: 0.0345 - val_mae: 0.1236\n",
      "Epoch 129/1000\n",
      "1570/1570 [==============================] - 0s 163us/sample - loss: 0.0026 - mae: 0.0368 - val_loss: 0.0355 - val_mae: 0.1168\n",
      "Epoch 130/1000\n",
      "1570/1570 [==============================] - 0s 160us/sample - loss: 0.0020 - mae: 0.0324 - val_loss: 0.0372 - val_mae: 0.1194\n",
      "Epoch 131/1000\n",
      "1570/1570 [==============================] - 0s 173us/sample - loss: 0.0014 - mae: 0.0273 - val_loss: 0.0344 - val_mae: 0.1173\n",
      "Epoch 132/1000\n",
      "1570/1570 [==============================] - 0s 188us/sample - loss: 9.0361e-04 - mae: 0.0219 - val_loss: 0.0362 - val_mae: 0.1153\n",
      "Epoch 133/1000\n",
      "1570/1570 [==============================] - 0s 175us/sample - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0384 - val_mae: 0.1201\n",
      "Epoch 134/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 0.0016 - mae: 0.0260 - val_loss: 0.0338 - val_mae: 0.1155\n",
      "Epoch 135/1000\n",
      "1570/1570 [==============================] - 0s 166us/sample - loss: 0.0010 - mae: 0.0225 - val_loss: 0.0388 - val_mae: 0.1201\n",
      "Epoch 136/1000\n",
      "1570/1570 [==============================] - 0s 168us/sample - loss: 7.9840e-04 - mae: 0.0196 - val_loss: 0.0340 - val_mae: 0.1157\n",
      "Epoch 137/1000\n",
      "1570/1570 [==============================] - 0s 184us/sample - loss: 6.1818e-04 - mae: 0.0174 - val_loss: 0.0370 - val_mae: 0.1166\n",
      "Epoch 138/1000\n",
      "1570/1570 [==============================] - 0s 163us/sample - loss: 6.5223e-04 - mae: 0.0178 - val_loss: 0.0355 - val_mae: 0.1174\n",
      "Epoch 139/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 7.6461e-04 - mae: 0.0193 - val_loss: 0.0347 - val_mae: 0.1168\n",
      "Epoch 140/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 5.6343e-04 - mae: 0.0167 - val_loss: 0.0352 - val_mae: 0.1157\n",
      "Epoch 141/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 5.4650e-04 - mae: 0.0162 - val_loss: 0.0360 - val_mae: 0.1168\n",
      "Epoch 142/1000\n",
      "1570/1570 [==============================] - 0s 174us/sample - loss: 4.4995e-04 - mae: 0.0151 - val_loss: 0.0364 - val_mae: 0.1173\n",
      "Epoch 143/1000\n",
      "1570/1570 [==============================] - 0s 172us/sample - loss: 6.2607e-04 - mae: 0.0170 - val_loss: 0.0365 - val_mae: 0.1177\n",
      "Epoch 144/1000\n",
      "1570/1570 [==============================] - 0s 165us/sample - loss: 8.1210e-04 - mae: 0.0201 - val_loss: 0.0370 - val_mae: 0.1176\n",
      "Epoch 145/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 7.5528e-04 - mae: 0.0202 - val_loss: 0.0372 - val_mae: 0.1173\n",
      "Epoch 146/1000\n",
      "1570/1570 [==============================] - 0s 162us/sample - loss: 5.8500e-04 - mae: 0.0178 - val_loss: 0.0372 - val_mae: 0.1194\n",
      "Epoch 147/1000\n",
      "1570/1570 [==============================] - 0s 156us/sample - loss: 5.5814e-04 - mae: 0.0168 - val_loss: 0.0356 - val_mae: 0.1197\n",
      "Epoch 148/1000\n",
      "1570/1570 [==============================] - 0s 175us/sample - loss: 5.6432e-04 - mae: 0.0166 - val_loss: 0.0382 - val_mae: 0.1188\n",
      "Epoch 149/1000\n",
      "1570/1570 [==============================] - 0s 167us/sample - loss: 6.0437e-04 - mae: 0.0149 - val_loss: 0.0350 - val_mae: 0.1189\n",
      "Epoch 150/1000\n",
      "1570/1570 [==============================] - 0s 157us/sample - loss: 8.2446e-04 - mae: 0.0196 - val_loss: 0.0366 - val_mae: 0.1182\n",
      "Epoch 151/1000\n",
      "1570/1570 [==============================] - 0s 164us/sample - loss: 8.3653e-04 - mae: 0.0212 - val_loss: 0.0413 - val_mae: 0.1226\n",
      "Epoch 152/1000\n",
      "1570/1570 [==============================] - 0s 158us/sample - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0374 - val_mae: 0.1201\n",
      "Epoch 153/1000\n",
      "1570/1570 [==============================] - 0s 163us/sample - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0357 - val_mae: 0.1201\n",
      "Epoch 154/1000\n",
      "1570/1570 [==============================] - 0s 177us/sample - loss: 0.0017 - mae: 0.0282 - val_loss: 0.0359 - val_mae: 0.1225\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5hcdZ3n8fe3bn1Pd/qSe0gCBgyXcElgQMYd0AEEHNTRVWBUnN2RWXdmHHdGV5gZncHdfVZnd12W0RFxZEcHRB1kHFQcERdERrkkGCKYhERISOfaSafvt7p894/f6U6l06lUd1Jdna7P63nq6apzqfrW6arzOb/fOXWOuTsiIiLHEit3ASIiMrMpKEREpCAFhYiIFKSgEBGRghQUIiJSkIJCREQKUlCIFMHM/t7M/muR0243s98sdU0i00VBITKNosBxM7th3PA7o+EfGDf8imj4fx43fHk0vG/c7T3T8DakwigoRKbfy8Atow/MLAH8W+BXE0x7C9CZP/04Te5en3f7xkmvViqegkJmjajL52NmttHM+s3sy2Y238y+b2a9ZvaYmc3Nm/4GM3vJzLrM7AkzW5U37kIzez6a7xtA9bjXequZbYjm/amZrZ5Eqd8BLs+r5S3ARmDvuNeoBd4F/AGw0szWTmqBiJwkCgqZbd4JXAWcCfwW8H3gz4BWwuf9wwBmdibwAPARoA14BPiOmaXMLAV8G/gHoBn4x+h5iea9CLgX+H2gBfgi8LCZVRVZ4xDwMHBj9Pj9wFeP8V76otf/QTSdyLRTUMhs8zfuvs/ddwE/AZ5x95+7+zDwT8CF0XTvAb7n7j909zTwP4Ea4A3ApUASuNPd0+7+IPBc3mt8EPiiuz/j7ll3/wowHM1XrK8C7zezRuA3CME03i3AN9w9C3wNuMnMkuOmORC1akZvq45+GpETo6CQ2WZf3v3BCR7XR/cXATtGR7h7DtgJLI7G7fIjz5i5I+/+MuBP81fQwNJovqK4+1OElsxfAN9198H88Wa2FLgSuD8a9M+E7q/rxz1Vq7s35d02FVuDSLES5S5ApEx2A+eNPjAzI6zsdwEOLDYzywuL0zi8s3kn8N/c/b+dYA33AZ8kBMJ47yNsyH0nlAaEoHg/E7c+REpGLQqpVN8ErjezN0fdOX9K6D76KfAzIAN82MwSZvbbwCV5834J+A9m9msW1JnZ9WbWMMka7iLsT3lygnHvB+4ALsi7vTOquWWSryNyQhQUUpHcfQvwXuBvgAOEHd+/5e4j7j4C/DbwAeAQYX/GQ3nzriPsp/hcNH5bNO1ka+h09x+N6+LCzC4FlgOfd/e9ebeHo9e6KW/yrnG/o/iTydYhcjymCxeJiEghalGIiEhBCgoRESlIQSEiIgUpKEREpKBT7ncUra2tvnz58nKXISJySlm/fv0Bd2+byrynXFAsX76cdevWlbsMEZFTipntOP5UE1PXk4iIFKSgEBGRghQUIiJS0Cm3j2Ii6XSa9vZ2hoaGyl1KyVVXV7NkyRKSyfFnmxYRKY1ZERTt7e00NDSwfPly8s60Oeu4OwcPHqS9vZ0VK1aUuxwRqRCzoutpaGiIlpaWWR0SAGZGS0tLRbScRGTmKFlQmNlSM3vczDZF1yX+4wmmucLMuqNrD28ws0+ewOudWMGniEp5nyIyc5Sy6ykD/Km7Px+dp3+9mf3Q3X85brqfuPtbS1iHiIicgJK1KNx9j7s/H93vBTYRLjM563R1dfG3f/u3k57vuuuuo6urqwQViYicPNOyj8LMlhMuav/MBKMvM7MXzOz7ZnbOMea/1czWmdm6jo6OElY6NccKimw2W3C+Rx55hKamplKVJSJyUpQ8KMysHvgW8BF37xk3+nlgmbufT7jS2ITXAnb3e9x9rbuvbWub0qlKSuq2227jV7/6FRdccAEXX3wxV155JTfffDPnnRcuyfz2t7+dNWvWcM4553DPPfeMzbd8+XIOHDjA9u3bWbVqFR/84Ac555xzuPrqqxkcHCzX2xEROUJJD4+NrkX8LeB+d39o/Pj84HD3R8zsb82s1d0PTPlFv38b7P3FlGef0ILz4NpPH3P0pz/9aV588UU2bNjAE088wfXXX8+LL744dgjrvffeS3NzM4ODg1x88cW8853vpKXlyMseb926lQceeIAvfelLvPvd7+Zb3/oW733ve0/u+xARmYJSHvVkwJeBTe7+2WNMsyCaDjO7JKrnYKlqmi6XXHLJEb9zuOuuuzj//PO59NJL2blzJ1u3bj1qnhUrVnDBBRcAsGbNGrZv3z5d5YqIFFTKFsXlwPuAX5jZhmjYnwGnAbj73cC7gA+ZWQYYBG4cf6H5SSuw5T9d6urqxu4/8cQTPPbYY/zsZz+jtraWK664YsLfQVRVVY3dj8fj6noSkRmjZEHh7k8BBQ/6d/fPAZ8rVQ3TpaGhgd7e3gnHdXd3M3fuXGpra9m8eTNPP/30NFcnInJiZsUpPMqtpaWFyy+/nHPPPZeamhrmz58/Nu4tb3kLd999N6tXr+ass87i0ksvLWOlIiKTZyfa0zPd1q5d6+MvXLRp0yZWrVpVpoqmX6W9XxE5cWa23t3XTmXeWXGuJxERKR0FhYiIFKSgEBGRghQUIiJSkIJCREQKUlCIiEhBCoqTYKqnGQe48847GRgYOMkViYicPAqKk0BBISKzmX6ZfRLkn2b8qquuYt68eXzzm99keHiYd7zjHdxxxx309/fz7ne/m/b2drLZLJ/4xCfYt28fu3fv5sorr6S1tZXHH3+83G9FROQosy4oPvPsZ9jcufmkPufrm1/Pxy/5+DHH559m/NFHH+XBBx/k2Wefxd254YYbePLJJ+no6GDRokV873vfA8I5oBobG/nsZz/L448/Tmtr60mtWUTkZFHX00n26KOP8uijj3LhhRdy0UUXsXnzZrZu3cp5553HY489xsc//nF+8pOf0NjYWO5SRUSKMutaFIW2/KeDu3P77bfz+7//+0eNW79+PY888gi33347V199NZ/85CfLUKGIyOSoRXES5J9m/JprruHee++lr68PgF27drF//352795NbW0t733ve/noRz/K888/f9S8IiIz0axrUZRD/mnGr732Wm6++WYuu+wyAOrr67nvvvvYtm0bH/vYx4jFYiSTSb7whS8AcOutt3LttdeycOFC7cwWkRlJpxk/BVXa+xWRE6fTjIuISMkoKEREpKBZExSnWhfaVFXK+xSRmWNWBEV1dTUHDx6c9StRd+fgwYNUV1eXuxQRqSCz4qinJUuW0N7eTkdHR7lLKbnq6mqWLFlS7jJEpILMiqBIJpOsWLGi3GWIiMxKs6LrSURESkdBISIiBSkoRESkIAWFiIgUpKAQEZGCFBQiIlKQgkJERApSUIiISEEKChERKUhBISIiBZUsKMxsqZk9bmabzOwlM/vjCaYxM7vLzLaZ2UYzu6hU9YiIyNSU8lxPGeBP3f15M2sA1pvZD939l3nTXAusjG6/Bnwh+isiIjNEyVoU7r7H3Z+P7vcCm4DF4yZ7G/BVD54GmsxsYalqEhGRyZuWfRRmthy4EHhm3KjFwM68x+0cHSaY2a1mts7M1lXCqcRFRGaSkgeFmdUD3wI+4u4940dPMMtRVx9y93vcfa27r21raytFmSIicgwlDQozSxJC4n53f2iCSdqBpXmPlwC7S1mTiIhMTimPejLgy8Amd//sMSZ7GHh/dPTTpUC3u+8pVU0iIjJ5pTzq6XLgfcAvzGxDNOzPgNMA3P1u4BHgOmAbMAD8bgnrERGRKShZULj7U0y8DyJ/Ggf+oFQ1iIjIidMvs0VEpCAFhYiIFKSgEBGRghQUIiJSkIJCREQKUlCIiEhBCgoRESlIQSEiIgUpKEREpCAFhYiIFKSgEBGRghQUIiJSkIJCREQKUlCIiEhBCgoRESlIQSEiIgUpKEREpCAFhYiIFKSgEBGRghQUIiJSkIJCREQKUlCIiEhBCgoRESlIQSEiIgUpKEREpCAFhYiIFFSRQfG1Z14rdwkiIqeMigwKEREpnoJCREQKUlCIiEhBCgoRESlIQSEiIgWVLCjM7F4z229mLx5j/BVm1m1mG6LbJ0tVi4iITF2ihM/998DngK8WmOYn7v7WEtYgIiInqGQtCnd/Eugs1fOLiMj0KPc+isvM7AUz+76ZnXOsiczsVjNbZ2brOjo6prM+EZGKV86geB5Y5u7nA38DfPtYE7r7Pe6+1t3XtrW1ndCLujtdAyMn9BwiIpWkbEHh7j3u3hfdfwRImllrqV/3vmde469/sIUXd3WX+qVERGaFsgWFmS0wM4vuXxLVcrDUr/vsq2G3ybb9faV+KRGRWaFkRz2Z2QPAFUCrmbUDfwkkAdz9buBdwIfMLAMMAje6u5eqnlFxC39zpX8pEZFZoWRB4e43HWf85wiHz06rWCwkRTanoBARKUa5j3qadokoKNSiEBEpTsGgMLM5BcaddvLLKb14FBQZtShERIpyvBbFE6N3zOxH48Yd83DWmSzaf45yQkSkOMcLCsu731xg3CkjPhoUSgoRkaIcLyj8GPcnenxKiGtntojIpBzvqKd5ZvYnhNbD6H2ixyf2E+kyiZl2ZouITMbxguJLQMME9wH+riQVlVg8akOpRSEiUpyCQeHudxxrnJldfPLLKb2YdmaLiEzKpH5wZ2ZnAzcCNwHdwNpSFFVKo/lgp+SueBGR6XfcoDCzZYRguAnIAMuAte6+vbSllcboWUKUEyIixTneD+5+CjxCOEfTu9x9DdB7qoYEwOg+bLUoRESKc7zDYzsIO7Dnc/gop1O6d39034QOehIRKU7BoHD3twHnES4ydIeZvQrMjU4LfkryKOe0M1tEpDjH3Ufh7t3AvcC9ZjYfeA9wp5ktdfelpS7wZBttSfip3TASEZk2kzp7rLvvc/e73P0NwK+XqKaSGv2hnbqeRESKU7BFYWYPH2f+G05iLdNirEWhpBARKcrxup4uA3YCDwDPMAuOKlWLQkRkco4XFAuAqwi/obgZ+B7wgLu/VOrCSmXsqKfyliEicso43lFPWXf/F3e/BbgU2AY8YWZ/NC3VlcToUU+KChGRYhTzy+wq4HpCq2I5cBfwUGnLKh3X7yhERCbleDuzvwKcC3wfuMPdX5yWqkpo9Kyx2pktIlKc47Uo3gf0A2cCH7bD570wwN39mNfUnqmyozuzy1yHiMip4ninGZ/U7yxOBaOXQNU+ChGR4sy6IDierPZRiIhMSuUFRS4HqOtJRKRYFRgU+sGdiMhkVG5QqE0hIlKUiguKxzbtD3eUEyIiRam4oBilnBARKU7lBoV2UoiIFKWCg6LcFYiInBoqNyjKXYCIyCmiZEFhZvea2X4zm/D8UBbcZWbbzGyjmV1UqlryVSXCW1aLQkSkOKVsUfw98JYC468FVka3W4EvlLCWMbpmtojI5JQsKNz9SaCzwCRvA77qwdNAk5ktLFU9o3SFOxGRySnnPorFhMusjmqPhh3FzG41s3Vmtq6jo+OEXlQnAxQRmZxyBsVE19+ecC3u7ve4+1p3X9vW1nZCL5pTToiITEo5g6IdWJr3eAmwu5QvmP/bCf2OQkSkOOUMioeB90dHP10KdLv7nlK+YDavOaGYEBEpznGvmT1VZvYAcAXQambtwF8CSQB3vxt4BLgO2AYMAL9bqlpG5Xc7qUEhIlKckgWFu990nPEO/EGpXn8i+TuydXisiEhxKuqX2a4WhYjIpFVUUBzZohARkWJUblAoKUREilJhQZH/SEkhIlKMygqKnFoUIiKTVVlBoa4nEZFJq7CgOHxfh8eKiBSnooLC1aIQEZm0igqKI1sUIiJSjAoLCrUoREQmq6KC4siTAiopRESKUVFBcUQrQjkhIlKUigoKncJDRGTyKjcotJNCRKQoFRYUh+8rJkREilNRQZHJ5cbuq0EhIlKcygqKrPZRiIhMVkUFRTqb36JQVIiIFKOigiKTU4tCRGSyKioo8lsUSgoRkeJUVFAcuY9CSSEiUozKCgod9SQiMmkVFRTprE4KKCIyWRUVFKNdT4a6nkREilVZQRF1PcVjphaFiEiRKiooRrue4jFTe0JEpEgVFhRqUYiITFZFBUUmLyj0QwoRkeJUVFCMdj0l1KIQESlaRQXFETuzy1yLiMipoqKC4oid2WpSiIgUpaKCIqOjnkREJq2kQWFmbzGzLWa2zcxum2D8B8ysw8w2RLffK2U9mVwOA2KmfRQiIsVKlOqJzSwOfB64CmgHnjOzh939l+Mm/Ya7/2Gp6siXzjqxmAE65klEpFilbFFcAmxz91fcfQT4OvC2Er7ecWWyOeJm4RQealKIiBSllEGxGNiZ97g9GjbeO81so5k9aGZLS1gPmZwTq6i9MiIiJ66Uq02bYNj4zfjvAMvdfTXwGPCVCZ/I7FYzW2dm6zo6OqZcUHq0RaF9FCIiRStlULQD+S2EJcDu/Anc/aC7D0cPvwSsmeiJ3P0ed1/r7mvb2tqmXFAm69GvsnX2WBGRYpUyKJ4DVprZCjNLATcCD+dPYGYL8x7eAGwqYT2kczlisdF9FKV8JRGR2aNkRz25e8bM/hD4ARAH7nX3l8zsU8A6d38Y+LCZ3QBkgE7gA6WqB6IWhRmYgkJEpFglCwoAd38EeGTcsE/m3b8duL2UNeTL5vxwi0JdTyIiRamoY4AyuRxhF4V2ZouIFKuigiKbC7/KNtMP7kREilVhQZEjZtFRu0oKEZGiVFRQZHJOLOzL1j4KEZEiVVRQjO7M1lFPIiLFq6igCC2K0aOeRESkGBUVFLmxridduEhEpFgVFRSjLQp01JOISNEqKiiy+V1PSgoRkaJUVFBkcrpwkYjIZFVUUGSjX2abmhQiIkWrsKDwsR/cKSZERIpTUUFRne2j0bujo57KXY2IyKmhcoIiPcS3B27hbUP/DOiX2SIixaqcoEhW84ot5azM5nBSQOWEiEhRKicogI12FivTLxPzrIJCRKRIFRUUL/iZ1Pggy3KvqeNJRKRIFRMU/el+Hmt5hSdqaliV3aJTeIiIFKligiIVT9FXtZdPtzSzMvPLcpcjInLKqJigSMaSZDtuYFcyzqbardpHISJSpIoJCoBM/0rOGmnj641QxWvlLkdE5JRQWUGRc87KXkcWo73pf/P5DZ9nc+dmhrPD5S5NRGTGSpS7gOmSyznu0Jm6hK/s+R/897nLuPuFu7n7hbuJWYw189dw/YrrObf1XNpq2xjJjtAz0sOS+iXUJmvLXb6ISNlUTFBko50S6XgtG+Lv4v5997HzzX/BS0vPY0vnFh7b8Rh/9bO/Omq+hCVY1bIKM6NnuIfVbau5bsV1tNa0kvEMmVyGdDZNxqO/uQw5ciypX8LpjaeTjCfHniudTY89zuQy7OjZQe9IL8PZYVqqW1hYv5C6ZF3R7ynnOWI2caPQ3RnKDpGKpYjH4hMvk1z2iHG9I738qutXHBw6yBmNZzCnag4v7H+BruEu3nTam2isaiy6NhGZPexUO0x07dq1vm7duknPN5TO8vpP/AvXnD2fPV0D3Nb9Kd7gP4ebvwGv+03cnZcPvcyOnh3sH9hPVaKK+mQ9Ww9tZUPHBhKWoCZRw3N7n6M33VvUayYsQWNVI/WpejoHO+lN9zK3ai4L6hawvWc7g5nBo+ZZ2rCUFY0r2NW7i939uzmt4TRWzl1JMhYCxnEGM4Ns7txMe287SxuWsrRhKQcGD9A51ElDqoGqeBU7e3fSM9IDQFW8itaaVubXzmd543JqE7U8vedptnVtoyZRQ02ihv50f8EuuOp4NW9c8kZaa1ppSDXQkGzAzOgY6ODg0EH6RvroS4ebYZw25zQW1S+iPllPzGL0DPfQPdJNz3APQ9khkrEkqXjqiL/JWJJkPEk2l6VruIt0Lk1dso7aRC11yToyuQyHhg+RzqapTlRTHa8OfxPV1CRqGM4Oc2DwANlclrpkHXXJOuqT9dSl6qhL1DGcHWb/wH7isTitNa1kc1k6BjtIxpIsbVhKQ6oBx3F3cp4bu+9M/Pjg4EF29e2ia7iLgfQAqXiKOak5YfmkGpiTmkN9qp5MLkN/up/+dD8DmQHOaTmHNfPXkIgdvZ2WyWWIWxyLTl6ZL5vL0pfuoy5ZN+G8x+LuDGeHqYpXTfi8Oc8xlBmiJlEz4fhRWw9t5Se7fsKyhmWcOfdMGlINpOIpsp4FoD5ZPza/uxd8rlJzdw4NHyJu8VNuA2c4O0zMYmPf+VED6QFynqM+VT+l5zWz9e6+dkrzVkpQ9A1nOPcvf8C15y6g/dAgDHXznbr/Ch1b4Or/Apf+x+j844UNZ4d5ds+zDGeHScQSJGIJkrHkEfcdZ0f3DrZ1baNzqJO+dB9zq+bSXNPMvv597Onfw/I5yzm39VxaqltIxpMcGDzAzt6dbO7czPae7SyuW8yi+kXs6NnBK92vkPUshmFmJGNJVjatZNmcZbzW+xrtve201rbSUt1C30gfg5lBljYsZWH9QjK5DH0jfRwYOsCevj1s79lOz0gPa+at4fx55zOcGWYgM0B9sp7GqkZe1/Q65lbP5ZXuVzg0dIjVbaupilfx4MsP8vSep+kd6aUv3UfOc0AIoZbqFhpSDdSn6qlP1pPxDDu6d7BvYB/pXBqAVCxFU1UTc6rmUBWvIp1Lk86lGcmOhPvZw4/jsThNVU0kYgkGM4P0p/sZzAwSsxhNVU2k4imGM8MMZYeOCtuaRA2JWIL+dP9YjaVkGHOq5lCXqCOdS9M70stQdui48zVVNVGbqKVnpIfGqkbm186nY7CD9t52ahI1zK+bz1BmiK7hLgwjEUuMLXfDaKlpoa2mjZaalrHpeoZ76E330pBsYE7VHIYyQ/Sme+kd6SXnOeqSdbTVtDGcHaY/3T8WNj3DPWQ8Q0OqgUV1iwDIepb6ZD1zquYwJzWHruEuntr1VMH3lIqlaEg1jG10NFY10lzdTHN1M3XJOvrT/YzkRqhL1BGLxdjXv4+u4a6x703+LeMZ9vbvZTg7zIrGFSyqW0TWs6EFnwst97HbaMs+Gj76fxjMDGIYZ7eczbI5y+gY7GAkOzJWU3N1MwOZAV468BLpXJrXN7+eRCzB5s7NZHNZVs5dObZ8R7IjDGWHxlrxhhGzWLhvNrZxMXYjRy6XA4OYxRhID9A51Bk+3xYfmzceixO3+NiwXX27eK33NdydlpoWquPVOE73cDd96T4+eN4H+fBFH57aZ1VBcXzdg2nOv+NRrj9vITsPDdA7lOHxP1oD3/4QbPoOLLkELv8wnHUdHKOrZrYY3+U0We7OQGaATC7DnNScgluO6WyarGepTlRP+fWAsZX++K620a3lwcwgqXhqrOtutOutP90/1tqpilcxr3YeOc+xf2A/yViStto2hjJD7OzdObZiMQu3GGElMDYsWjkAmBnNVc0sqFtwRPcihI2J3pHesVsilqA+WU9tspZkLMmze5/liZ1P4O40pBroGu5i38A+2mraOG3OaQykB9jbv5faZO3Y1nA6m6Yh1UBTVRO96V46BjrYN7CPg4MHqUnU0FTVNNZ67Rvpo3u4m5pkzVho1CRq6BjooGOwg5pEDbWJWnKeI+tZ5lbPpS5Zx97+vezt3zu2Eusb6aNnpIeekR5ynuMdr3sHv73yt9k7sJdXul5hIDMQNpgsgeMcHDpIz3DPWEuje7ibzqFODg4epD/dT12yjlQ8xUB6gIxnmF87n+bqZrKePXKjIZcmbnEW1i0kGUuyrWsbHQMdYxtj429JSx61wVaXrGNx/WL60n38665/pWOwg/m180nGkxwaOkTnUCeHhg6RiqdY1byKZCzJps5N5DzHWc1nkbAEW7u20jPcQ1Wiiqp4uI3+/0cDwd3JenYsMPJDYHTabC5LTbKG5upmquJVY/NmPRv+5rJkPUs2l2Ve7TzObD4TgI6BjrFW/pzUHNpq21gzfw0XzrtwSt8hBUUROvtHuOi//JDfWr2QHZ0D9AymeeJjV4azA67/v/DUndC1A+oXwHnvgtdfD0suhnErARGZHcZvfIyuC8vZZVZKJxIUFbMzO5MLHwobvWb26AgzWPvv4ML3w5ZHYOM34Jkvws8+B6l6aHkdNCyE/g7o3gltZ8HrroKVV0Hb64vqrhKRmWd863S2BsTJUDFBEeUE8ahb4aiGVDwBZ98QboNd8OqT4Xbo1RAQtc1wxptgzwvww0+E25zFMP8caFoG2REY7oW5y2DBebBgNTSfPuu7sURk9quYoBhtUcSijYiCFy6qaTocGhPp3gXbHoNXnoADW+G1ZyBZDcla2PQw5DJhumRtCJL550LTUqifD1gYn8tAZgi6XoPePdC4NEw7b1VoqSRrTtp7n5ThPtj6KLz2NAx1w8X/HpZeUp5aTkWvPBGW4dzlYaOhqqHcFYmcsIoJimzucP+jcYIXLmpcDGtuCbfxMsPhSKq9vzh8e+mfYKhr4udK1kHDAtjyL5B/eGqqAarnhELNoPXMECSpuvAau9aHkGo7CxZfFMKl+XSomQuJqtC6GeoOt8xQ2PdS2xJeI5cNr5kehBe+Drufh4XnQywRut0GO0PIxZOw8esw/7wQhIlqmHc2LDg31NK2ClL6MSIA6SF45KPw8384cnhtS+i+bD0z/K9azwotzjkLy1NnIV074fmvhv/vGW+aOOTcYftT0P4crLw6TCuzXsUFRWzsOO8SvVCiChauDrd8I/1hPwdALBlWyvFkWLGbQTYTurn2vRQCYLAzrOQtFlof+16CZ78UVvQWC11bp/8GdGyGn/7N4VbMpFlYkW37EXgWVl4Dl/9xaEVkhsOO/m2PhelG+uDn90G6//Ds9fPD1nPTsvBehrph8FC4xVOwZE2otbY59P8d3BqmqWsNy6B3Xwiy2pZwq2sN09a2Qnd72G/UfwBaXxeGZdOhmy87Ep6/ri10Gw73hWXfuCQs34GDYXzT0tBayw8091DfSF94nKqH6qbDzc1ipIdCWG9/Cva/BLt/HlqHb/xoOBDi0PZwcETnq3BwG2z5/pEh0rAw1FrTDG1nwrxzoGMT7HwuhPDZN0BmBDpfCdMtXB3mKVVX5u6fw9feA337ogEGVXOgpjEsm6qGsNx62sP7BCnE25wAAAvzSURBVPjRHWEj4syr4bTLwkaM58LnPDMCS9aGjZdCff+5LKz/e3j1x+G5Fl0ATadFGzUj4bORGQnPUdd2+PsC4X+QHgifFwh1bX0Utv4wfC/Sg2GazGCobcHq8PwLL4BFF4bPbX5tIwOAh2lnCvewPPv2hc90w6LweZlmJT3qyczeAvwfIA78nbt/etz4KuCrwBrgIPAed99e6DmnetTTlr29XHPnk9x48VJe3tfLvp5h/vW2N036ecrOPXwZ81cYmZGwYjq0HYZ7whekqgGqG0M3WjwFvXthoDO0DDDo2xvmW/VbYWU63BdWnk1LC79+LhcF2ouh5XRoR1ghHtoeAqC6KbxmTVMIxz0bIfotxTFZPITUROKpEBC9u4tfRhOpbQ0rgGw6hHBm3G8dYokwTV0b1LeFv0cc0uth+fXsgp7d0Lc/DMOgeQW0rISLfy+sNI9loDMssz0bYPeG6Mt/ADpeDhsAsWQIiY7NR9cHYQNhtMa6lrBsLB4+C7F4aJ0mq0Ow9u0LAVjXGla6NXPD5yabDv+PbDr8f4Z7wvvZ+4vwvDc9EP6P258K9Q51h9bwcF9Yqabq4Zy3w/Jfh83fgxcfCq2LY/3/auaGmmuawucxVRfCAUIQ7d0YPkv1C8Jn8niStWHDJp4K+wtz6bCRkqiGA1vCNE3LwhGLqbrQhZuoDp/tPS/A/l+GAILwfhuXhg2G/o4wzejwucvDrXEJ1M2DRCp8r0YGQjgla2DOoqgFX3O4xZ0dCc8z0Hl4g2nwUBjXMD98P5K1YcMlWRPuJ6rDftA9G8P3NH++7p2HN2ggbMRd9anjL6cJzMjDY80sDrwMXAW0A88BN7n7L/Om+Y/Aanf/D2Z2I/AOd39PoeedalD847qdfOzBjdx8yWls3tvL3u5Bfnr7myf9PDJJ6cEQJoOdgEHryrDCGDgYVlb180PLaqg7DBvoDCvPgYNhRXLGlSH0hvvCSi1eFaaPp8LKtL8jrHiq6sPWY/fO8Li2OXxpu3aGIOt6LbSQ4onw5W5YdLhrZXRF0bc/rGT79x/eKs5XMzesHBoXh/kXroZlbwjDT0RmBDp/FVZaVfWh2/DVJ8Pzzl0Rat+7Maz8+zuiGg+E9+fZEN65TGjppQejIGkLQTC6TIe7w2vFEiGQ4smwIq1qCC2VljPgN24LK7PJGuoJK/v8FTAGO58OK7+hrnCAyFBXWNHGEoCH95mqgytuh7PfFj4D+zeFluRgZ/gfJ6pDS9Fz4f/TvTOEbXowtFhqW2DXuvC8Z7wpdIe1rjx2KyYzEsJi1/oQcH37wzKobQ4Hp8DhDZ/O7WEDZXxrPZ4Kn91irpMZS4RwyAzDyHHO6GDx8H2omRvdmkJQNZ8euoprW8LnoXHx8V93oqefoUFxGfBX7n5N9Ph2AHf/73nT/CCa5mdmlgD2Am1eoKipBsV3N+7mD7/2cz70G2ew/WA/VYkYd944tR+uiJxyctnQItEhoJOTy4WAy6ZDKyBREzY2MiPhIJTRfYDpwfA3njq8oq9tDi2w0WU+3Btu6cHQKhn9OzIQgmDe2VGLvzRm6u8oFgM78x63A792rGncPWNm3UALcCB/IjO7Fbg1ethnZlumWtRtn6F19Pn/z01TfZaSGqtvBprJtYHqOxEzuTZQfSeqFVg21ZlLGRQTbbqMbykUMw3ufg9wz0kpymzdVFN1Oszk+mZybaD6TsRMrg1U34mK6ls+1flLeeGidiB/z+gSYPweybFpoq6nRqCzhDWJiMgklTIongNWmtkKM0sBNwIPj5vmYWD0xwjvAv5fof0TIiIy/UrW9RTtc/hD4AeEw2PvdfeXzOxTwDp3fxj4MvAPZraN0JK4sVT15DkpXVglNJPrm8m1geo7ETO5NlB9J+qE6jvlzh4rIiLTq5RdTyIiMgsoKEREpKCKCQoze4uZbTGzbWZ22wyoZ6mZPW5mm8zsJTP742h4s5n90My2Rn9P8Ge/J1Rj3Mx+bmbfjR6vMLNnotq+ER2kUK7amszsQTPbHC3Dy2bYsvtP0f/1RTN7wMyqy7n8zOxeM9tvZi/mDZtweVlwV/Rd2WhmF5Wpvv8R/X83mtk/mVlT3rjbo/q2mNk15agvb9xHzczNrDV6PK3L71i1mdkfRcvnJTP767zhk1927j7rb4Sd6b8CTgdSwAvA2WWuaSFwUXS/gXC6k7OBvwZui4bfBnymjDX+CfA14LvR428CN0b37wY+VMbavgL8XnQ/BTTNlGVH+CHpq0BN3nL7QDmXH/BvgIuAF/OGTbi8gOuA7xN+53Qp8EyZ6rsaSET3P5NX39nRd7gKWBF9t+PTXV80fCnhgJ0dQGs5lt8xlt2VwGNAVfR43oksu2n5kJb7BlwG/CDv8e3A7eWua1yN/0w4L9YWYGE0bCGwpUz1LAF+BLwJ+G70oT+Q98U9YplOc21zohWxjRs+U5bd6BkHmglHFn4XuKbcyw9YPm5lMuHyAr5IOC/bUdNNZ33jxr0DuD+6f8T3N1pRX1aO+oAHgfOB7XlBMe3Lb4L/7TeB35xguiktu0rpeprodCJTO7NWCZjZcuBC4BlgvrvvAYj+zitTWXcC/xmIrg1IC9Dl7qNnSCvnMjwd6AD+b9Q19ndmVscMWXbuvgv4n8BrwB6gG1jPzFl+o461vGbi9+XfEbbSYYbUZ2Y3ALvc/YVxo2ZCfWcCb4y6On9sZhefSG2VEhRFnSqkHMysHvgW8BF37yl3PQBm9lZgv7uvzx88waTlWoYJQlP7C+5+IdBP6DqZEaK+/rcRmvaLgDrg2gkmnRGfwQnMpP81ZvbnQAa4f3TQBJNNa31mVgv8OfDJiUZPMGy6l18CmEvo+voY8E0zM6ZYW6UERTGnE5l2ZpYkhMT97v5QNHifmS2Mxi8E9pehtMuBG8xsO/B1QvfTnUBTdKoVKO8ybAfa3f2Z6PGDhOCYCcsO4DeBV929w93TwEPAG5g5y2/UsZbXjPm+mNktwFuB3/Gor4SZUd8ZhA2BF6LvyRLgeTNbMEPqawce8uBZQs9A61Rrq5SgKOZ0ItMqSvcvA5vc/bN5o/JPa3ILYd/FtHL32919iYeTiN1IOLXK7wCPE061Urbaovr2AjvN7Kxo0JuBXzIDll3kNeBSM6uN/s+j9c2I5ZfnWMvrYeD90dE7lwLdo11U08nChc8+Dtzg7gN5ox4GbjSzKjNbAawEnp3O2tz9F+4+z92XR9+TdsLBKXuZGcvv24QNPMzsTMIBHweY6rIr9Q6gmXIjHInwMmEv/5/PgHp+ndDk2whsiG7XEfYF/AjYGv1tLnOdV3D4qKfTow/VNuAfiY6oKFNdFwDrouX3bUIze8YsO+AOYDPwIvAPhKNMyrb8gAcI+0vShJXavz/W8iJ0T3w++q78Alhbpvq2EfrTR78fd+dN/+dRfVuAa8tR37jx2zm8M3tal98xll0KuC/6/D0PvOlElp1O4SEiIgVVSteTiIhMkYJCREQKUlCIiEhBCgoRESlIQSEiIgUpKESmkZldYdHZeEVOFQoKEREpSEEhMgEze6+ZPWtmG8zsixauzdFnZv/LzJ43sx+ZWVs07QVm9nTedRNGr+vwOjN7zMxeiOY5I3r6ejt8LY37o19vi8xYCgqRccxsFfAe4HJ3vwDIAr9DOLnf8+5+EfBj4C+jWb4KfNzdVxN+iTs6/H7g8+5+PuFcT6OncbgQ+Ajh2gCnE86tJTJjJY4/iUjFeTOwBngu2tivIZwwLwd8I5rmPuAhM2sEmtz9x9HwrwD/aGYNwGJ3/ycAdx8CiJ7vWXdvjx5vIFxL4KnSvy2RqVFQiBzNgK+4++1HDDT7xLjpCp3/plB30nDe/Sz6HsoMp64nkaP9CHiXmc2DsWtLLyN8X0bP/noz8JS7dwOHzOyN0fD3AT/2cG2RdjN7e/QcVdE1DEROOdqSERnH3X9pZn8BPGpmMcJZOf+AcIGkc8xsPeGqde+JZrkFuDsKgleA342Gvw/4opl9KnqOfzuNb0PkpNHZY0WKZGZ97l5f7jpEppu6nkREpCC1KEREpCC1KEREpCAFhYiIFKSgEBGRghQUIiJSkIJCREQK+v+2IWcppsMHugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.11115711182355881\n",
      "Overfit checks:\n",
      "Model Accuracy: 0.0364975668489933\n"
     ]
    }
   ],
   "source": [
    "def silent_evaluation(model, x_test, y_test):\n",
    "    f = open('/dev/null', 'w')\n",
    "    regular_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    sys.stdout = regular_stdout\n",
    "    print('Model Accuracy: {}'.format(test_acc))\n",
    "    \n",
    "def split_data(train_x, train_y, training=0.70, validation=0.5):\n",
    "    train_size = training\n",
    "\n",
    "    train_cnt = math.floor(train_x.shape[0] * train_size)\n",
    "    x_train = train_x[0:train_cnt]\n",
    "    y_train = train_y[0:train_cnt]\n",
    "    x_test = train_x[train_cnt:]\n",
    "    y_test = train_y[train_cnt:]\n",
    "\n",
    "    division = validation\n",
    "\n",
    "    train_cnt = math.floor(x_test.shape[0] * division)\n",
    "    x_validate = x_test[0:train_cnt]\n",
    "    y_validate = y_test[0:train_cnt]\n",
    "    x_test = x_test[train_cnt:]\n",
    "    y_test = y_test[train_cnt:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_validate, y_validate    \n",
    "\n",
    "frame = load_frame()\n",
    "data_x, data_y, number_of_features = load_all_data_with_mine(frame) #load_meaningful_subset(frame)\n",
    "data_y = pd.concat([frame.mutation], axis = 1).round(2).values\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "sns.distplot(data_y);\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test, x_validate, y_validate = split_data(data_x, data_y)\n",
    "\n",
    "print(x_train.shape)\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(number_of_features, activation='relu', input_dim=number_of_features))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(20, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(patience=50,restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=1000, verbose=1, validation_data=(x_validate, y_validate),\n",
    "                    callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "silent_evaluation(model, x_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Overfit checks:\")\n",
    "silent_evaluation(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24680519104003906,\n",
       " 0.6155930161476135,\n",
       " 0.5725759267807007,\n",
       " 0.7378982901573181,\n",
       " 0.4911053776741028,\n",
       " 0.31005042791366577,\n",
       " 0.7591556906700134,\n",
       " 0.35281258821487427,\n",
       " 0.9439566731452942,\n",
       " 0.12693369388580322,\n",
       " 0.2403559386730194,\n",
       " 0.15719057619571686,\n",
       " 0.5805023312568665,\n",
       " 0.7238458395004272,\n",
       " 0.6244462728500366,\n",
       " 0.891404926776886,\n",
       " 0.987051248550415,\n",
       " 0.7764669060707092,\n",
       " 0.6993514895439148,\n",
       " 1.0973371267318726,\n",
       " 0.7163787484169006,\n",
       " 0.6466306447982788,\n",
       " 0.830091655254364,\n",
       " 0.956176221370697,\n",
       " 0.7805989384651184,\n",
       " 0.5929784178733826,\n",
       " 0.815973699092865,\n",
       " 0.757851779460907,\n",
       " 0.6383528113365173,\n",
       " 0.14846409857273102,\n",
       " 0.6922799944877625,\n",
       " 0.45556771755218506,\n",
       " 0.5626355409622192,\n",
       " 0.36289501190185547,\n",
       " 0.9565737843513489,\n",
       " 0.8427945971488953,\n",
       " 0.8606021404266357,\n",
       " 0.2592965364456177,\n",
       " 0.4871818423271179,\n",
       " 0.3564484119415283,\n",
       " 0.11690826714038849,\n",
       " 0.7426065802574158,\n",
       " 0.6174106001853943,\n",
       " 0.7962482571601868,\n",
       " 0.1673336774110794,\n",
       " 0.5636953115463257,\n",
       " 0.5859554409980774,\n",
       " 0.18189387023448944,\n",
       " 1.2320245504379272,\n",
       " 0.3749989867210388,\n",
       " 0.46136438846588135,\n",
       " 0.6298810839653015,\n",
       " 0.9872312545776367,\n",
       " 0.9885141253471375,\n",
       " 0.16986101865768433,\n",
       " 0.308085173368454,\n",
       " 1.0689083337783813,\n",
       " 0.8889062404632568,\n",
       " 0.7894424200057983,\n",
       " 0.8228845000267029,\n",
       " 0.6688860654830933,\n",
       " 0.9119282364845276,\n",
       " 0.8187276721000671,\n",
       " 0.9394364953041077,\n",
       " 0.6847925782203674,\n",
       " 0.4450249671936035,\n",
       " 0.8176539540290833,\n",
       " 0.7506542801856995,\n",
       " 0.6301760673522949,\n",
       " 0.8731857538223267,\n",
       " 0.8331771492958069,\n",
       " 0.9274061918258667,\n",
       " 0.5374999642372131,\n",
       " 0.6401036977767944,\n",
       " 0.4953274726867676,\n",
       " 0.8884679675102234,\n",
       " 0.8209972381591797,\n",
       " 0.5213010907173157,\n",
       " 0.7066026329994202,\n",
       " 0.1739336997270584,\n",
       " 0.8480691313743591,\n",
       " 0.563506543636322,\n",
       " 0.6092777848243713,\n",
       " 0.6409183144569397,\n",
       " 0.8478187918663025,\n",
       " 0.3902645707130432,\n",
       " 0.4745592474937439,\n",
       " 0.8733969926834106,\n",
       " 0.7239404916763306,\n",
       " 0.916604220867157,\n",
       " 0.7430735230445862,\n",
       " 0.6052283644676208,\n",
       " 0.755943775177002,\n",
       " 0.5865453481674194,\n",
       " 0.27111148834228516,\n",
       " 0.4909259080886841,\n",
       " 0.6523072719573975,\n",
       " 0.40367448329925537,\n",
       " 0.7891719937324524,\n",
       " 0.8473434448242188,\n",
       " 0.8165335059165955,\n",
       " 0.8910431265830994,\n",
       " 0.5385445356369019,\n",
       " 0.790964663028717,\n",
       " 0.15542136132717133,\n",
       " 0.8308318853378296,\n",
       " 1.0155515670776367,\n",
       " 0.5523998737335205,\n",
       " 0.7000817656517029,\n",
       " 0.08500783890485764,\n",
       " 0.825465738773346,\n",
       " 0.7974348664283752,\n",
       " 0.964752733707428,\n",
       " 0.7486531138420105,\n",
       " 0.7733638882637024,\n",
       " 0.32344794273376465,\n",
       " 0.6141729950904846,\n",
       " 0.704544723033905,\n",
       " 0.29673007130622864,\n",
       " 0.6157180666923523,\n",
       " 0.8782145380973816,\n",
       " 0.9251355528831482,\n",
       " 0.9892669320106506,\n",
       " 0.8693707585334778,\n",
       " 0.7857547998428345,\n",
       " 0.5200303792953491,\n",
       " 0.969811201095581,\n",
       " 0.3682519793510437,\n",
       " 0.620842695236206,\n",
       " 0.8263360857963562,\n",
       " 0.9881812930107117,\n",
       " 0.9551264047622681,\n",
       " 0.8208164572715759,\n",
       " 0.8381484150886536,\n",
       " 0.6245094537734985,\n",
       " 0.9337565302848816,\n",
       " 0.621191143989563,\n",
       " 0.9061696529388428,\n",
       " 0.6826744675636292,\n",
       " 0.3209500312805176,\n",
       " 0.8029055595397949,\n",
       " 0.6178892254829407,\n",
       " 0.9446577429771423,\n",
       " 0.9949907064437866,\n",
       " 0.717749297618866,\n",
       " 0.5298066139221191,\n",
       " 0.8725598454475403,\n",
       " 1.0107272863388062,\n",
       " 0.5626133680343628,\n",
       " 0.5903844833374023,\n",
       " 0.8491596579551697,\n",
       " 0.9083806276321411,\n",
       " 0.7416815161705017,\n",
       " 0.5195826888084412,\n",
       " 0.9329798817634583,\n",
       " 0.8905683159828186,\n",
       " 0.7761402130126953,\n",
       " 0.7106992602348328,\n",
       " 0.5396566390991211,\n",
       " 0.9541603922843933,\n",
       " 0.08205627650022507,\n",
       " 1.0330989360809326,\n",
       " 0.324130654335022,\n",
       " 0.8683362603187561,\n",
       " 0.7117691040039062,\n",
       " 0.6816295385360718,\n",
       " 0.39818519353866577,\n",
       " 0.8544812798500061,\n",
       " 0.9275367856025696,\n",
       " 0.7020965218544006,\n",
       " 0.6771622896194458,\n",
       " 0.8908945918083191,\n",
       " 0.8422902822494507,\n",
       " 0.9005406498908997,\n",
       " 0.9314189553260803,\n",
       " 0.6411068439483643,\n",
       " 0.7325189113616943,\n",
       " 1.0476276874542236,\n",
       " 0.507519543170929,\n",
       " 0.5782120227813721,\n",
       " 0.628140389919281,\n",
       " 0.6412307620048523,\n",
       " 0.802765965461731,\n",
       " 0.8668151497840881,\n",
       " 0.8174812197685242,\n",
       " 0.7692598700523376,\n",
       " 0.8073269724845886,\n",
       " 0.3497971296310425,\n",
       " 0.9453830122947693,\n",
       " 0.5748533606529236,\n",
       " 0.818598747253418,\n",
       " 0.46599942445755005,\n",
       " 1.1082189083099365,\n",
       " 0.7161234021186829,\n",
       " 0.7700098156929016,\n",
       " 0.7174620032310486,\n",
       " 1.004726529121399,\n",
       " 0.8268497586250305,\n",
       " 0.801515519618988,\n",
       " 0.6672689914703369,\n",
       " 0.478796124458313,\n",
       " 0.7260787487030029,\n",
       " 0.4962238073348999,\n",
       " 0.37117087841033936,\n",
       " 1.0174336433410645,\n",
       " 0.766232967376709,\n",
       " 0.9636779427528381,\n",
       " 0.7518497109413147,\n",
       " 0.8094472289085388,\n",
       " 0.8316745758056641,\n",
       " 0.18558380007743835,\n",
       " 0.8933699727058411,\n",
       " 0.9720168709754944,\n",
       " 0.7347496151924133,\n",
       " 0.5546106100082397,\n",
       " 0.3403952717781067,\n",
       " 0.8564684987068176,\n",
       " 0.9191134572029114,\n",
       " 0.8311672806739807,\n",
       " 0.0921328142285347,\n",
       " 0.7592840790748596,\n",
       " 0.4934820532798767,\n",
       " 0.7083466053009033,\n",
       " 0.6773386597633362,\n",
       " 0.3691621422767639,\n",
       " 0.3719977140426636,\n",
       " 0.8483784198760986,\n",
       " 0.4637869596481323,\n",
       " 0.8497077822685242,\n",
       " 0.7103540301322937,\n",
       " 0.5224134922027588,\n",
       " 0.6242923140525818,\n",
       " 0.5414184927940369,\n",
       " 0.1769758015871048,\n",
       " 0.6386760473251343,\n",
       " 0.18293830752372742,\n",
       " 0.8300051689147949,\n",
       " 0.44580936431884766,\n",
       " 0.35912513732910156,\n",
       " 0.9140393137931824,\n",
       " 0.9589308500289917,\n",
       " 0.3620569705963135,\n",
       " 0.8323899507522583,\n",
       " 0.5646229386329651,\n",
       " 0.5494125485420227,\n",
       " 0.32006630301475525,\n",
       " 0.23368674516677856,\n",
       " 0.9157074093818665,\n",
       " 0.49123191833496094,\n",
       " 0.9105404019355774,\n",
       " 0.5259281396865845,\n",
       " 0.8609525561332703,\n",
       " 0.5964949131011963,\n",
       " 0.9665740132331848,\n",
       " 0.7365323305130005,\n",
       " 0.5835555791854858,\n",
       " 0.5868104100227356,\n",
       " 0.27226585149765015,\n",
       " 1.0893688201904297,\n",
       " 0.7182901501655579,\n",
       " 0.8601251244544983,\n",
       " 0.4711849093437195,\n",
       " 0.47640472650527954,\n",
       " 0.5195175409317017,\n",
       " 0.2808125615119934,\n",
       " 0.40078192949295044,\n",
       " 0.1911781132221222,\n",
       " 0.482558012008667,\n",
       " 0.6078999042510986,\n",
       " 0.7217462658882141,\n",
       " 0.6051015853881836,\n",
       " 0.6202751398086548,\n",
       " 0.5724629759788513,\n",
       " 0.7398555874824524,\n",
       " 0.7658408284187317,\n",
       " 0.7905398011207581,\n",
       " 0.4105309247970581,\n",
       " 0.2058451622724533,\n",
       " 0.7471750974655151,\n",
       " 0.7166417241096497,\n",
       " 0.8475753664970398,\n",
       " 0.6517333388328552,\n",
       " 0.9125138521194458,\n",
       " 0.6322697401046753,\n",
       " 0.5502889752388,\n",
       " 0.8561654686927795,\n",
       " 0.6528456211090088,\n",
       " 0.65529465675354,\n",
       " 0.31800949573516846,\n",
       " 0.866891086101532,\n",
       " 0.8182775378227234,\n",
       " 0.8280244469642639,\n",
       " 0.9422144293785095,\n",
       " 0.08155488967895508,\n",
       " 0.849365234375,\n",
       " 0.2514035701751709,\n",
       " 0.7803956866264343,\n",
       " 0.9973297715187073,\n",
       " 0.2972429394721985,\n",
       " 0.44149816036224365,\n",
       " 0.9331374764442444,\n",
       " 0.787531316280365,\n",
       " 0.4094539284706116,\n",
       " 0.4552745819091797,\n",
       " 0.7061446309089661,\n",
       " 0.6763939261436462,\n",
       " 0.5954948663711548,\n",
       " 0.4178513288497925,\n",
       " 0.7577415108680725,\n",
       " 0.8169046640396118,\n",
       " 0.756923496723175,\n",
       " 0.40838444232940674,\n",
       " 0.839260995388031,\n",
       " 0.42025578022003174,\n",
       " 0.4016077518463135,\n",
       " 0.8763638138771057,\n",
       " 0.6513059735298157,\n",
       " 1.0547515153884888,\n",
       " 0.8710665702819824,\n",
       " 0.7680264115333557,\n",
       " 0.4572522044181824,\n",
       " 0.3691333532333374,\n",
       " 0.5382311940193176,\n",
       " 0.5384222865104675,\n",
       " 0.5589584708213806,\n",
       " 0.9231346249580383,\n",
       " 1.1509666442871094,\n",
       " 0.7618151307106018,\n",
       " 0.8294313549995422,\n",
       " 0.8980230093002319,\n",
       " 0.6059439182281494,\n",
       " 0.9838661551475525,\n",
       " 0.8383694291114807,\n",
       " 0.5381954312324524,\n",
       " 0.8056140542030334,\n",
       " 0.5487674474716187,\n",
       " 0.8289979696273804]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.concatenate(y_pred).tolist()\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37,\n",
       " 0.53,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.51,\n",
       " 0.44,\n",
       " 1.0,\n",
       " 0.14,\n",
       " 0.89,\n",
       " 0.37,\n",
       " 0.33,\n",
       " 0.06,\n",
       " 0.57,\n",
       " 0.54,\n",
       " 0.78,\n",
       " 0.78,\n",
       " 1.0,\n",
       " 0.65,\n",
       " 0.88,\n",
       " 1.0,\n",
       " 0.77,\n",
       " 0.6,\n",
       " 0.44,\n",
       " 0.98,\n",
       " 0.8,\n",
       " 0.54,\n",
       " 0.53,\n",
       " 0.64,\n",
       " 0.82,\n",
       " 0.33,\n",
       " 0.76,\n",
       " 0.22,\n",
       " 0.31,\n",
       " 0.29,\n",
       " 1.0,\n",
       " 0.78,\n",
       " 0.84,\n",
       " 0.29,\n",
       " 0.48,\n",
       " 0.15,\n",
       " 0.19,\n",
       " 0.78,\n",
       " 0.76,\n",
       " 0.79,\n",
       " 0.19,\n",
       " 0.58,\n",
       " 0.5,\n",
       " 0.09,\n",
       " 0.87,\n",
       " 0.4,\n",
       " 0.39,\n",
       " 0.61,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.13,\n",
       " 0.32,\n",
       " 0.7,\n",
       " 1.0,\n",
       " 0.63,\n",
       " 0.74,\n",
       " 0.82,\n",
       " 0.91,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 0.45,\n",
       " 0.76,\n",
       " 0.71,\n",
       " 0.76,\n",
       " 0.73,\n",
       " 0.89,\n",
       " 1.0,\n",
       " 0.57,\n",
       " 0.52,\n",
       " 0.88,\n",
       " 0.77,\n",
       " 1.0,\n",
       " 0.68,\n",
       " 0.79,\n",
       " 0.23,\n",
       " 0.59,\n",
       " 0.73,\n",
       " 0.9,\n",
       " 0.59,\n",
       " 0.73,\n",
       " 0.47,\n",
       " 0.69,\n",
       " 1.0,\n",
       " 0.6,\n",
       " 0.9,\n",
       " 0.67,\n",
       " 0.59,\n",
       " 0.57,\n",
       " 0.5,\n",
       " 0.18,\n",
       " 0.52,\n",
       " 0.9,\n",
       " 0.31,\n",
       " 0.61,\n",
       " 0.83,\n",
       " 1.0,\n",
       " 0.97,\n",
       " 0.62,\n",
       " 0.9,\n",
       " 0.28,\n",
       " 0.73,\n",
       " 1.0,\n",
       " 0.33,\n",
       " 0.66,\n",
       " 0.14,\n",
       " 0.78,\n",
       " 0.85,\n",
       " 0.67,\n",
       " 0.6,\n",
       " 0.96,\n",
       " 0.23,\n",
       " 0.58,\n",
       " 0.64,\n",
       " 0.3,\n",
       " 0.8,\n",
       " 0.92,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.86,\n",
       " 0.87,\n",
       " 0.45,\n",
       " 0.81,\n",
       " 0.25,\n",
       " 0.67,\n",
       " 0.91,\n",
       " 0.64,\n",
       " 0.75,\n",
       " 0.82,\n",
       " 0.85,\n",
       " 0.92,\n",
       " 1.0,\n",
       " 0.69,\n",
       " 0.88,\n",
       " 0.71,\n",
       " 0.31,\n",
       " 0.66,\n",
       " 0.45,\n",
       " 0.78,\n",
       " 1.0,\n",
       " 0.55,\n",
       " 0.61,\n",
       " 0.78,\n",
       " 0.93,\n",
       " 0.59,\n",
       " 0.52,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.83,\n",
       " 0.55,\n",
       " 0.78,\n",
       " 1.0,\n",
       " 0.77,\n",
       " 0.72,\n",
       " 1.0,\n",
       " 0.88,\n",
       " 0.01,\n",
       " 1.0,\n",
       " 0.32,\n",
       " 0.86,\n",
       " 0.5,\n",
       " 0.97,\n",
       " 0.69,\n",
       " 0.72,\n",
       " 1.0,\n",
       " 0.55,\n",
       " 0.75,\n",
       " 0.9,\n",
       " 0.48,\n",
       " 0.1,\n",
       " 0.97,\n",
       " 0.74,\n",
       " 0.68,\n",
       " 0.89,\n",
       " 0.5,\n",
       " 0.64,\n",
       " 0.91,\n",
       " 0.8,\n",
       " 0.76,\n",
       " 0.84,\n",
       " 0.66,\n",
       " 0.67,\n",
       " 0.94,\n",
       " 0.33,\n",
       " 0.67,\n",
       " 0.5,\n",
       " 0.99,\n",
       " 0.26,\n",
       " 1.0,\n",
       " 0.67,\n",
       " 0.64,\n",
       " 0.75,\n",
       " 0.81,\n",
       " 0.93,\n",
       " 0.36,\n",
       " 0.74,\n",
       " 0.35,\n",
       " 0.52,\n",
       " 0.78,\n",
       " 0.49,\n",
       " 1.0,\n",
       " 0.87,\n",
       " 1.0,\n",
       " 0.66,\n",
       " 0.87,\n",
       " 0.71,\n",
       " 0.31,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.68,\n",
       " 0.48,\n",
       " 0.36,\n",
       " 0.86,\n",
       " 0.97,\n",
       " 0.71,\n",
       " 0.09,\n",
       " 0.81,\n",
       " 0.6,\n",
       " 0.86,\n",
       " 0.69,\n",
       " 0.1,\n",
       " 0.74,\n",
       " 0.78,\n",
       " 0.47,\n",
       " 1.0,\n",
       " 0.83,\n",
       " 0.55,\n",
       " 0.59,\n",
       " 1.0,\n",
       " 0.02,\n",
       " 0.48,\n",
       " 0.16,\n",
       " 0.74,\n",
       " 0.49,\n",
       " 0.29,\n",
       " 0.84,\n",
       " 0.83,\n",
       " 0.18,\n",
       " 0.87,\n",
       " 0.79,\n",
       " 0.5,\n",
       " 0.48,\n",
       " 0.1,\n",
       " 1.0,\n",
       " 0.48,\n",
       " 0.74,\n",
       " 0.44,\n",
       " 0.93,\n",
       " 0.64,\n",
       " 0.63,\n",
       " 0.8,\n",
       " 0.52,\n",
       " 0.54,\n",
       " 0.52,\n",
       " 0.48,\n",
       " 0.63,\n",
       " 0.9,\n",
       " 0.52,\n",
       " 0.54,\n",
       " 0.48,\n",
       " 0.1,\n",
       " 0.61,\n",
       " 0.21,\n",
       " 0.61,\n",
       " 0.54,\n",
       " 0.73,\n",
       " 0.59,\n",
       " 0.76,\n",
       " 0.46,\n",
       " 0.84,\n",
       " 0.86,\n",
       " 0.69,\n",
       " 0.18,\n",
       " 0.18,\n",
       " 0.64,\n",
       " 0.66,\n",
       " 0.67,\n",
       " 0.46,\n",
       " 1.0,\n",
       " 0.46,\n",
       " 0.5,\n",
       " 0.86,\n",
       " 0.72,\n",
       " 0.7,\n",
       " 0.37,\n",
       " 0.85,\n",
       " 0.86,\n",
       " 0.64,\n",
       " 1.0,\n",
       " 0.09,\n",
       " 0.77,\n",
       " 0.22,\n",
       " 0.77,\n",
       " 1.0,\n",
       " 0.22,\n",
       " 0.36,\n",
       " 0.88,\n",
       " 0.85,\n",
       " 0.26,\n",
       " 0.39,\n",
       " 0.44,\n",
       " 0.61,\n",
       " 0.58,\n",
       " 0.36,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 0.78,\n",
       " 0.33,\n",
       " 0.78,\n",
       " 0.45,\n",
       " 0.46,\n",
       " 1.0,\n",
       " 0.85,\n",
       " 0.88,\n",
       " 0.92,\n",
       " 0.78,\n",
       " 0.71,\n",
       " 0.05,\n",
       " 0.83,\n",
       " 0.3,\n",
       " 0.51,\n",
       " 0.74,\n",
       " 0.86,\n",
       " 0.69,\n",
       " 0.69,\n",
       " 0.66,\n",
       " 0.61,\n",
       " 0.97,\n",
       " 0.88,\n",
       " 0.48,\n",
       " 0.93,\n",
       " 0.66,\n",
       " 0.69]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testi = np.concatenate(y_test).tolist()\n",
    "y_testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12319480895996093,\n",
       " 0.0855930161476135,\n",
       " 0.17742407321929932,\n",
       " 0.2621017098426819,\n",
       " 0.018894622325897226,\n",
       " 0.12994957208633423,\n",
       " 0.24084430932998657,\n",
       " 0.21281258821487425,\n",
       " 0.053956673145294176,\n",
       " 0.24306630611419677,\n",
       " 0.0896440613269806,\n",
       " 0.09719057619571686,\n",
       " 0.010502331256866504,\n",
       " 0.1838458395004272,\n",
       " 0.1555537271499634,\n",
       " 0.11140492677688596,\n",
       " 0.012948751449584961,\n",
       " 0.1264669060707092,\n",
       " 0.1806485104560852,\n",
       " 0.09733712673187256,\n",
       " 0.05362125158309938,\n",
       " 0.04663064479827883,\n",
       " 0.390091655254364,\n",
       " 0.02382377862930296,\n",
       " 0.019401061534881636,\n",
       " 0.05297841787338253,\n",
       " 0.28597369909286496,\n",
       " 0.11785177946090697,\n",
       " 0.18164718866348262,\n",
       " 0.181535901427269,\n",
       " 0.06772000551223756,\n",
       " 0.23556771755218506,\n",
       " 0.25263554096221924,\n",
       " 0.07289501190185549,\n",
       " 0.04342621564865112,\n",
       " 0.06279459714889524,\n",
       " 0.020602140426635773,\n",
       " 0.030703463554382304,\n",
       " 0.007181842327117938,\n",
       " 0.20644841194152833,\n",
       " 0.07309173285961151,\n",
       " 0.037393419742584255,\n",
       " 0.14258939981460572,\n",
       " 0.006248257160186732,\n",
       " 0.022666322588920595,\n",
       " 0.016304688453674276,\n",
       " 0.08595544099807739,\n",
       " 0.09189387023448944,\n",
       " 0.36202455043792725,\n",
       " 0.025001013278961204,\n",
       " 0.07136438846588133,\n",
       " 0.019881083965301527,\n",
       " 0.012768745422363281,\n",
       " 0.011485874652862549,\n",
       " 0.03986101865768432,\n",
       " 0.011914826631546027,\n",
       " 0.3689083337783814,\n",
       " 0.11109375953674316,\n",
       " 0.15944242000579834,\n",
       " 0.08288450002670289,\n",
       " 0.1511139345169067,\n",
       " 0.0019282364845275568,\n",
       " 0.18127232789993286,\n",
       " 0.060563504695892334,\n",
       " 0.08479257822036745,\n",
       " 0.0049750328063964955,\n",
       " 0.05765395402908324,\n",
       " 0.0406542801856995,\n",
       " 0.1298239326477051,\n",
       " 0.14318575382232668,\n",
       " 0.05682285070419313,\n",
       " 0.0725938081741333,\n",
       " 0.032500035762786816,\n",
       " 0.12010369777679442,\n",
       " 0.3846725273132324,\n",
       " 0.11846796751022337,\n",
       " 0.1790027618408203,\n",
       " 0.15869890928268438,\n",
       " 0.08339736700057987,\n",
       " 0.0560663002729416,\n",
       " 0.25806913137435916,\n",
       " 0.16649345636367796,\n",
       " 0.2907222151756287,\n",
       " 0.05091831445693973,\n",
       " 0.11781879186630251,\n",
       " 0.07973542928695676,\n",
       " 0.21544075250625605,\n",
       " 0.12660300731658936,\n",
       " 0.12394049167633059,\n",
       " 0.01660422086715696,\n",
       " 0.07307352304458614,\n",
       " 0.01522836446762088,\n",
       " 0.185943775177002,\n",
       " 0.08654534816741943,\n",
       " 0.09111148834228516,\n",
       " 0.029074091911315936,\n",
       " 0.24769272804260256,\n",
       " 0.09367448329925537,\n",
       " 0.1791719937324524,\n",
       " 0.01734344482421879,\n",
       " 0.18346649408340454,\n",
       " 0.07895687341690061,\n",
       " 0.08145546436309814,\n",
       " 0.10903533697128298,\n",
       " 0.1245786386728287,\n",
       " 0.10083188533782961,\n",
       " 0.015551567077636719,\n",
       " 0.2223998737335205,\n",
       " 0.04008176565170285,\n",
       " 0.05499216109514238,\n",
       " 0.04546573877334592,\n",
       " 0.052565133571624734,\n",
       " 0.29475273370742794,\n",
       " 0.14865311384201052,\n",
       " 0.18663611173629757,\n",
       " 0.09344794273376464,\n",
       " 0.03417299509048466,\n",
       " 0.06454472303390502,\n",
       " 0.003269928693771351,\n",
       " 0.18428193330764775,\n",
       " 0.04178546190261845,\n",
       " 0.0748644471168518,\n",
       " 0.010733067989349365,\n",
       " 0.009370758533477797,\n",
       " 0.08424520015716552,\n",
       " 0.07003037929534911,\n",
       " 0.159811201095581,\n",
       " 0.1182519793510437,\n",
       " 0.049157304763793985,\n",
       " 0.08366391420364383,\n",
       " 0.34818129301071166,\n",
       " 0.20512640476226807,\n",
       " 0.0008164572715759766,\n",
       " 0.011851584911346413,\n",
       " 0.2954905462265015,\n",
       " 0.06624346971511841,\n",
       " 0.06880885601043696,\n",
       " 0.02616965293884277,\n",
       " 0.027325532436370814,\n",
       " 0.01095003128051758,\n",
       " 0.1429055595397949,\n",
       " 0.16788922548294066,\n",
       " 0.1646577429771423,\n",
       " 0.005009293556213379,\n",
       " 0.16774929761886592,\n",
       " 0.08019338607788085,\n",
       " 0.09255984544754026,\n",
       " 0.0807272863388061,\n",
       " 0.027386631965637176,\n",
       " 0.07038448333740233,\n",
       " 0.15084034204483032,\n",
       " 0.09161937236785889,\n",
       " 0.08831848382949825,\n",
       " 0.030417311191558882,\n",
       " 0.15297988176345823,\n",
       " 0.1094316840171814,\n",
       " 0.006140213012695295,\n",
       " 0.00930073976516721,\n",
       " 0.4603433609008789,\n",
       " 0.0741603922843933,\n",
       " 0.07205627650022507,\n",
       " 0.03309893608093262,\n",
       " 0.004130654335021966,\n",
       " 0.008336260318756117,\n",
       " 0.21176910400390625,\n",
       " 0.2883704614639282,\n",
       " 0.2918148064613342,\n",
       " 0.13448127985000613,\n",
       " 0.07246321439743042,\n",
       " 0.1520965218544006,\n",
       " 0.0728377103805542,\n",
       " 0.00910540819168093,\n",
       " 0.3622902822494507,\n",
       " 0.8005406498908997,\n",
       " 0.03858104467391965,\n",
       " 0.09889315605163573,\n",
       " 0.05251891136169429,\n",
       " 0.15762768745422362,\n",
       " 0.007519543170928955,\n",
       " 0.06178797721862794,\n",
       " 0.281859610080719,\n",
       " 0.15876923799514775,\n",
       " 0.04276596546173095,\n",
       " 0.026815149784088166,\n",
       " 0.15748121976852414,\n",
       " 0.0992598700523376,\n",
       " 0.13267302751541132,\n",
       " 0.019797129631042465,\n",
       " 0.27538301229476925,\n",
       " 0.07485336065292358,\n",
       " 0.17140125274658202,\n",
       " 0.20599942445755004,\n",
       " 0.10821890830993652,\n",
       " 0.04612340211868282,\n",
       " 0.1300098156929016,\n",
       " 0.032537996768951416,\n",
       " 0.19472652912139887,\n",
       " 0.10315024137496953,\n",
       " 0.44151551961898805,\n",
       " 0.07273100852966308,\n",
       " 0.128796124458313,\n",
       " 0.2060787487030029,\n",
       " 0.2837761926651001,\n",
       " 0.11882912158966064,\n",
       " 0.017433643341064453,\n",
       " 0.10376703262329101,\n",
       " 0.036322057247161865,\n",
       " 0.09184971094131467,\n",
       " 0.06055277109146118,\n",
       " 0.1216745758056641,\n",
       " 0.12441619992256164,\n",
       " 0.10663002729415894,\n",
       " 0.027983129024505615,\n",
       " 0.05474961519241328,\n",
       " 0.07461061000823976,\n",
       " 0.019604728221893297,\n",
       " 0.0035315012931823597,\n",
       " 0.050886542797088596,\n",
       " 0.12116728067398075,\n",
       " 0.002132814228534702,\n",
       " 0.050715920925140434,\n",
       " 0.10651794672012327,\n",
       " 0.15165339469909667,\n",
       " 0.012661340236663765,\n",
       " 0.26916214227676394,\n",
       " 0.3680022859573364,\n",
       " 0.0683784198760986,\n",
       " 0.006213040351867649,\n",
       " 0.15029221773147583,\n",
       " 0.11964596986770626,\n",
       " 0.027586507797241255,\n",
       " 0.03429231405258182,\n",
       " 0.45858150720596313,\n",
       " 0.1569758015871048,\n",
       " 0.1586760473251343,\n",
       " 0.022938307523727414,\n",
       " 0.09000516891479493,\n",
       " 0.044190635681152335,\n",
       " 0.06912513732910158,\n",
       " 0.0740393137931824,\n",
       " 0.12893085002899174,\n",
       " 0.18205697059631348,\n",
       " 0.037610049247741695,\n",
       " 0.22537706136703495,\n",
       " 0.049412548542022705,\n",
       " 0.15993369698524473,\n",
       " 0.13368674516677856,\n",
       " 0.08429259061813354,\n",
       " 0.011231918334960955,\n",
       " 0.1705404019355774,\n",
       " 0.08592813968658447,\n",
       " 0.06904744386672979,\n",
       " 0.043505086898803724,\n",
       " 0.3365740132331848,\n",
       " 0.06346766948699956,\n",
       " 0.06355557918548582,\n",
       " 0.04681041002273556,\n",
       " 0.24773414850234987,\n",
       " 0.6093688201904297,\n",
       " 0.08829015016555786,\n",
       " 0.03987487554550173,\n",
       " 0.048815090656280535,\n",
       " 0.0635952734947205,\n",
       " 0.03951754093170168,\n",
       " 0.1808125615119934,\n",
       " 0.20921807050704955,\n",
       " 0.0188218867778778,\n",
       " 0.127441987991333,\n",
       " 0.0678999042510986,\n",
       " 0.008253734111785871,\n",
       " 0.015101585388183625,\n",
       " 0.13972486019134522,\n",
       " 0.1124629759788513,\n",
       " 0.10014441251754758,\n",
       " 0.0941591715812683,\n",
       " 0.10053980112075811,\n",
       " 0.2305309247970581,\n",
       " 0.025845162272453315,\n",
       " 0.10717509746551512,\n",
       " 0.05664172410964963,\n",
       " 0.17757536649703975,\n",
       " 0.1917333388328552,\n",
       " 0.0874861478805542,\n",
       " 0.17226974010467527,\n",
       " 0.05028897523880005,\n",
       " 0.0038345313072204457,\n",
       " 0.06715437889099118,\n",
       " 0.044705343246459917,\n",
       " 0.05199050426483154,\n",
       " 0.016891086101532005,\n",
       " 0.0417224621772766,\n",
       " 0.1880244469642639,\n",
       " 0.05778557062149048,\n",
       " 0.008445110321044919,\n",
       " 0.07936523437499998,\n",
       " 0.0314035701751709,\n",
       " 0.010395686626434308,\n",
       " 0.0026702284812927246,\n",
       " 0.07724293947219849,\n",
       " 0.08149816036224367,\n",
       " 0.05313747644424438,\n",
       " 0.06246868371963499,\n",
       " 0.14945392847061156,\n",
       " 0.06527458190917967,\n",
       " 0.26614463090896606,\n",
       " 0.06639392614364625,\n",
       " 0.015494866371154825,\n",
       " 0.057851328849792494,\n",
       " 0.00774151086807251,\n",
       " 0.016904664039611772,\n",
       " 0.023076503276824978,\n",
       " 0.07838444232940672,\n",
       " 0.05926099538803098,\n",
       " 0.029744219779968273,\n",
       " 0.05839224815368654,\n",
       " 0.12363618612289429,\n",
       " 0.1986940264701843,\n",
       " 0.17475151538848877,\n",
       " 0.04893342971801762,\n",
       " 0.011973588466644314,\n",
       " 0.2527477955818176,\n",
       " 0.3191333532333374,\n",
       " 0.29176880598068233,\n",
       " 0.23842228651046754,\n",
       " 0.048958470821380606,\n",
       " 0.18313462495803834,\n",
       " 0.2909666442871094,\n",
       " 0.07181513071060186,\n",
       " 0.1394313549995423,\n",
       " 0.2380230093002319,\n",
       " 0.004056081771850573,\n",
       " 0.013866155147552517,\n",
       " 0.04163057088851929,\n",
       " 0.05819543123245241,\n",
       " 0.1243859457969666,\n",
       " 0.11123255252838138,\n",
       " 0.13899796962738042]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#err = y_test - y_pred\n",
    "err = [abs(e1 - e2) for e1, e2 in zip(y_test,y_pred)]\n",
    "err = np.concatenate(err).tolist()\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.37, 0.12319480895996093),\n",
       " (0.53, 0.0855930161476135),\n",
       " (0.75, 0.17742407321929932),\n",
       " (1.0, 0.2621017098426819),\n",
       " (0.51, 0.018894622325897226),\n",
       " (0.44, 0.12994957208633423),\n",
       " (1.0, 0.24084430932998657),\n",
       " (0.14, 0.21281258821487425),\n",
       " (0.89, 0.053956673145294176),\n",
       " (0.37, 0.24306630611419677),\n",
       " (0.33, 0.0896440613269806),\n",
       " (0.06, 0.09719057619571686),\n",
       " (0.57, 0.010502331256866504),\n",
       " (0.54, 0.1838458395004272),\n",
       " (0.78, 0.1555537271499634),\n",
       " (0.78, 0.11140492677688596),\n",
       " (1.0, 0.012948751449584961),\n",
       " (0.65, 0.1264669060707092),\n",
       " (0.88, 0.1806485104560852),\n",
       " (1.0, 0.09733712673187256),\n",
       " (0.77, 0.05362125158309938),\n",
       " (0.6, 0.04663064479827883),\n",
       " (0.44, 0.390091655254364),\n",
       " (0.98, 0.02382377862930296),\n",
       " (0.8, 0.019401061534881636),\n",
       " (0.54, 0.05297841787338253),\n",
       " (0.53, 0.28597369909286496),\n",
       " (0.64, 0.11785177946090697),\n",
       " (0.82, 0.18164718866348262),\n",
       " (0.33, 0.181535901427269),\n",
       " (0.76, 0.06772000551223756),\n",
       " (0.22, 0.23556771755218506),\n",
       " (0.31, 0.25263554096221924),\n",
       " (0.29, 0.07289501190185549),\n",
       " (1.0, 0.04342621564865112),\n",
       " (0.78, 0.06279459714889524),\n",
       " (0.84, 0.020602140426635773),\n",
       " (0.29, 0.030703463554382304),\n",
       " (0.48, 0.007181842327117938),\n",
       " (0.15, 0.20644841194152833),\n",
       " (0.19, 0.07309173285961151),\n",
       " (0.78, 0.037393419742584255),\n",
       " (0.76, 0.14258939981460572),\n",
       " (0.79, 0.006248257160186732),\n",
       " (0.19, 0.022666322588920595),\n",
       " (0.58, 0.016304688453674276),\n",
       " (0.5, 0.08595544099807739),\n",
       " (0.09, 0.09189387023448944),\n",
       " (0.87, 0.36202455043792725),\n",
       " (0.4, 0.025001013278961204),\n",
       " (0.39, 0.07136438846588133),\n",
       " (0.61, 0.019881083965301527),\n",
       " (1.0, 0.012768745422363281),\n",
       " (1.0, 0.011485874652862549),\n",
       " (0.13, 0.03986101865768432),\n",
       " (0.32, 0.011914826631546027),\n",
       " (0.7, 0.3689083337783814),\n",
       " (1.0, 0.11109375953674316),\n",
       " (0.63, 0.15944242000579834),\n",
       " (0.74, 0.08288450002670289),\n",
       " (0.82, 0.1511139345169067),\n",
       " (0.91, 0.0019282364845275568),\n",
       " (1.0, 0.18127232789993286),\n",
       " (1.0, 0.060563504695892334),\n",
       " (0.6, 0.08479257822036745),\n",
       " (0.45, 0.0049750328063964955),\n",
       " (0.76, 0.05765395402908324),\n",
       " (0.71, 0.0406542801856995),\n",
       " (0.76, 0.1298239326477051),\n",
       " (0.73, 0.14318575382232668),\n",
       " (0.89, 0.05682285070419313),\n",
       " (1.0, 0.0725938081741333),\n",
       " (0.57, 0.032500035762786816),\n",
       " (0.52, 0.12010369777679442),\n",
       " (0.88, 0.3846725273132324),\n",
       " (0.77, 0.11846796751022337),\n",
       " (1.0, 0.1790027618408203),\n",
       " (0.68, 0.15869890928268438),\n",
       " (0.79, 0.08339736700057987),\n",
       " (0.23, 0.0560663002729416),\n",
       " (0.59, 0.25806913137435916),\n",
       " (0.73, 0.16649345636367796),\n",
       " (0.9, 0.2907222151756287),\n",
       " (0.59, 0.05091831445693973),\n",
       " (0.73, 0.11781879186630251),\n",
       " (0.47, 0.07973542928695676),\n",
       " (0.69, 0.21544075250625605),\n",
       " (1.0, 0.12660300731658936),\n",
       " (0.6, 0.12394049167633059),\n",
       " (0.9, 0.01660422086715696),\n",
       " (0.67, 0.07307352304458614),\n",
       " (0.59, 0.01522836446762088),\n",
       " (0.57, 0.185943775177002),\n",
       " (0.5, 0.08654534816741943),\n",
       " (0.18, 0.09111148834228516),\n",
       " (0.52, 0.029074091911315936),\n",
       " (0.9, 0.24769272804260256),\n",
       " (0.31, 0.09367448329925537),\n",
       " (0.61, 0.1791719937324524),\n",
       " (0.83, 0.01734344482421879),\n",
       " (1.0, 0.18346649408340454),\n",
       " (0.97, 0.07895687341690061),\n",
       " (0.62, 0.08145546436309814),\n",
       " (0.9, 0.10903533697128298),\n",
       " (0.28, 0.1245786386728287),\n",
       " (0.73, 0.10083188533782961),\n",
       " (1.0, 0.015551567077636719),\n",
       " (0.33, 0.2223998737335205),\n",
       " (0.66, 0.04008176565170285),\n",
       " (0.14, 0.05499216109514238),\n",
       " (0.78, 0.04546573877334592),\n",
       " (0.85, 0.052565133571624734),\n",
       " (0.67, 0.29475273370742794),\n",
       " (0.6, 0.14865311384201052),\n",
       " (0.96, 0.18663611173629757),\n",
       " (0.23, 0.09344794273376464),\n",
       " (0.58, 0.03417299509048466),\n",
       " (0.64, 0.06454472303390502),\n",
       " (0.3, 0.003269928693771351),\n",
       " (0.8, 0.18428193330764775),\n",
       " (0.92, 0.04178546190261845),\n",
       " (1.0, 0.0748644471168518),\n",
       " (1.0, 0.010733067989349365),\n",
       " (0.86, 0.009370758533477797),\n",
       " (0.87, 0.08424520015716552),\n",
       " (0.45, 0.07003037929534911),\n",
       " (0.81, 0.159811201095581),\n",
       " (0.25, 0.1182519793510437),\n",
       " (0.67, 0.049157304763793985),\n",
       " (0.91, 0.08366391420364383),\n",
       " (0.64, 0.34818129301071166),\n",
       " (0.75, 0.20512640476226807),\n",
       " (0.82, 0.0008164572715759766),\n",
       " (0.85, 0.011851584911346413),\n",
       " (0.92, 0.2954905462265015),\n",
       " (1.0, 0.06624346971511841),\n",
       " (0.69, 0.06880885601043696),\n",
       " (0.88, 0.02616965293884277),\n",
       " (0.71, 0.027325532436370814),\n",
       " (0.31, 0.01095003128051758),\n",
       " (0.66, 0.1429055595397949),\n",
       " (0.45, 0.16788922548294066),\n",
       " (0.78, 0.1646577429771423),\n",
       " (1.0, 0.005009293556213379),\n",
       " (0.55, 0.16774929761886592),\n",
       " (0.61, 0.08019338607788085),\n",
       " (0.78, 0.09255984544754026),\n",
       " (0.93, 0.0807272863388061),\n",
       " (0.59, 0.027386631965637176),\n",
       " (0.52, 0.07038448333740233),\n",
       " (1.0, 0.15084034204483032),\n",
       " (1.0, 0.09161937236785889),\n",
       " (0.83, 0.08831848382949825),\n",
       " (0.55, 0.030417311191558882),\n",
       " (0.78, 0.15297988176345823),\n",
       " (1.0, 0.1094316840171814),\n",
       " (0.77, 0.006140213012695295),\n",
       " (0.72, 0.00930073976516721),\n",
       " (1.0, 0.4603433609008789),\n",
       " (0.88, 0.0741603922843933),\n",
       " (0.01, 0.07205627650022507),\n",
       " (1.0, 0.03309893608093262),\n",
       " (0.32, 0.004130654335021966),\n",
       " (0.86, 0.008336260318756117),\n",
       " (0.5, 0.21176910400390625),\n",
       " (0.97, 0.2883704614639282),\n",
       " (0.69, 0.2918148064613342),\n",
       " (0.72, 0.13448127985000613),\n",
       " (1.0, 0.07246321439743042),\n",
       " (0.55, 0.1520965218544006),\n",
       " (0.75, 0.0728377103805542),\n",
       " (0.9, 0.00910540819168093),\n",
       " (0.48, 0.3622902822494507),\n",
       " (0.1, 0.8005406498908997),\n",
       " (0.97, 0.03858104467391965),\n",
       " (0.74, 0.09889315605163573),\n",
       " (0.68, 0.05251891136169429),\n",
       " (0.89, 0.15762768745422362),\n",
       " (0.5, 0.007519543170928955),\n",
       " (0.64, 0.06178797721862794),\n",
       " (0.91, 0.281859610080719),\n",
       " (0.8, 0.15876923799514775),\n",
       " (0.76, 0.04276596546173095),\n",
       " (0.84, 0.026815149784088166),\n",
       " (0.66, 0.15748121976852414),\n",
       " (0.67, 0.0992598700523376),\n",
       " (0.94, 0.13267302751541132),\n",
       " (0.33, 0.019797129631042465),\n",
       " (0.67, 0.27538301229476925),\n",
       " (0.5, 0.07485336065292358),\n",
       " (0.99, 0.17140125274658202),\n",
       " (0.26, 0.20599942445755004),\n",
       " (1.0, 0.10821890830993652),\n",
       " (0.67, 0.04612340211868282),\n",
       " (0.64, 0.1300098156929016),\n",
       " (0.75, 0.032537996768951416),\n",
       " (0.81, 0.19472652912139887),\n",
       " (0.93, 0.10315024137496953),\n",
       " (0.36, 0.44151551961898805),\n",
       " (0.74, 0.07273100852966308),\n",
       " (0.35, 0.128796124458313),\n",
       " (0.52, 0.2060787487030029),\n",
       " (0.78, 0.2837761926651001),\n",
       " (0.49, 0.11882912158966064),\n",
       " (1.0, 0.017433643341064453),\n",
       " (0.87, 0.10376703262329101),\n",
       " (1.0, 0.036322057247161865),\n",
       " (0.66, 0.09184971094131467),\n",
       " (0.87, 0.06055277109146118),\n",
       " (0.71, 0.1216745758056641),\n",
       " (0.31, 0.12441619992256164),\n",
       " (1.0, 0.10663002729415894),\n",
       " (1.0, 0.027983129024505615),\n",
       " (0.68, 0.05474961519241328),\n",
       " (0.48, 0.07461061000823976),\n",
       " (0.36, 0.019604728221893297),\n",
       " (0.86, 0.0035315012931823597),\n",
       " (0.97, 0.050886542797088596),\n",
       " (0.71, 0.12116728067398075),\n",
       " (0.09, 0.002132814228534702),\n",
       " (0.81, 0.050715920925140434),\n",
       " (0.6, 0.10651794672012327),\n",
       " (0.86, 0.15165339469909667),\n",
       " (0.69, 0.012661340236663765),\n",
       " (0.1, 0.26916214227676394),\n",
       " (0.74, 0.3680022859573364),\n",
       " (0.78, 0.0683784198760986),\n",
       " (0.47, 0.006213040351867649),\n",
       " (1.0, 0.15029221773147583),\n",
       " (0.83, 0.11964596986770626),\n",
       " (0.55, 0.027586507797241255),\n",
       " (0.59, 0.03429231405258182),\n",
       " (1.0, 0.45858150720596313),\n",
       " (0.02, 0.1569758015871048),\n",
       " (0.48, 0.1586760473251343),\n",
       " (0.16, 0.022938307523727414),\n",
       " (0.74, 0.09000516891479493),\n",
       " (0.49, 0.044190635681152335),\n",
       " (0.29, 0.06912513732910158),\n",
       " (0.84, 0.0740393137931824),\n",
       " (0.83, 0.12893085002899174),\n",
       " (0.18, 0.18205697059631348),\n",
       " (0.87, 0.037610049247741695),\n",
       " (0.79, 0.22537706136703495),\n",
       " (0.5, 0.049412548542022705),\n",
       " (0.48, 0.15993369698524473),\n",
       " (0.1, 0.13368674516677856),\n",
       " (1.0, 0.08429259061813354),\n",
       " (0.48, 0.011231918334960955),\n",
       " (0.74, 0.1705404019355774),\n",
       " (0.44, 0.08592813968658447),\n",
       " (0.93, 0.06904744386672979),\n",
       " (0.64, 0.043505086898803724),\n",
       " (0.63, 0.3365740132331848),\n",
       " (0.8, 0.06346766948699956),\n",
       " (0.52, 0.06355557918548582),\n",
       " (0.54, 0.04681041002273556),\n",
       " (0.52, 0.24773414850234987),\n",
       " (0.48, 0.6093688201904297),\n",
       " (0.63, 0.08829015016555786),\n",
       " (0.9, 0.03987487554550173),\n",
       " (0.52, 0.048815090656280535),\n",
       " (0.54, 0.0635952734947205),\n",
       " (0.48, 0.03951754093170168),\n",
       " (0.1, 0.1808125615119934),\n",
       " (0.61, 0.20921807050704955),\n",
       " (0.21, 0.0188218867778778),\n",
       " (0.61, 0.127441987991333),\n",
       " (0.54, 0.0678999042510986),\n",
       " (0.73, 0.008253734111785871),\n",
       " (0.59, 0.015101585388183625),\n",
       " (0.76, 0.13972486019134522),\n",
       " (0.46, 0.1124629759788513),\n",
       " (0.84, 0.10014441251754758),\n",
       " (0.86, 0.0941591715812683),\n",
       " (0.69, 0.10053980112075811),\n",
       " (0.18, 0.2305309247970581),\n",
       " (0.18, 0.025845162272453315),\n",
       " (0.64, 0.10717509746551512),\n",
       " (0.66, 0.05664172410964963),\n",
       " (0.67, 0.17757536649703975),\n",
       " (0.46, 0.1917333388328552),\n",
       " (1.0, 0.0874861478805542),\n",
       " (0.46, 0.17226974010467527),\n",
       " (0.5, 0.05028897523880005),\n",
       " (0.86, 0.0038345313072204457),\n",
       " (0.72, 0.06715437889099118),\n",
       " (0.7, 0.044705343246459917),\n",
       " (0.37, 0.05199050426483154),\n",
       " (0.85, 0.016891086101532005),\n",
       " (0.86, 0.0417224621772766),\n",
       " (0.64, 0.1880244469642639),\n",
       " (1.0, 0.05778557062149048),\n",
       " (0.09, 0.008445110321044919),\n",
       " (0.77, 0.07936523437499998),\n",
       " (0.22, 0.0314035701751709),\n",
       " (0.77, 0.010395686626434308),\n",
       " (1.0, 0.0026702284812927246),\n",
       " (0.22, 0.07724293947219849),\n",
       " (0.36, 0.08149816036224367),\n",
       " (0.88, 0.05313747644424438),\n",
       " (0.85, 0.06246868371963499),\n",
       " (0.26, 0.14945392847061156),\n",
       " (0.39, 0.06527458190917967),\n",
       " (0.44, 0.26614463090896606),\n",
       " (0.61, 0.06639392614364625),\n",
       " (0.58, 0.015494866371154825),\n",
       " (0.36, 0.057851328849792494),\n",
       " (0.75, 0.00774151086807251),\n",
       " (0.8, 0.016904664039611772),\n",
       " (0.78, 0.023076503276824978),\n",
       " (0.33, 0.07838444232940672),\n",
       " (0.78, 0.05926099538803098),\n",
       " (0.45, 0.029744219779968273),\n",
       " (0.46, 0.05839224815368654),\n",
       " (1.0, 0.12363618612289429),\n",
       " (0.85, 0.1986940264701843),\n",
       " (0.88, 0.17475151538848877),\n",
       " (0.92, 0.04893342971801762),\n",
       " (0.78, 0.011973588466644314),\n",
       " (0.71, 0.2527477955818176),\n",
       " (0.05, 0.3191333532333374),\n",
       " (0.83, 0.29176880598068233),\n",
       " (0.3, 0.23842228651046754),\n",
       " (0.51, 0.048958470821380606),\n",
       " (0.74, 0.18313462495803834),\n",
       " (0.86, 0.2909666442871094),\n",
       " (0.69, 0.07181513071060186),\n",
       " (0.69, 0.1394313549995423),\n",
       " (0.66, 0.2380230093002319),\n",
       " (0.61, 0.004056081771850573),\n",
       " (0.97, 0.013866155147552517),\n",
       " (0.88, 0.04163057088851929),\n",
       " (0.48, 0.05819543123245241),\n",
       " (0.93, 0.1243859457969666),\n",
       " (0.66, 0.11123255252838138),\n",
       " (0.69, 0.13899796962738042)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(y_testi,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*zip(*list(zip(y_testi,err))))\n",
    "plt.ylabel('Absolute Prediction Error')\n",
    "plt.xlabel('Real Mutation Score')\n",
    "#plt.show()\n",
    "#plt.show()\n",
    "plt.savefig(\"foo.pdf\", bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert type 'ndarray' to numerator/denominator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-5f3c482d8cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean requires at least one data point'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(data, start)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_coerce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or raise TypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_exact_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mpartials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartials_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36m_exact_ratio\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"can't convert type '{}' to numerator/denominator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert type 'ndarray' to numerator/denominator"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "err = [abs(e1 - e2) for e1, e2 in zip(y_test,y_pred)]\n",
    "statistics.mean(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "#frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "#           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "#           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "frame = load_frame()\n",
    "\n",
    "\n",
    "x1 = frame['mutation']#[12, 2, 1, 12, 2]\n",
    "x2 = frame['TryCatch'] #[1, 4, 7, 1, 0]\n",
    "\n",
    "#x1 = [12, 1, 2, 12, 2]\n",
    "#x2 = [1, 7, 4, 1, 0]\n",
    "\n",
    "tau, p_value = stats.kendalltau(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08356339374825926"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3212083462945562e-07"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = load_frame_with_projects()\n",
    "#pd.unique(frame['project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>isAssertionRoulette</th>\n",
       "      <th>isEagerTest</th>\n",
       "      <th>isLazyTest</th>\n",
       "      <th>isMysteryGuest</th>\n",
       "      <th>isSensitiveEquality</th>\n",
       "      <th>isResourceOptimism</th>\n",
       "      <th>...</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>55</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>javapoet</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>220</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>21364.0</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RxJava</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>6181.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>111</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41722.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>RxJava</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5005.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>104</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6814.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>junit4</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>31</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>93</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9227.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.698462</td>\n",
       "      <td>325</td>\n",
       "      <td>0.941748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>3017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11829.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           project  mutation  no_mutations  line_coverage  \\\n",
       "0     commons-math  0.690909            55       0.875000   \n",
       "1         javapoet  0.372727           220       0.500000   \n",
       "2     commons-math  0.830769           130       0.900000   \n",
       "3           RxJava  1.000000             5       1.000000   \n",
       "4     commons-math  0.738739           111       0.750000   \n",
       "...            ...       ...           ...            ...   \n",
       "2240        RxJava  0.875000             8       1.000000   \n",
       "2241  commons-math  0.865385           104       0.954545   \n",
       "2242        junit4  0.806452            31       0.695652   \n",
       "2243  commons-math  0.827957            93       0.903226   \n",
       "2244  commons-math  0.698462           325       0.941748   \n",
       "\n",
       "      isAssertionRoulette  isEagerTest  isLazyTest  isMysteryGuest  \\\n",
       "0                       1            0           0               0   \n",
       "1                       0            1           0               0   \n",
       "2                       0            0           0               0   \n",
       "3                       1            0           0               0   \n",
       "4                       1            1           0               0   \n",
       "...                   ...          ...         ...             ...   \n",
       "2240                    1            0           0               0   \n",
       "2241                    1            0           0               0   \n",
       "2242                    1            0           0               0   \n",
       "2243                    1            1           0               1   \n",
       "2244                    0            0           0               0   \n",
       "\n",
       "      isSensitiveEquality  isResourceOptimism  ...  Mockito  BadApi    LOC  \\\n",
       "0                       0                   0  ...      0.0     0.0   41.0   \n",
       "1                       1                   0  ...      0.0     0.0  222.0   \n",
       "2                       0                   0  ...      0.0     0.0  249.0   \n",
       "3                       0                   0  ...      0.0     0.0  164.0   \n",
       "4                       0                   0  ...      0.0     0.0  176.0   \n",
       "...                   ...                 ...  ...      ...     ...    ...   \n",
       "2240                    1                   0  ...      0.0     0.0  180.0   \n",
       "2241                    0                   0  ...      0.0     0.0  205.0   \n",
       "2242                    0                   0  ...      0.0     0.0  185.0   \n",
       "2243                    0                   0  ...      0.0     0.0  319.0   \n",
       "2244                    0                   0  ...      0.0     0.0  769.0   \n",
       "\n",
       "      Expressions  Depth  Vocabulary  Understandability  BodySize  Dexterity  \\\n",
       "0           121.0    9.0        17.0             1118.0     156.0        2.0   \n",
       "1          1807.0   26.0       101.0            21364.0    1889.0        2.0   \n",
       "2           587.0   21.0        36.0             7056.0     847.0        3.0   \n",
       "3           440.0   16.0        63.0             6181.0     673.0        2.0   \n",
       "4           850.0   12.0        53.0            41722.0     593.0        2.0   \n",
       "...           ...    ...         ...                ...       ...        ...   \n",
       "2240        451.0   15.0        58.0             5005.0     577.0        2.0   \n",
       "2241        540.0   13.0        44.0             6814.0     867.0        2.0   \n",
       "2242        514.0   13.0        77.0             5271.0     470.0        3.0   \n",
       "2243        858.0   15.0        72.0             9227.0    1107.0        3.0   \n",
       "2244       2061.0   16.0        56.0            28605.0    3017.0        3.0   \n",
       "\n",
       "      NonWhiteCharacters  \n",
       "0                  937.0  \n",
       "1                16425.0  \n",
       "2                 3503.0  \n",
       "3                 3149.0  \n",
       "4                 6628.0  \n",
       "...                  ...  \n",
       "2240              2807.0  \n",
       "2241              3430.0  \n",
       "2242              4317.0  \n",
       "2243              5789.0  \n",
       "2244             11829.0  \n",
       "\n",
       "[2243 rows x 85 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>isAssertionRoulette</th>\n",
       "      <th>isEagerTest</th>\n",
       "      <th>isLazyTest</th>\n",
       "      <th>isMysteryGuest</th>\n",
       "      <th>isSensitiveEquality</th>\n",
       "      <th>isResourceOptimism</th>\n",
       "      <th>...</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>55</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>220</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>21364.0</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>6181.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>111</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41722.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>8</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5005.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>4</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>104</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6814.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>13</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>31</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>4</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>93</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9227.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>4</td>\n",
       "      <td>0.698462</td>\n",
       "      <td>325</td>\n",
       "      <td>0.941748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>3017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11829.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      project  mutation  no_mutations  line_coverage  isAssertionRoulette  \\\n",
       "0           4  0.690909            55       0.875000                    1   \n",
       "1           1  0.372727           220       0.500000                    0   \n",
       "2           4  0.830769           130       0.900000                    0   \n",
       "3           8  1.000000             5       1.000000                    1   \n",
       "4           4  0.738739           111       0.750000                    1   \n",
       "...       ...       ...           ...            ...                  ...   \n",
       "2240        8  0.875000             8       1.000000                    1   \n",
       "2241        4  0.865385           104       0.954545                    1   \n",
       "2242       13  0.806452            31       0.695652                    1   \n",
       "2243        4  0.827957            93       0.903226                    1   \n",
       "2244        4  0.698462           325       0.941748                    0   \n",
       "\n",
       "      isEagerTest  isLazyTest  isMysteryGuest  isSensitiveEquality  \\\n",
       "0               0           0               0                    0   \n",
       "1               1           0               0                    1   \n",
       "2               0           0               0                    0   \n",
       "3               0           0               0                    0   \n",
       "4               1           0               0                    0   \n",
       "...           ...         ...             ...                  ...   \n",
       "2240            0           0               0                    1   \n",
       "2241            0           0               0                    0   \n",
       "2242            0           0               0                    0   \n",
       "2243            1           0               1                    0   \n",
       "2244            0           0               0                    0   \n",
       "\n",
       "      isResourceOptimism  ...  Mockito  BadApi    LOC  Expressions  Depth  \\\n",
       "0                      0  ...      0.0     0.0   41.0        121.0    9.0   \n",
       "1                      0  ...      0.0     0.0  222.0       1807.0   26.0   \n",
       "2                      0  ...      0.0     0.0  249.0        587.0   21.0   \n",
       "3                      0  ...      0.0     0.0  164.0        440.0   16.0   \n",
       "4                      0  ...      0.0     0.0  176.0        850.0   12.0   \n",
       "...                  ...  ...      ...     ...    ...          ...    ...   \n",
       "2240                   0  ...      0.0     0.0  180.0        451.0   15.0   \n",
       "2241                   0  ...      0.0     0.0  205.0        540.0   13.0   \n",
       "2242                   0  ...      0.0     0.0  185.0        514.0   13.0   \n",
       "2243                   0  ...      0.0     0.0  319.0        858.0   15.0   \n",
       "2244                   0  ...      0.0     0.0  769.0       2061.0   16.0   \n",
       "\n",
       "      Vocabulary  Understandability  BodySize  Dexterity  NonWhiteCharacters  \n",
       "0           17.0             1118.0     156.0        2.0               937.0  \n",
       "1          101.0            21364.0    1889.0        2.0             16425.0  \n",
       "2           36.0             7056.0     847.0        3.0              3503.0  \n",
       "3           63.0             6181.0     673.0        2.0              3149.0  \n",
       "4           53.0            41722.0     593.0        2.0              6628.0  \n",
       "...          ...                ...       ...        ...                 ...  \n",
       "2240        58.0             5005.0     577.0        2.0              2807.0  \n",
       "2241        44.0             6814.0     867.0        2.0              3430.0  \n",
       "2242        77.0             5271.0     470.0        3.0              4317.0  \n",
       "2243        72.0             9227.0    1107.0        3.0              5789.0  \n",
       "2244        56.0            28605.0    3017.0        3.0             11829.0  \n",
       "\n",
       "[2243 rows x 85 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project commons-lang, tau: 0.01742127854436364, p: 0.7873241936854231\n",
      "project javapoet, tau: 0.19767441860465115, p: 0.3439053279944314\n",
      "project commons-io, tau: -0.031482587918892135, p: 0.7301967773648329\n",
      "project jfreechart, tau: 0.2338729936316801, p: 9.901299820714473e-10\n",
      "project commons-math, tau: 0.009994365652805792, p: 0.7750782099346211\n",
      "project opengrok, tau: 0.09178657790594506, p: 0.14587429514478942\n",
      "project checkstyle, tau: -0.030041674231558454, p: 0.5174776973820301\n",
      "project closure-compiler, tau: -0.281426531784519, p: 6.407863679488957e-10\n",
      "project RxJava, tau: -0.05124484535057572, p: 0.14118719938920765\n",
      "project fastjson, tau: -0.12980352659818656, p: 0.1630773518413966\n",
      "project cat, tau: 0.20029544520870746, p: 0.030626784063831295\n",
      "project joda-beans, tau: 0.4112329130008325, p: 0.08393637367791504\n",
      "project commons-collections, tau: -0.027573590260267383, p: 0.7904296853624124\n",
      "project junit4, tau: 0.046554553461689845, p: 0.6683826688503443\n",
      "project gson, tau: 0.912870929175277, p: 0.07095149242730567\n",
      "project jsoup, tau: 0.27106408958257083, p: 0.06592890401949764\n",
      "project guice, tau: 0.2222222222222222, p: 0.6027418291290805\n"
     ]
    }
   ],
   "source": [
    "frame = load_frame_with_projects()\n",
    "projects = ['commons-lang', 'javapoet', 'commons-io', 'jfreechart',\n",
    "       'commons-math', 'opengrok', 'checkstyle', 'closure-compiler',\n",
    "       'RxJava', 'fastjson', 'cat', 'joda-beans', 'commons-collections',\n",
    "       'junit4', 'gson', 'jsoup', 'guice']\n",
    "for project in projects:\n",
    "    small_frame = frame[frame['project']==project]\n",
    "    x1 = small_frame['mutation']\n",
    "    x2= small_frame['Assrtions']\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    print('project {}, tau: {}, p: {}'.format(project,tau,p_value))\n",
    "    \n",
    "#['commons-lang', 'javapoet', 'commons-io', 'jfreechart',\n",
    "#       'commons-math', 'opengrok', 'checkstyle', 'closure-compiler',\n",
    "#       'RxJava', 'fastjson', 'cat', 'joda-beans', 'commons-collections',\n",
    "#       'junit4', 'gson', 'jsoup', 'guice']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric Assrtions, tau: -0.069, p: 0.000\n",
      "metric Conditions, tau: 0.081, p: 0.000\n",
      "metric TryCatch, tau: 0.089, p: 0.000\n",
      "metric Loop, tau: 0.153, p: 0.000\n",
      "metric Hamcrest, tau: -0.031, p: 0.084\n",
      "metric Mockito, tau: -0.169, p: 0.000\n",
      "metric BadApi, tau: -0.203, p: 0.000\n",
      "metric LOC, tau: 0.102, p: 0.000\n",
      "metric Expressions, tau: 0.125, p: 0.000\n",
      "metric Depth, tau: 0.224, p: 0.000\n",
      "metric Vocabulary, tau: 0.140, p: 0.000\n",
      "metric Understandability, tau: 0.141, p: 0.000\n",
      "metric BodySize, tau: 0.123, p: 0.000\n",
      "metric Dexterity, tau: -0.136, p: 0.000\n",
      "metric NonWhiteCharacters, tau: 0.116, p: 0.000\n"
     ]
    }
   ],
   "source": [
    "frame = load_frame_with_projects()\n",
    "me = ['Assrtions', 'Conditions', 'TryCatch', 'Loop', 'Hamcrest' , 'Mockito',\n",
    "           'BadApi', 'LOC' ,'Expressions', 'Depth', 'Vocabulary', 'Understandability', 'BodySize'\n",
    "     , 'Dexterity', 'NonWhiteCharacters']\n",
    "\n",
    "labels = [1,2]\n",
    "bins = [0,frame.mutation.median(),1]\n",
    "frame['mutation_bins'] = pd.cut(frame.mutation, bins=bins, labels = labels, include_lowest=True)\n",
    "    \n",
    "for m in me:\n",
    "    x1 = frame['mutation'].round(1) #['mutation_bins']\n",
    "    x2 = frame[m] #/frame['LOC_test']\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    print('metric {}, tau: {:.3f}, p: {:.3f}'.format(m,tau,p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-b3bd6789edde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutation_bins'\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutation_bins'\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "import scipy.stats as s\n",
    "\n",
    "s.ranksums(frame.Loop['mutation_bins'==1],frame.Loop['mutation_bins'==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2240    2\n",
       "2241    1\n",
       "2242    2\n",
       "2243    1\n",
       "2244    2\n",
       "Name: mutation_bins, Length: 2243, dtype: category\n",
       "Categories (2, int64): [1 < 2]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "frame['mutation_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        332\n",
       "1         85\n",
       "2       1168\n",
       "3        132\n",
       "4        120\n",
       "        ... \n",
       "2240      99\n",
       "2241      88\n",
       "2242    1519\n",
       "2243      67\n",
       "2244      38\n",
       "Name: LOC_test, Length: 2243, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['LOC_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.038961\n",
       "1       0.076923\n",
       "2       0.065789\n",
       "3       0.065789\n",
       "4       0.136628\n",
       "          ...   \n",
       "2240    0.144330\n",
       "2241    0.112500\n",
       "2242    0.000000\n",
       "2243    0.000000\n",
       "2244    0.009539\n",
       "Length: 2243, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = load_frame_with_projects()\n",
    "frame['Assrtions']/frame['LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        181.0\n",
       "1        215.0\n",
       "2        398.0\n",
       "3        560.0\n",
       "4        140.0\n",
       "         ...  \n",
       "2240      73.0\n",
       "2241    1077.0\n",
       "2242      70.0\n",
       "2243      50.0\n",
       "2244      27.0\n",
       "Name: LOC, Length: 2243, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = is_checkstyle['mutation']#[12, 2, 1, 12, 2]\n",
    "x2 = is_checkstyle['TryCatch'] #[1, 4, 7, 1, 0]\n",
    "\n",
    "#x1 = [12, 1, 2, 12, 2]\n",
    "#x2 = [1, 7, 4, 1, 0]\n",
    "\n",
    "tau, p_value = stats.kendalltau(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07391248429554852"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05580407629140783"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = pd.read_csv('good_tests.csv', sep=\",\")\n",
    "frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "#frame\n",
    "frame.to_csv ('good_tests_extended.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = pd.read_csv('bad_tests.csv', sep=\",\")\n",
    "frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "frame.to_csv ('bad_tests_extended.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
