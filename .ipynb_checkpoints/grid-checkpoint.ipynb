{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import sklearn\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"complete-frame.csv\"\n",
    "CSV_MINER_PATH = \"testminereffectiveness.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_rename (row):\n",
    "    return row['path_test'].split('/')[len(row['path_test'].split('/')) - 1].split('.')[0]\n",
    "\n",
    "def load_frame():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['project', 'module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','Nº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "def load_frame_with_projects():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','Nº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def load_static_frame():\n",
    "    frame1 = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "    frame1 = frame1.sample(frac=1).reset_index(drop=True)\n",
    "    frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "    frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "    frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "    frame = frame.drop(['project', 'module', 'path_test','test_name','path_src',\n",
    "                        'class_name','TestClassName','commit','Nº','Project'], axis=1)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    frame = frame.dropna()\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def load_quartile(frame):\n",
    "    low, high = frame.mutation.quantile([0.25,0.75])\n",
    "    frame_low = frame.query('mutation<{low}'.format(low=low))\n",
    "    frame_high = frame.query('mutation>{high}'.format(high=high))\n",
    "    frame_low['mutation'] = 0\n",
    "    frame_high['mutation'] = 1\n",
    "    frame = pd.concat([frame_low, frame_high], ignore_index=True)\n",
    "    frame = frame.sample(frac=1).reset_index(drop=True)\n",
    "    return frame;\n",
    "\n",
    "def load_meaningful_subset(frame):\n",
    "    columns = [frame.no_mutations,\n",
    "                         frame.line_coverage,\n",
    "                         frame.csm_FE,\n",
    "                         frame.CONNECTIVITY_prod,\n",
    "                         frame.CONNECTIVITY_test,\n",
    "                         frame.isEagerTest,\n",
    "                         frame.LOC_prod, frame.LOC_test, frame.WMC_prod,\n",
    "                         frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "                         frame.LCOM4_prod, frame.McCABE_prod,\n",
    "                         frame.RFC_prod, frame.MPC_prod,\n",
    "                         frame.RFC_test, frame.MPC_test,\n",
    "                         frame.LCOM1_test, frame.LCOM2_test,\n",
    "                         frame.LCOM4_test, frame.LCC_test,\n",
    "                         frame.LCC_test, frame.WMC_test,\n",
    "                         frame.McCABE_test, frame.NOP_prod]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "def load_meaningful_subset_static(frame):\n",
    "    columns = [frame.no_mutations,\n",
    "                         frame.csm_FE,\n",
    "                         frame.CONNECTIVITY_prod,\n",
    "                         frame.CONNECTIVITY_test,\n",
    "                         frame.isEagerTest,\n",
    "                         frame.LOC_prod, frame.LOC_test, frame.WMC_prod,\n",
    "                         frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "                         frame.LCOM4_prod, frame.McCABE_prod,\n",
    "                         frame.RFC_prod, frame.MPC_prod,\n",
    "                         frame.RFC_test, frame.MPC_test,\n",
    "                         frame.LCOM1_test, frame.LCOM2_test,\n",
    "                         frame.LCOM4_test, frame.LCC_test,\n",
    "                         frame.LCC_test, frame.WMC_test,\n",
    "                         frame.McCABE_test, frame.NOP_prod]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "def load_meaningful_subset_2(frame):\n",
    "    #columns = [frame.line_coverage, frame.isAssertionRoulette, frame.isMysteryGuest,\n",
    "    #   frame.isResourceOptimism, frame.isForTestersOnly, frame.COH_prod, frame.BUSWEIMER_prod,\n",
    "    #   frame.BUSWEIMER_test, frame.csm_LM, frame.prod_readability]\n",
    "    \n",
    "    [frame.line_coverage,\n",
    "    frame.COH_prod, frame.BUSWEIMER_prod, frame.csm_MC,\n",
    "       frame.prod_readability, frame.prod_readability]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_all_data(frame):\n",
    "    columns = [frame.no_mutations, frame.line_coverage, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "    \n",
    "\n",
    "def load_all_data_with_mine(frame):\n",
    "    columns = [frame.no_mutations, frame.line_coverage, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability,frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "\n",
    "def load_all_data_static(frame):\n",
    "    columns = [frame.no_mutations, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "    \n",
    "\n",
    "def load_all_data_with_mine_static(frame):\n",
    "    columns = [frame.no_mutations, frame.isAssertionRoulette, frame.isEagerTest, frame.isLazyTest,\n",
    "frame.isMysteryGuest, frame.isSensitiveEquality, frame.isResourceOptimism, frame.isForTestersOnly,\n",
    "frame.isIndirectTesting, frame.LOC_prod, frame.HALSTEAD_prod, frame.RFC_prod, frame.CBO_prod, frame.MPC_prod, frame.IFC_prod, frame.DAC_prod,frame.DAC2_prod, frame.LCOM1_prod, frame.LCOM2_prod,\n",
    "frame.LCOM3_prod, frame.LCOM4_prod, frame.CONNECTIVITY_prod, frame.LCOM5_prod, frame.COH_prod, frame.TCC_prod,\n",
    "frame.LCC_prod, frame.ICH_prod, frame.WMC_prod, frame.NOA_prod, frame.NOPA_prod, frame.NOP_prod,\n",
    "frame.McCABE_prod, frame.BUSWEIMER_prod, frame.LOC_test, frame.HALSTEAD_test, frame.RFC_test, frame.CBO_test,\n",
    "frame.MPC_test, frame.IFC_test, frame.DAC_test, frame.DAC2_test, frame.LCOM1_test, frame.LCOM2_test,\n",
    "frame.LCOM3_test, frame.LCOM4_test, frame.CONNECTIVITY_test, frame.LCOM5_test, frame.COH_test, frame.TCC_test,\n",
    "frame.LCC_test, frame.ICH_test, frame.WMC_test, frame.NOA_test, frame.NOPA_test, frame.NOP_test, frame.McCABE_test,\n",
    "frame.BUSWEIMER_test, frame.csm_CDSBP, frame.csm_CC, frame.csm_FD, frame.csm_Blob, frame.csm_SC, frame.csm_MC,\n",
    "frame.csm_LM, frame.csm_FE, frame.prod_readability, frame.test_readability,frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)\n",
    "\n",
    "def load_mine(frame):\n",
    "    columns = [frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters, frame.mutation]\n",
    "    data_x = pd.concat(columns, axis = 1).round(2)\n",
    "    data_y = pd.concat([frame.mutation], axis = 1)\n",
    "    return data_x, data_y, len(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid - Static with all their data + my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Dor/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1008 candidates, totalling 10080 fits\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   5.6s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.4s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.4s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.9s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   5.0s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.4s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   7.3s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   5.0s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.9s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD \n",
      "[CV]  activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=SGD, total=   3.9s\n",
      "[CV] activation=softmax, batch_size=10, dropout_rate=0.2, optimizer=RMSprop \n",
      "WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e9759df08b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 73\u001b[0;31m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[1;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2132\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    168\u001b[0m               \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m               reduction=losses_utils.ReductionV2.NONE)\n\u001b[0m\u001b[1;32m    171\u001b[0m           \u001b[0mloss_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     weighted_losses = tf_losses_utils.scale_losses_by_sample_weight(\n\u001b[0;32m--> 107\u001b[0;31m         losses, sample_weight)\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/util.py\u001b[0m in \u001b[0;36mscale_losses_by_sample_weight\u001b[0;34m(losses, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;31m# Broadcast weights if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_broadcast_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6699\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6700\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6701\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6702\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6703\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    794\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1773\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1774\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', activation='linear', init_mode='uniform', dropout_rate=0.1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(82,)))\n",
    "    model.add(keras.layers.Dense(40, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(20, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(2, kernel_initializer=init_mode, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load dataset\n",
    "frame = load_frame()\n",
    "frame = load_quartile(frame)\n",
    "\n",
    "data_x, data_y, number_of_features = load_all_data_with_mine_static(frame)\n",
    "\n",
    "data_x = data_x.values\n",
    "data_y = data_y.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0003, patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=2000)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "dropout_rate = [0.2, 0.25, 0.3]\n",
    "param_grid = dict(batch_size=batch_size, optimizer=optimizer, activation=activation, dropout_rate=dropout_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=kfold, verbose=2)\n",
    "grid_result = grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid - predict the exact mutation score value (Dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', activation='linear', init_mode='uniform', dropout_rate=0.1):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(dropout_rate, input_shape=(82,)))\n",
    "    model.add(keras.layers.Dense(40, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(20, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(keras.layers.Dense(2, kernel_initializer=init_mode, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# load dataset\n",
    "frame = load_frame()\n",
    "frame = load_quartile(frame)\n",
    "\n",
    "data_x, data_y, number_of_features = load_all_data_with_mine(frame)\n",
    "\n",
    "data_x = data_x.values\n",
    "data_y = data_y.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0003, patience=10, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=2000)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "dropout_rate = [0.2, 0.25, 0.3]\n",
    "param_grid = dict(batch_size=batch_size, optimizer=optimizer, activation=activation, dropout_rate=dropout_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=kfold, verbose=2)\n",
    "grid_result = grid.fit(data_x, data_y, callbacks=[early_stopping_monitor])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>isAssertionRoulette</th>\n",
       "      <th>isEagerTest</th>\n",
       "      <th>isLazyTest</th>\n",
       "      <th>isMysteryGuest</th>\n",
       "      <th>isSensitiveEquality</th>\n",
       "      <th>isResourceOptimism</th>\n",
       "      <th>isForTestersOnly</th>\n",
       "      <th>...</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960591</td>\n",
       "      <td>203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>11284.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1520.0</td>\n",
       "      <td>4113.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>69372.0</td>\n",
       "      <td>6332.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453172</td>\n",
       "      <td>331</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4461.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>70</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.296651</td>\n",
       "      <td>209</td>\n",
       "      <td>0.424779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3310.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>0.179487</td>\n",
       "      <td>117</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1779.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3397.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>0.967742</td>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6472.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6046.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>0.602273</td>\n",
       "      <td>88</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>7641.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>0.163793</td>\n",
       "      <td>116</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>6872.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5088.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mutation  no_mutations  line_coverage  isAssertionRoulette  isEagerTest  \\\n",
       "0     0.960591           203       1.000000                    1            1   \n",
       "1     0.666667             6       1.000000                    1            0   \n",
       "2     0.453172           331       0.560606                    1            1   \n",
       "3     0.700000            70       0.848485                    1            1   \n",
       "4     0.296651           209       0.424779                    1            1   \n",
       "...        ...           ...            ...                  ...          ...   \n",
       "2240  0.179487           117       0.316667                    1            1   \n",
       "2241  1.000000             3       1.000000                    1            0   \n",
       "2242  0.967742            31       1.000000                    1            1   \n",
       "2243  0.602273            88       1.000000                    1            1   \n",
       "2244  0.163793           116       0.189189                    1            1   \n",
       "\n",
       "      isLazyTest  isMysteryGuest  isSensitiveEquality  isResourceOptimism  \\\n",
       "0              0               0                    1                   0   \n",
       "1              0               1                    0                   0   \n",
       "2              0               0                    1                   0   \n",
       "3              0               0                    0                   0   \n",
       "4              0               0                    0                   0   \n",
       "...          ...             ...                  ...                 ...   \n",
       "2240           0               0                    0                   0   \n",
       "2241           0               0                    0                   0   \n",
       "2242           0               0                    0                   0   \n",
       "2243           0               0                    0                   0   \n",
       "2244           0               0                    0                   0   \n",
       "\n",
       "      isForTestersOnly  ...  Mockito  BadApi     LOC  Expressions  Depth  \\\n",
       "0                    0  ...      0.0     0.0   757.0       1316.0    9.0   \n",
       "1                    0  ...      0.0     0.0  1520.0       4113.0   27.0   \n",
       "2                    0  ...      0.0     0.0   287.0        520.0   10.0   \n",
       "3                    0  ...      0.0     1.0   126.0        321.0   15.0   \n",
       "4                    0  ...      0.0    17.0   174.0        361.0    8.0   \n",
       "...                ...  ...      ...     ...     ...          ...    ...   \n",
       "2240                 0  ...      0.0     8.0   100.0        198.0    8.0   \n",
       "2241                 0  ...      0.0     0.0    43.0        154.0   25.0   \n",
       "2242                 0  ...      0.0     0.0   363.0        631.0   13.0   \n",
       "2243                 0  ...      0.0     0.0   338.0        546.0   18.0   \n",
       "2244                 0  ...      0.0    36.0   330.0        661.0    9.0   \n",
       "\n",
       "      Vocabulary  Understandability  BodySize  Dexterity  NonWhiteCharacters  \n",
       "0           81.0            11284.0    1410.0        3.0             12516.0  \n",
       "1          240.0            69372.0    6332.0        2.0             31106.0  \n",
       "2           44.0             4461.0     666.0        3.0              3026.0  \n",
       "3           45.0             3206.0     427.0        2.0              2359.0  \n",
       "4           42.0             3310.0     451.0        3.0              2828.0  \n",
       "...          ...                ...       ...        ...                 ...  \n",
       "2240        25.0             1779.0     254.0        3.0              1591.0  \n",
       "2241        41.0             3397.0     269.0        2.0              1307.0  \n",
       "2242        53.0             6472.0     879.0        5.0              6046.0  \n",
       "2243        52.0             7641.0     838.0        2.0              4274.0  \n",
       "2244        67.0             6872.0     847.0        3.0              5088.0  \n",
       "\n",
       "[2243 rows x 84 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      no_mutations  line_coverage  isAssertionRoulette  isEagerTest  \\\n",
       " 0              176           0.95                    0            0   \n",
       " 1              165           1.00                    0            0   \n",
       " 2              147           0.68                    1            1   \n",
       " 3                6           1.00                    1            0   \n",
       " 4             1187           1.00                    0            0   \n",
       " ...            ...            ...                  ...          ...   \n",
       " 2240           164           0.94                    0            1   \n",
       " 2241           100           0.78                    1            0   \n",
       " 2242          4379           0.09                    1            1   \n",
       " 2243           255           0.86                    0            1   \n",
       " 2244            23           1.00                    1            1   \n",
       " \n",
       "       isLazyTest  isMysteryGuest  isSensitiveEquality  isResourceOptimism  \\\n",
       " 0              0               0                    0                   0   \n",
       " 1              0               0                    0                   0   \n",
       " 2              0               0                    0                   0   \n",
       " 3              0               0                    0                   0   \n",
       " 4              0               0                    0                   0   \n",
       " ...          ...             ...                  ...                 ...   \n",
       " 2240           0               0                    0                   0   \n",
       " 2241           0               0                    1                   0   \n",
       " 2242           0               0                    1                   0   \n",
       " 2243           0               0                    1                   0   \n",
       " 2244           0               0                    0                   0   \n",
       " \n",
       "       isForTestersOnly  isIndirectTesting  ...  Mockito  BadApi    LOC  \\\n",
       " 0                    0                  0  ...      0.0     0.0   79.0   \n",
       " 1                    0                  0  ...      0.0     0.0  198.0   \n",
       " 2                    0                  0  ...      0.0    15.0  127.0   \n",
       " 3                    0                  0  ...      0.0     0.0  199.0   \n",
       " 4                    0                  0  ...      0.0     0.0  464.0   \n",
       " ...                ...                ...  ...      ...     ...    ...   \n",
       " 2240                 0                  0  ...      0.0     0.0  117.0   \n",
       " 2241                 0                  0  ...      0.0     0.0   76.0   \n",
       " 2242                 0                  0  ...      0.0     0.0   65.0   \n",
       " 2243                 0                  0  ...      0.0     0.0   86.0   \n",
       " 2244                 0                  0  ...      0.0     0.0  443.0   \n",
       " \n",
       "       Expressions  Depth  Vocabulary  Understandability  BodySize  Dexterity  \\\n",
       " 0           152.0    9.0        25.0             1353.0     189.0        3.0   \n",
       " 1           440.0    8.0        81.0             5152.0     377.0        4.0   \n",
       " 2           270.0    8.0        37.0             2448.0     337.0        3.0   \n",
       " 3           475.0   18.0        92.0             6044.0     593.0        2.0   \n",
       " 4          1078.0   15.0       136.0            12279.0    1527.0        3.0   \n",
       " ...           ...    ...         ...                ...       ...        ...   \n",
       " 2240        373.0    8.0        19.0             2925.0     400.0        4.0   \n",
       " 2241        214.0   12.0        44.0             1916.0     270.0        2.0   \n",
       " 2242        114.0    8.0        17.0              916.0     142.0        3.0   \n",
       " 2243        226.0    9.0        40.0             2024.0     277.0        2.0   \n",
       " 2244        915.0   20.0       114.0            12422.0    1356.0        2.0   \n",
       " \n",
       "       NonWhiteCharacters  \n",
       " 0                 1600.0  \n",
       " 1                 6105.0  \n",
       " 2                 1945.0  \n",
       " 3                 3702.0  \n",
       " 4                12403.0  \n",
       " ...                  ...  \n",
       " 2240              3032.0  \n",
       " 2241              1698.0  \n",
       " 2242               892.0  \n",
       " 2243              1812.0  \n",
       " 2244              7155.0  \n",
       " \n",
       " [2243 rows x 83 columns],       mutation\n",
       " 0     0.795455\n",
       " 1     0.945455\n",
       " 2     0.394558\n",
       " 3     0.833333\n",
       " 4     0.213142\n",
       " ...        ...\n",
       " 2240  0.902439\n",
       " 2241  0.600000\n",
       " 2242  0.040877\n",
       " 2243  0.654902\n",
       " 2244  1.000000\n",
       " \n",
       " [2243 rows x 1 columns], 83)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_all_data_with_mine(load_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1570, 25)\n",
      "Train on 1570 samples, validate on 336 samples\n",
      "Epoch 1/1000\n",
      "1570/1570 [==============================] - 1s 603us/sample - loss: 0.1760 - mae: 0.3061 - val_loss: 0.0817 - val_mae: 0.2318\n",
      "Epoch 2/1000\n",
      "1570/1570 [==============================] - 0s 96us/sample - loss: 0.0603 - mae: 0.1905 - val_loss: 0.0475 - val_mae: 0.1749\n",
      "Epoch 3/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0420 - mae: 0.1561 - val_loss: 0.0419 - val_mae: 0.1633\n",
      "Epoch 4/1000\n",
      "1570/1570 [==============================] - 0s 93us/sample - loss: 0.0346 - mae: 0.1446 - val_loss: 0.0352 - val_mae: 0.1503\n",
      "Epoch 5/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0310 - mae: 0.1357 - val_loss: 0.0339 - val_mae: 0.1469\n",
      "Epoch 6/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0287 - mae: 0.1291 - val_loss: 0.0325 - val_mae: 0.1425\n",
      "Epoch 7/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0273 - mae: 0.1240 - val_loss: 0.0338 - val_mae: 0.1398\n",
      "Epoch 8/1000\n",
      "1570/1570 [==============================] - 0s 99us/sample - loss: 0.0273 - mae: 0.1213 - val_loss: 0.0329 - val_mae: 0.1386\n",
      "Epoch 9/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0286 - mae: 0.1225 - val_loss: 0.0310 - val_mae: 0.1368\n",
      "Epoch 10/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0284 - mae: 0.1218 - val_loss: 0.0394 - val_mae: 0.1479\n",
      "Epoch 11/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0272 - mae: 0.1184 - val_loss: 0.0296 - val_mae: 0.1314\n",
      "Epoch 12/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0266 - mae: 0.1161 - val_loss: 0.0345 - val_mae: 0.1356\n",
      "Epoch 13/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0263 - mae: 0.1154 - val_loss: 0.0301 - val_mae: 0.1325\n",
      "Epoch 14/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0242 - mae: 0.1112 - val_loss: 0.0390 - val_mae: 0.1333\n",
      "Epoch 15/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0226 - mae: 0.1095 - val_loss: 0.0324 - val_mae: 0.1312\n",
      "Epoch 16/1000\n",
      "1570/1570 [==============================] - 0s 102us/sample - loss: 0.0221 - mae: 0.1093 - val_loss: 0.0395 - val_mae: 0.1325\n",
      "Epoch 17/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0214 - mae: 0.1076 - val_loss: 0.0305 - val_mae: 0.1248\n",
      "Epoch 18/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0208 - mae: 0.1057 - val_loss: 0.0359 - val_mae: 0.1266\n",
      "Epoch 19/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0206 - mae: 0.1049 - val_loss: 0.0325 - val_mae: 0.1280\n",
      "Epoch 20/1000\n",
      "1570/1570 [==============================] - 0s 93us/sample - loss: 0.0202 - mae: 0.1043 - val_loss: 0.0339 - val_mae: 0.1256\n",
      "Epoch 21/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0195 - mae: 0.1025 - val_loss: 0.0338 - val_mae: 0.1292\n",
      "Epoch 22/1000\n",
      "1570/1570 [==============================] - 0s 96us/sample - loss: 0.0202 - mae: 0.1052 - val_loss: 0.0346 - val_mae: 0.1245\n",
      "Epoch 23/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0195 - mae: 0.1031 - val_loss: 0.0365 - val_mae: 0.1271\n",
      "Epoch 24/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0198 - mae: 0.1022 - val_loss: 0.0320 - val_mae: 0.1230\n",
      "Epoch 25/1000\n",
      "1570/1570 [==============================] - 0s 96us/sample - loss: 0.0191 - mae: 0.1005 - val_loss: 0.0407 - val_mae: 0.1274\n",
      "Epoch 26/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0200 - mae: 0.1026 - val_loss: 0.0282 - val_mae: 0.1228\n",
      "Epoch 27/1000\n",
      "1570/1570 [==============================] - 0s 107us/sample - loss: 0.0201 - mae: 0.1033 - val_loss: 0.0369 - val_mae: 0.1272\n",
      "Epoch 28/1000\n",
      "1570/1570 [==============================] - 0s 103us/sample - loss: 0.0199 - mae: 0.1028 - val_loss: 0.0290 - val_mae: 0.1210\n",
      "Epoch 29/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0197 - mae: 0.1004 - val_loss: 0.0342 - val_mae: 0.1297\n",
      "Epoch 30/1000\n",
      "1570/1570 [==============================] - 0s 96us/sample - loss: 0.0218 - mae: 0.1058 - val_loss: 0.0278 - val_mae: 0.1222\n",
      "Epoch 31/1000\n",
      "1570/1570 [==============================] - 0s 93us/sample - loss: 0.0205 - mae: 0.1015 - val_loss: 0.0322 - val_mae: 0.1230\n",
      "Epoch 32/1000\n",
      "1570/1570 [==============================] - 0s 96us/sample - loss: 0.0203 - mae: 0.1009 - val_loss: 0.0231 - val_mae: 0.1172\n",
      "Epoch 33/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0209 - mae: 0.1011 - val_loss: 0.0266 - val_mae: 0.1182\n",
      "Epoch 34/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0205 - mae: 0.1014 - val_loss: 0.0245 - val_mae: 0.1188\n",
      "Epoch 35/1000\n",
      "1570/1570 [==============================] - 0s 103us/sample - loss: 0.0194 - mae: 0.0986 - val_loss: 0.0285 - val_mae: 0.1204\n",
      "Epoch 36/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0189 - mae: 0.0983 - val_loss: 0.0251 - val_mae: 0.1185\n",
      "Epoch 37/1000\n",
      "1570/1570 [==============================] - 0s 93us/sample - loss: 0.0181 - mae: 0.0972 - val_loss: 0.0288 - val_mae: 0.1210\n",
      "Epoch 38/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0177 - mae: 0.0959 - val_loss: 0.0247 - val_mae: 0.1216\n",
      "Epoch 39/1000\n",
      "1570/1570 [==============================] - 0s 102us/sample - loss: 0.0178 - mae: 0.0961 - val_loss: 0.0274 - val_mae: 0.1245\n",
      "Epoch 40/1000\n",
      "1570/1570 [==============================] - 0s 99us/sample - loss: 0.0171 - mae: 0.0949 - val_loss: 0.0253 - val_mae: 0.1169\n",
      "Epoch 41/1000\n",
      "1570/1570 [==============================] - 0s 105us/sample - loss: 0.0164 - mae: 0.0930 - val_loss: 0.0283 - val_mae: 0.1221\n",
      "Epoch 42/1000\n",
      "1570/1570 [==============================] - 0s 99us/sample - loss: 0.0168 - mae: 0.0932 - val_loss: 0.0245 - val_mae: 0.1166\n",
      "Epoch 43/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0172 - mae: 0.0944 - val_loss: 0.0299 - val_mae: 0.1192\n",
      "Epoch 44/1000\n",
      "1570/1570 [==============================] - 0s 115us/sample - loss: 0.0175 - mae: 0.0946 - val_loss: 0.0249 - val_mae: 0.1155\n",
      "Epoch 45/1000\n",
      "1570/1570 [==============================] - 0s 105us/sample - loss: 0.0194 - mae: 0.1014 - val_loss: 0.0269 - val_mae: 0.1193\n",
      "Epoch 46/1000\n",
      "1570/1570 [==============================] - 0s 104us/sample - loss: 0.0168 - mae: 0.0941 - val_loss: 0.0245 - val_mae: 0.1178\n",
      "Epoch 47/1000\n",
      "1570/1570 [==============================] - 0s 106us/sample - loss: 0.0168 - mae: 0.0940 - val_loss: 0.0270 - val_mae: 0.1234\n",
      "Epoch 48/1000\n",
      "1570/1570 [==============================] - 0s 101us/sample - loss: 0.0166 - mae: 0.0934 - val_loss: 0.0247 - val_mae: 0.1207\n",
      "Epoch 49/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0166 - mae: 0.0933 - val_loss: 0.0267 - val_mae: 0.1174\n",
      "Epoch 50/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0161 - mae: 0.0917 - val_loss: 0.0243 - val_mae: 0.1146\n",
      "Epoch 51/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0160 - mae: 0.0900 - val_loss: 0.0278 - val_mae: 0.1176\n",
      "Epoch 52/1000\n",
      "1570/1570 [==============================] - 0s 104us/sample - loss: 0.0163 - mae: 0.0913 - val_loss: 0.0242 - val_mae: 0.1162\n",
      "Epoch 53/1000\n",
      "1570/1570 [==============================] - 0s 116us/sample - loss: 0.0162 - mae: 0.0912 - val_loss: 0.0270 - val_mae: 0.1158\n",
      "Epoch 54/1000\n",
      "1570/1570 [==============================] - 0s 101us/sample - loss: 0.0156 - mae: 0.0887 - val_loss: 0.0243 - val_mae: 0.1137\n",
      "Epoch 55/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0158 - mae: 0.0900 - val_loss: 0.0246 - val_mae: 0.1161\n",
      "Epoch 56/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0156 - mae: 0.0889 - val_loss: 0.0248 - val_mae: 0.1167\n",
      "Epoch 57/1000\n",
      "1570/1570 [==============================] - 0s 101us/sample - loss: 0.0155 - mae: 0.0893 - val_loss: 0.0271 - val_mae: 0.1180\n",
      "Epoch 58/1000\n",
      "1570/1570 [==============================] - 0s 102us/sample - loss: 0.0151 - mae: 0.0871 - val_loss: 0.0248 - val_mae: 0.1176\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0151 - mae: 0.0882 - val_loss: 0.0263 - val_mae: 0.1184\n",
      "Epoch 60/1000\n",
      "1570/1570 [==============================] - 0s 96us/sample - loss: 0.0158 - mae: 0.0902 - val_loss: 0.0246 - val_mae: 0.1184\n",
      "Epoch 61/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0153 - mae: 0.0891 - val_loss: 0.0267 - val_mae: 0.1166\n",
      "Epoch 62/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0151 - mae: 0.0881 - val_loss: 0.0245 - val_mae: 0.1143\n",
      "Epoch 63/1000\n",
      "1570/1570 [==============================] - 0s 104us/sample - loss: 0.0156 - mae: 0.0876 - val_loss: 0.0250 - val_mae: 0.1172\n",
      "Epoch 64/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0151 - mae: 0.0879 - val_loss: 0.0265 - val_mae: 0.1207\n",
      "Epoch 65/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0147 - mae: 0.0870 - val_loss: 0.0254 - val_mae: 0.1195\n",
      "Epoch 66/1000\n",
      "1570/1570 [==============================] - 0s 93us/sample - loss: 0.0145 - mae: 0.0851 - val_loss: 0.0250 - val_mae: 0.1176\n",
      "Epoch 67/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0149 - mae: 0.0879 - val_loss: 0.0272 - val_mae: 0.1159\n",
      "Epoch 68/1000\n",
      "1570/1570 [==============================] - 0s 95us/sample - loss: 0.0142 - mae: 0.0865 - val_loss: 0.0250 - val_mae: 0.1154\n",
      "Epoch 69/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0157 - mae: 0.0888 - val_loss: 0.0291 - val_mae: 0.1247\n",
      "Epoch 70/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0166 - mae: 0.0935 - val_loss: 0.0258 - val_mae: 0.1236\n",
      "Epoch 71/1000\n",
      "1570/1570 [==============================] - 0s 105us/sample - loss: 0.0157 - mae: 0.0890 - val_loss: 0.0236 - val_mae: 0.1194\n",
      "Epoch 72/1000\n",
      "1570/1570 [==============================] - 0s 98us/sample - loss: 0.0167 - mae: 0.0890 - val_loss: 0.0264 - val_mae: 0.1180\n",
      "Epoch 73/1000\n",
      "1570/1570 [==============================] - 0s 92us/sample - loss: 0.0227 - mae: 0.0995 - val_loss: 0.0469 - val_mae: 0.1343\n",
      "Epoch 74/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0282 - mae: 0.1031 - val_loss: 0.0298 - val_mae: 0.1251\n",
      "Epoch 75/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0213 - mae: 0.0970 - val_loss: 0.0325 - val_mae: 0.1282\n",
      "Epoch 76/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0189 - mae: 0.1002 - val_loss: 0.0338 - val_mae: 0.1290\n",
      "Epoch 77/1000\n",
      "1570/1570 [==============================] - 0s 101us/sample - loss: 0.0160 - mae: 0.0900 - val_loss: 0.0319 - val_mae: 0.1252\n",
      "Epoch 78/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0152 - mae: 0.0878 - val_loss: 0.0323 - val_mae: 0.1217\n",
      "Epoch 79/1000\n",
      "1570/1570 [==============================] - 0s 94us/sample - loss: 0.0147 - mae: 0.0868 - val_loss: 0.0319 - val_mae: 0.1200\n",
      "Epoch 80/1000\n",
      "1570/1570 [==============================] - 0s 99us/sample - loss: 0.0144 - mae: 0.0855 - val_loss: 0.0317 - val_mae: 0.1213\n",
      "Epoch 81/1000\n",
      "1570/1570 [==============================] - 0s 97us/sample - loss: 0.0143 - mae: 0.0848 - val_loss: 0.0310 - val_mae: 0.1238\n",
      "Epoch 82/1000\n",
      "1570/1570 [==============================] - 0s 100us/sample - loss: 0.0138 - mae: 0.0835 - val_loss: 0.0313 - val_mae: 0.1179\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.1340799480676651\n",
      "Overfit checks:\n",
      "Model Accuracy: 0.09953203797340393\n"
     ]
    }
   ],
   "source": [
    "def silent_evaluation(model, x_test, y_test):\n",
    "    f = open('/dev/null', 'w')\n",
    "    regular_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    sys.stdout = regular_stdout\n",
    "    print('Model Accuracy: {}'.format(test_acc))\n",
    "    \n",
    "def split_data(train_x, train_y, training=0.70, validation=0.5):\n",
    "    train_size = training\n",
    "\n",
    "    train_cnt = math.floor(train_x.shape[0] * train_size)\n",
    "    x_train = train_x[0:train_cnt]\n",
    "    y_train = train_y[0:train_cnt]\n",
    "    x_test = train_x[train_cnt:]\n",
    "    y_test = train_y[train_cnt:]\n",
    "\n",
    "    division = validation\n",
    "\n",
    "    train_cnt = math.floor(x_test.shape[0] * division)\n",
    "    x_validate = x_test[0:train_cnt]\n",
    "    y_validate = y_test[0:train_cnt]\n",
    "    x_test = x_test[train_cnt:]\n",
    "    y_test = y_test[train_cnt:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_validate, y_validate    \n",
    "\n",
    "frame = load_frame()\n",
    "data_x, data_y, number_of_features = load_meaningful_subset(frame)\n",
    "data_y = pd.concat([frame.mutation], axis = 1).round(2).values\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_x)\n",
    "data_x = scaler.transform(data_x)\n",
    "\n",
    "sns.distplot(data_y);\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test, x_validate, y_validate = split_data(data_x, data_y)\n",
    "\n",
    "print(x_train.shape)\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(number_of_features, activation='relu', input_dim=number_of_features))\n",
    "model.add(keras.layers.Dense(40, activation='relu'))\n",
    "model.add(keras.layers.Dense(20, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "early_stopping_monitor = keras.callbacks.EarlyStopping(patience=50,restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=1000, verbose=1, validation_data=(x_validate, y_validate),\n",
    "                    callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "silent_evaluation(model, x_test, y_test)\n",
    "\n",
    "\n",
    "print(\"Overfit checks:\")\n",
    "silent_evaluation(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6493157744407654,\n",
       " 0.6235284805297852,\n",
       " 0.409065306186676,\n",
       " 0.8036282062530518,\n",
       " 0.1475561261177063,\n",
       " 0.8172507882118225,\n",
       " 0.6712965965270996,\n",
       " 0.7556335926055908,\n",
       " 0.845294177532196,\n",
       " 0.2664996385574341,\n",
       " 0.7334466576576233,\n",
       " 0.7235182523727417,\n",
       " 0.5217807292938232,\n",
       " 0.7529882192611694,\n",
       " 0.4862895607948303,\n",
       " 0.8229130506515503,\n",
       " 0.8322136998176575,\n",
       " 0.4084024429321289,\n",
       " 0.9095289707183838,\n",
       " 0.8171208500862122,\n",
       " 0.7998390793800354,\n",
       " 0.037189461290836334,\n",
       " 0.6389078497886658,\n",
       " 0.6923748850822449,\n",
       " 1.0108826160430908,\n",
       " 0.8343718647956848,\n",
       " 0.5367078185081482,\n",
       " 0.8873881697654724,\n",
       " 0.927151083946228,\n",
       " 0.7767894864082336,\n",
       " 0.9824255704879761,\n",
       " 0.6998414397239685,\n",
       " 0.9346991777420044,\n",
       " 0.8242970705032349,\n",
       " 0.8536975979804993,\n",
       " 0.7026914954185486,\n",
       " 0.5588530898094177,\n",
       " 0.5353091359138489,\n",
       " 0.45884937047958374,\n",
       " 0.7434186935424805,\n",
       " 0.7485308647155762,\n",
       " 0.6152995228767395,\n",
       " 0.7876898050308228,\n",
       " 0.5905715227127075,\n",
       " 0.8297582864761353,\n",
       " 0.08778990805149078,\n",
       " -0.012580133974552155,\n",
       " 0.8147168159484863,\n",
       " 0.717842698097229,\n",
       " 0.7873611450195312,\n",
       " 0.8718750476837158,\n",
       " 0.9253476858139038,\n",
       " 0.7967488169670105,\n",
       " 0.6559113264083862,\n",
       " 0.6166220307350159,\n",
       " 0.5588696599006653,\n",
       " 0.7134467959403992,\n",
       " 0.8305023312568665,\n",
       " 0.4108603000640869,\n",
       " 0.3443703055381775,\n",
       " 1.1162906885147095,\n",
       " 0.13390232622623444,\n",
       " 0.9368852376937866,\n",
       " 0.64717036485672,\n",
       " 0.7478752732276917,\n",
       " 0.765579104423523,\n",
       " 0.7980349659919739,\n",
       " 0.696391761302948,\n",
       " 0.809619128704071,\n",
       " 0.723986029624939,\n",
       " 0.9033315777778625,\n",
       " 0.6497480273246765,\n",
       " 0.74754798412323,\n",
       " 0.6214655637741089,\n",
       " 0.29017144441604614,\n",
       " 0.6369460821151733,\n",
       " 0.15615728497505188,\n",
       " 0.569001317024231,\n",
       " 0.15114831924438477,\n",
       " 0.5832134485244751,\n",
       " 0.15590210258960724,\n",
       " 0.687049388885498,\n",
       " 0.24000997841358185,\n",
       " 0.8322646617889404,\n",
       " 0.8253752589225769,\n",
       " 0.41506272554397583,\n",
       " 0.7011527419090271,\n",
       " 0.7513925433158875,\n",
       " 0.6231427788734436,\n",
       " 0.7052261829376221,\n",
       " 0.9579508900642395,\n",
       " 0.6164283752441406,\n",
       " 0.7067047953605652,\n",
       " 0.6936624646186829,\n",
       " 0.6366402506828308,\n",
       " 0.5151284337043762,\n",
       " 0.21402721107006073,\n",
       " 0.3551555871963501,\n",
       " 0.6565552353858948,\n",
       " 0.8170365691184998,\n",
       " 0.8612196445465088,\n",
       " 0.9271218180656433,\n",
       " 0.7186635136604309,\n",
       " 0.729500949382782,\n",
       " 0.517288088798523,\n",
       " 0.42376476526260376,\n",
       " 0.4788520336151123,\n",
       " 0.44774144887924194,\n",
       " 0.8833009600639343,\n",
       " 0.3909335732460022,\n",
       " 0.832571804523468,\n",
       " 0.8823323845863342,\n",
       " -0.1486338973045349,\n",
       " 0.5211604237556458,\n",
       " 0.44467586278915405,\n",
       " 0.4161059856414795,\n",
       " 0.5037083029747009,\n",
       " 0.9151768684387207,\n",
       " 0.9451940655708313,\n",
       " 0.02166854590177536,\n",
       " 0.7715298533439636,\n",
       " 0.7220547795295715,\n",
       " 0.16506363451480865,\n",
       " 0.6266719698905945,\n",
       " 0.7421913743019104,\n",
       " 0.6219784617424011,\n",
       " 0.4363431930541992,\n",
       " 0.9670477509498596,\n",
       " 0.8629603981971741,\n",
       " 0.28004997968673706,\n",
       " 0.9864557981491089,\n",
       " 0.7670719027519226,\n",
       " 0.2193678468465805,\n",
       " 0.6913565397262573,\n",
       " 0.8864704966545105,\n",
       " 0.1512148082256317,\n",
       " 0.7271072268486023,\n",
       " 0.14702042937278748,\n",
       " 0.31381142139434814,\n",
       " 0.7087342143058777,\n",
       " 0.6353752017021179,\n",
       " 0.8435462117195129,\n",
       " 0.2806648015975952,\n",
       " 0.769786536693573,\n",
       " 0.6858322024345398,\n",
       " 0.6256976127624512,\n",
       " 0.3886677026748657,\n",
       " 0.33160150051116943,\n",
       " 0.8196333646774292,\n",
       " 0.5730686187744141,\n",
       " 0.5866984128952026,\n",
       " 0.43595635890960693,\n",
       " 0.319175124168396,\n",
       " 0.7813876271247864,\n",
       " 0.7413703203201294,\n",
       " 0.6666065454483032,\n",
       " 0.7806643843650818,\n",
       " 0.8431037068367004,\n",
       " 0.7956081032752991,\n",
       " 0.7505038380622864,\n",
       " 0.9679641127586365,\n",
       " 0.7559998631477356,\n",
       " 0.3384149670600891,\n",
       " 0.7740233540534973,\n",
       " 0.623237669467926,\n",
       " 0.3325638175010681,\n",
       " 0.16932204365730286,\n",
       " 0.18707945942878723,\n",
       " 0.653137743473053,\n",
       " 0.16254162788391113,\n",
       " 0.04715496301651001,\n",
       " 0.720122218132019,\n",
       " 0.7746111750602722,\n",
       " 0.4732557535171509,\n",
       " 0.8612196445465088,\n",
       " 0.8075899481773376,\n",
       " 0.21115389466285706,\n",
       " 0.8944751620292664,\n",
       " 0.1099991649389267,\n",
       " 0.7552570700645447,\n",
       " 0.7124506235122681,\n",
       " 0.06609273701906204,\n",
       " 0.7527615427970886,\n",
       " 0.7464975714683533,\n",
       " 0.6688041090965271,\n",
       " 0.5507382750511169,\n",
       " 0.6598506569862366,\n",
       " 0.626555323600769,\n",
       " 0.7084628939628601,\n",
       " 0.7485751509666443,\n",
       " 0.9401507377624512,\n",
       " 0.7247113585472107,\n",
       " 0.9252005219459534,\n",
       " 0.889018714427948,\n",
       " 0.605417788028717,\n",
       " 0.8024545311927795,\n",
       " 0.7356259822845459,\n",
       " 0.06212437152862549,\n",
       " 0.6256062984466553,\n",
       " 0.1901414841413498,\n",
       " 0.3674566149711609,\n",
       " 0.7182029485702515,\n",
       " 0.1945185363292694,\n",
       " 0.5729900598526001,\n",
       " 0.7543511986732483,\n",
       " 0.5709268450737,\n",
       " 0.7706047892570496,\n",
       " 0.6182129979133606,\n",
       " 0.6771689057350159,\n",
       " 0.9163625836372375,\n",
       " 0.6399548649787903,\n",
       " 0.7846070528030396,\n",
       " 0.7350441813468933,\n",
       " 0.863106369972229,\n",
       " 0.6867861747741699,\n",
       " 0.8585329651832581,\n",
       " 0.4113912582397461,\n",
       " 0.14463651180267334,\n",
       " 0.5433870553970337,\n",
       " 0.7273024916648865,\n",
       " 0.778340220451355,\n",
       " 0.6625508666038513,\n",
       " 0.7318583130836487,\n",
       " 0.7142595648765564,\n",
       " 0.7078214287757874,\n",
       " 0.6150226593017578,\n",
       " 0.3196982145309448,\n",
       " 0.7268205881118774,\n",
       " 0.6698746085166931,\n",
       " 0.9035153388977051,\n",
       " 0.3149759769439697,\n",
       " 0.4286388158798218,\n",
       " 0.7537769079208374,\n",
       " 0.553989827632904,\n",
       " 0.644100546836853,\n",
       " 0.6194024682044983,\n",
       " 0.3702425956726074,\n",
       " 0.43168842792510986,\n",
       " 0.6482821702957153,\n",
       " 0.7962743639945984,\n",
       " 0.6516242623329163,\n",
       " 0.6978434324264526,\n",
       " 0.6572790741920471,\n",
       " 0.5300952792167664,\n",
       " 0.493333101272583,\n",
       " 0.8361766934394836,\n",
       " 0.8146801590919495,\n",
       " 0.8546639084815979,\n",
       " 0.43344783782958984,\n",
       " 0.7633175253868103,\n",
       " 0.18703517317771912,\n",
       " 0.7493908405303955,\n",
       " 0.43008899688720703,\n",
       " 0.26077544689178467,\n",
       " 0.8159463405609131,\n",
       " 0.4215430021286011,\n",
       " 0.9655760526657104,\n",
       " 0.7839089035987854,\n",
       " 0.8031596541404724,\n",
       " 0.8986419439315796,\n",
       " 0.40920621156692505,\n",
       " 0.7725022435188293,\n",
       " 0.8372660279273987,\n",
       " 0.8259839415550232,\n",
       " 0.8151440024375916,\n",
       " 0.7358629107475281,\n",
       " 0.1021614670753479,\n",
       " 0.8299852609634399,\n",
       " 0.643097460269928,\n",
       " 0.9143304824829102,\n",
       " 0.5234721302986145,\n",
       " 0.44534021615982056,\n",
       " 0.8518664240837097,\n",
       " 0.364351749420166,\n",
       " 0.8283489942550659,\n",
       " 0.6641502976417542,\n",
       " 0.1810932457447052,\n",
       " 0.9141741991043091,\n",
       " 0.8618336319923401,\n",
       " 0.7875095009803772,\n",
       " 0.4063683748245239,\n",
       " 0.9523453712463379,\n",
       " 0.5298371911048889,\n",
       " 0.5414003729820251,\n",
       " 0.853370726108551,\n",
       " 0.6347164511680603,\n",
       " 0.6905331611633301,\n",
       " 0.9425637125968933,\n",
       " 0.5623738169670105,\n",
       " 0.7233793139457703,\n",
       " 0.6103665232658386,\n",
       " 0.6764634251594543,\n",
       " 0.33195650577545166,\n",
       " 0.40643441677093506,\n",
       " 0.8027234673500061,\n",
       " 0.4774201512336731,\n",
       " 0.6657983064651489,\n",
       " 0.6606008410453796,\n",
       " 0.37715011835098267,\n",
       " 0.7106373310089111,\n",
       " 0.5932176113128662,\n",
       " 0.25974372029304504,\n",
       " 0.8846626281738281,\n",
       " 0.8836258053779602,\n",
       " 0.5438191294670105,\n",
       " 0.5952758193016052,\n",
       " 0.4928979277610779,\n",
       " 0.7315883040428162,\n",
       " 0.7370404601097107,\n",
       " 0.4905744791030884,\n",
       " 0.44102150201797485,\n",
       " 0.9190568327903748,\n",
       " 0.7872847318649292,\n",
       " 0.5847226977348328,\n",
       " 0.8114245533943176,\n",
       " 0.8258516192436218,\n",
       " 0.3607858419418335,\n",
       " 0.6020879149436951,\n",
       " 0.9914599061012268,\n",
       " 0.5400331616401672,\n",
       " 0.8261142373085022,\n",
       " 0.6490193009376526,\n",
       " -0.03554007411003113,\n",
       " 0.9555651545524597,\n",
       " 0.9582820534706116,\n",
       " 0.9304305911064148,\n",
       " 0.44501787424087524,\n",
       " 0.1720990687608719,\n",
       " 0.8037642240524292,\n",
       " 0.7784530520439148,\n",
       " 0.5918488502502441,\n",
       " 0.6583907008171082,\n",
       " 0.7205295562744141,\n",
       " 0.8625537753105164,\n",
       " 0.7334745526313782,\n",
       " 0.12934349477291107,\n",
       " 0.7666222453117371]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.concatenate(y_pred).tolist()\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.73,\n",
       " 0.35,\n",
       " 0.42,\n",
       " 0.95,\n",
       " 0.04,\n",
       " 0.7,\n",
       " 0.51,\n",
       " 0.87,\n",
       " 0.64,\n",
       " 0.98,\n",
       " 0.75,\n",
       " 0.86,\n",
       " 0.5,\n",
       " 0.48,\n",
       " 0.41,\n",
       " 0.82,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.94,\n",
       " 0.8,\n",
       " 0.02,\n",
       " 0.64,\n",
       " 0.79,\n",
       " 1.0,\n",
       " 0.9,\n",
       " 0.78,\n",
       " 0.83,\n",
       " 1.0,\n",
       " 0.7,\n",
       " 0.88,\n",
       " 0.9,\n",
       " 0.95,\n",
       " 0.89,\n",
       " 0.86,\n",
       " 0.67,\n",
       " 0.62,\n",
       " 0.45,\n",
       " 0.14,\n",
       " 0.25,\n",
       " 0.91,\n",
       " 0.14,\n",
       " 0.96,\n",
       " 0.46,\n",
       " 0.92,\n",
       " 0.14,\n",
       " 0.08,\n",
       " 0.81,\n",
       " 0.44,\n",
       " 0.92,\n",
       " 0.88,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.28,\n",
       " 0.42,\n",
       " 0.74,\n",
       " 0.69,\n",
       " 0.83,\n",
       " 0.5,\n",
       " 0.52,\n",
       " 0.92,\n",
       " 0.16,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.61,\n",
       " 0.6,\n",
       " 0.86,\n",
       " 0.72,\n",
       " 0.53,\n",
       " 0.51,\n",
       " 0.81,\n",
       " 0.79,\n",
       " 0.81,\n",
       " 0.54,\n",
       " 0.29,\n",
       " 0.46,\n",
       " 0.09,\n",
       " 0.58,\n",
       " 0.14,\n",
       " 0.67,\n",
       " 0.09,\n",
       " 0.92,\n",
       " 0.21,\n",
       " 0.78,\n",
       " 1.0,\n",
       " 0.84,\n",
       " 0.71,\n",
       " 0.59,\n",
       " 0.62,\n",
       " 0.88,\n",
       " 0.8,\n",
       " 0.43,\n",
       " 0.63,\n",
       " 0.5,\n",
       " 0.49,\n",
       " 0.59,\n",
       " 0.54,\n",
       " 0.54,\n",
       " 0.81,\n",
       " 0.86,\n",
       " 0.33,\n",
       " 1.0,\n",
       " 0.87,\n",
       " 0.81,\n",
       " 0.58,\n",
       " 0.35,\n",
       " 0.15,\n",
       " 0.48,\n",
       " 1.0,\n",
       " 0.67,\n",
       " 1.0,\n",
       " 0.67,\n",
       " 0.01,\n",
       " 0.18,\n",
       " 0.5,\n",
       " 0.51,\n",
       " 0.64,\n",
       " 1.0,\n",
       " 0.79,\n",
       " 0.07,\n",
       " 0.78,\n",
       " 0.61,\n",
       " 0.12,\n",
       " 0.32,\n",
       " 0.47,\n",
       " 0.49,\n",
       " 0.5,\n",
       " 0.89,\n",
       " 0.9,\n",
       " 0.28,\n",
       " 0.85,\n",
       " 0.83,\n",
       " 0.21,\n",
       " 1.0,\n",
       " 0.79,\n",
       " 0.05,\n",
       " 0.58,\n",
       " 0.12,\n",
       " 0.22,\n",
       " 0.4,\n",
       " 0.65,\n",
       " 0.84,\n",
       " 0.29,\n",
       " 0.93,\n",
       " 0.74,\n",
       " 0.76,\n",
       " 0.36,\n",
       " 0.28,\n",
       " 1.0,\n",
       " 0.64,\n",
       " 0.68,\n",
       " 0.47,\n",
       " 0.3,\n",
       " 0.77,\n",
       " 0.9,\n",
       " 0.74,\n",
       " 0.92,\n",
       " 0.83,\n",
       " 0.53,\n",
       " 0.73,\n",
       " 0.86,\n",
       " 1.0,\n",
       " 0.41,\n",
       " 0.88,\n",
       " 0.5,\n",
       " 0.63,\n",
       " 0.2,\n",
       " 0.09,\n",
       " 0.62,\n",
       " 0.09,\n",
       " 0.19,\n",
       " 0.97,\n",
       " 1.0,\n",
       " 0.37,\n",
       " 0.33,\n",
       " 1.0,\n",
       " 0.2,\n",
       " 1.0,\n",
       " 0.27,\n",
       " 1.0,\n",
       " 0.77,\n",
       " 0.05,\n",
       " 0.67,\n",
       " 1.0,\n",
       " 0.9,\n",
       " 0.54,\n",
       " 0.56,\n",
       " 0.78,\n",
       " 0.82,\n",
       " 0.97,\n",
       " 0.91,\n",
       " 0.17,\n",
       " 0.75,\n",
       " 0.45,\n",
       " 0.55,\n",
       " 0.92,\n",
       " 0.48,\n",
       " 0.04,\n",
       " 0.73,\n",
       " 0.3,\n",
       " 0.35,\n",
       " 0.8,\n",
       " 0.05,\n",
       " 0.33,\n",
       " 0.64,\n",
       " 0.81,\n",
       " 0.85,\n",
       " 1.0,\n",
       " 0.98,\n",
       " 0.61,\n",
       " 0.82,\n",
       " 0.82,\n",
       " 0.48,\n",
       " 0.82,\n",
       " 0.52,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.09,\n",
       " 0.57,\n",
       " 0.59,\n",
       " 0.94,\n",
       " 0.6,\n",
       " 0.82,\n",
       " 0.89,\n",
       " 0.72,\n",
       " 0.7,\n",
       " 0.33,\n",
       " 0.7,\n",
       " 0.71,\n",
       " 1.0,\n",
       " 0.83,\n",
       " 0.52,\n",
       " 1.0,\n",
       " 0.49,\n",
       " 0.9,\n",
       " 0.94,\n",
       " 0.67,\n",
       " 0.35,\n",
       " 0.49,\n",
       " 0.76,\n",
       " 0.57,\n",
       " 0.58,\n",
       " 0.94,\n",
       " 0.74,\n",
       " 0.46,\n",
       " 0.7,\n",
       " 1.0,\n",
       " 0.8,\n",
       " 0.38,\n",
       " 0.97,\n",
       " 0.17,\n",
       " 0.86,\n",
       " 0.54,\n",
       " 0.24,\n",
       " 0.81,\n",
       " 0.15,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.97,\n",
       " 1.0,\n",
       " 0.47,\n",
       " 0.66,\n",
       " 0.89,\n",
       " 0.97,\n",
       " 0.92,\n",
       " 0.8,\n",
       " 0.06,\n",
       " 1.0,\n",
       " 0.59,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.64,\n",
       " 0.67,\n",
       " 0.36,\n",
       " 0.53,\n",
       " 1.0,\n",
       " 0.23,\n",
       " 0.76,\n",
       " 1.0,\n",
       " 0.82,\n",
       " 0.37,\n",
       " 0.97,\n",
       " 0.73,\n",
       " 0.56,\n",
       " 0.79,\n",
       " 0.8,\n",
       " 0.83,\n",
       " 1.0,\n",
       " 0.9,\n",
       " 0.82,\n",
       " 0.52,\n",
       " 0.51,\n",
       " 0.44,\n",
       " 0.47,\n",
       " 0.9,\n",
       " 0.48,\n",
       " 1.0,\n",
       " 0.91,\n",
       " 0.45,\n",
       " 0.92,\n",
       " 1.0,\n",
       " 0.32,\n",
       " 1.0,\n",
       " 0.87,\n",
       " 0.51,\n",
       " 0.41,\n",
       " 0.66,\n",
       " 1.0,\n",
       " 0.88,\n",
       " 0.43,\n",
       " 0.46,\n",
       " 0.9,\n",
       " 0.46,\n",
       " 0.77,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.36,\n",
       " 0.46,\n",
       " 0.86,\n",
       " 0.72,\n",
       " 0.82,\n",
       " 0.82,\n",
       " 0.03,\n",
       " 1.0,\n",
       " 0.88,\n",
       " 0.83,\n",
       " 0.54,\n",
       " 0.19,\n",
       " 0.88,\n",
       " 0.32,\n",
       " 0.75,\n",
       " 0.66,\n",
       " 0.94,\n",
       " 1.0,\n",
       " 0.53,\n",
       " 0.07,\n",
       " 0.72]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testi = np.concatenate(y_test).tolist()\n",
    "y_testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0806842255592346,\n",
       " 0.2735284805297852,\n",
       " 0.010934693813323959,\n",
       " 0.1463717937469482,\n",
       " 0.10755612611770629,\n",
       " 0.11725078821182255,\n",
       " 0.1612965965270996,\n",
       " 0.11436640739440918,\n",
       " 0.20529417753219603,\n",
       " 0.7135003614425659,\n",
       " 0.01655334234237671,\n",
       " 0.1364817476272583,\n",
       " 0.021780729293823242,\n",
       " 0.27298821926116945,\n",
       " 0.07628956079483035,\n",
       " 0.002913050651550342,\n",
       " 0.16778630018234253,\n",
       " 0.5915975570678711,\n",
       " 0.09047102928161621,\n",
       " 0.12287914991378779,\n",
       " 0.00016092061996464402,\n",
       " 0.017189461290836334,\n",
       " 0.0010921502113342418,\n",
       " 0.09762511491775516,\n",
       " 0.01088261604309082,\n",
       " 0.06562813520431521,\n",
       " 0.24329218149185183,\n",
       " 0.05738816976547245,\n",
       " 0.07284891605377197,\n",
       " 0.07678948640823369,\n",
       " 0.10242557048797607,\n",
       " 0.20015856027603152,\n",
       " 0.015300822257995561,\n",
       " 0.06570292949676515,\n",
       " 0.006302402019500719,\n",
       " 0.032691495418548544,\n",
       " 0.06114691019058227,\n",
       " 0.08530913591384887,\n",
       " 0.3188493704795837,\n",
       " 0.49341869354248047,\n",
       " 0.16146913528442386,\n",
       " 0.4752995228767395,\n",
       " 0.1723101949691772,\n",
       " 0.1305715227127075,\n",
       " 0.09024171352386479,\n",
       " 0.05221009194850923,\n",
       " 0.09258013397455216,\n",
       " 0.004716815948486275,\n",
       " 0.277842698097229,\n",
       " 0.1326388549804688,\n",
       " 0.008124952316284184,\n",
       " 0.07465231418609619,\n",
       " 0.2032511830329895,\n",
       " 0.3759113264083862,\n",
       " 0.19662203073501588,\n",
       " 0.1811303400993347,\n",
       " 0.023446795940399223,\n",
       " 0.000502331256866495,\n",
       " 0.08913969993591309,\n",
       " 0.17562969446182253,\n",
       " 0.19629068851470943,\n",
       " 0.026097673773765567,\n",
       " 0.06311476230621338,\n",
       " 0.15282963514328007,\n",
       " 0.13787527322769166,\n",
       " 0.16557910442352297,\n",
       " 0.06196503400802611,\n",
       " 0.023608238697051975,\n",
       " 0.279619128704071,\n",
       " 0.21398602962493896,\n",
       " 0.0933315777778625,\n",
       " 0.14025197267532352,\n",
       " 0.06245201587677007,\n",
       " 0.08146556377410885,\n",
       " 0.00017144441604616256,\n",
       " 0.17694608211517332,\n",
       " 0.06615728497505188,\n",
       " 0.010998682975769003,\n",
       " 0.011148319244384752,\n",
       " 0.08678655147552494,\n",
       " 0.06590210258960724,\n",
       " 0.232950611114502,\n",
       " 0.030009978413581856,\n",
       " 0.0522646617889404,\n",
       " 0.1746247410774231,\n",
       " 0.42493727445602414,\n",
       " 0.008847258090972865,\n",
       " 0.16139254331588748,\n",
       " 0.003142778873443608,\n",
       " 0.17477381706237793,\n",
       " 0.15795089006423946,\n",
       " 0.18642837524414063,\n",
       " 0.07670479536056518,\n",
       " 0.19366246461868286,\n",
       " 0.14664025068283082,\n",
       " 0.07487156629562375,\n",
       " 0.3259727889299393,\n",
       " 0.18484441280364994,\n",
       " 0.15344476461410528,\n",
       " 0.04296343088150023,\n",
       " 0.5312196445465087,\n",
       " 0.07287818193435669,\n",
       " 0.1513364863395691,\n",
       " 0.08049905061721807,\n",
       " 0.06271191120147701,\n",
       " 0.07376476526260378,\n",
       " 0.3288520336151123,\n",
       " 0.03225855112075804,\n",
       " 0.11669903993606567,\n",
       " 0.27906642675399784,\n",
       " 0.16742819547653198,\n",
       " 0.2123323845863342,\n",
       " 0.15863389730453492,\n",
       " 0.34116042375564576,\n",
       " 0.05532413721084595,\n",
       " 0.09389401435852052,\n",
       " 0.13629169702529909,\n",
       " 0.0848231315612793,\n",
       " 0.15519406557083126,\n",
       " 0.04833145409822465,\n",
       " 0.008470146656036404,\n",
       " 0.11205477952957155,\n",
       " 0.04506363451480866,\n",
       " 0.3066719698905945,\n",
       " 0.2721913743019104,\n",
       " 0.13197846174240113,\n",
       " 0.06365680694580078,\n",
       " 0.0770477509498596,\n",
       " 0.03703960180282595,\n",
       " 4.99796867370339e-05,\n",
       " 0.1364557981491089,\n",
       " 0.06292809724807735,\n",
       " 0.009367846846580513,\n",
       " 0.3086434602737427,\n",
       " 0.09647049665451046,\n",
       " 0.10121480822563171,\n",
       " 0.14710722684860233,\n",
       " 0.02702042937278748,\n",
       " 0.09381142139434814,\n",
       " 0.30873421430587766,\n",
       " 0.014624798297882102,\n",
       " 0.0035462117195129705,\n",
       " 0.009335198402404765,\n",
       " 0.16021346330642705,\n",
       " 0.054167797565460196,\n",
       " 0.13430238723754884,\n",
       " 0.028667702674865736,\n",
       " 0.05160150051116941,\n",
       " 0.1803666353225708,\n",
       " 0.06693138122558595,\n",
       " 0.09330158710479741,\n",
       " 0.03404364109039304,\n",
       " 0.019175124168396007,\n",
       " 0.01138762712478636,\n",
       " 0.15862967967987063,\n",
       " 0.07339345455169677,\n",
       " 0.13933561563491825,\n",
       " 0.01310370683670048,\n",
       " 0.26560810327529905,\n",
       " 0.020503838062286395,\n",
       " 0.10796411275863649,\n",
       " 0.2440001368522644,\n",
       " 0.07158503293991086,\n",
       " 0.10597664594650269,\n",
       " 0.12323766946792603,\n",
       " 0.2974361824989319,\n",
       " 0.030677956342697155,\n",
       " 0.09707945942878723,\n",
       " 0.03313774347305298,\n",
       " 0.07254162788391114,\n",
       " 0.14284503698349,\n",
       " 0.24987778186798093,\n",
       " 0.22538882493972778,\n",
       " 0.10325575351715088,\n",
       " 0.5312196445465087,\n",
       " 0.19241005182266235,\n",
       " 0.011153894662857045,\n",
       " 0.10552483797073364,\n",
       " 0.16000083506107332,\n",
       " 0.24474292993545532,\n",
       " 0.05754937648773195,\n",
       " 0.01609273701906204,\n",
       " 0.08276154279708858,\n",
       " 0.25350242853164673,\n",
       " 0.23119589090347292,\n",
       " 0.010738275051116908,\n",
       " 0.09985065698623652,\n",
       " 0.15344467639923098,\n",
       " 0.11153710603713984,\n",
       " 0.2214248490333557,\n",
       " 0.03015073776245114,\n",
       " 0.5547113585472107,\n",
       " 0.17520052194595337,\n",
       " 0.439018714427948,\n",
       " 0.055417788028717,\n",
       " 0.1175454688072205,\n",
       " 0.2556259822845459,\n",
       " 0.022124371528625487,\n",
       " 0.10439370155334471,\n",
       " 0.1098585158586502,\n",
       " 0.01745661497116091,\n",
       " 0.08179705142974858,\n",
       " 0.14451853632926942,\n",
       " 0.24299005985260008,\n",
       " 0.11435119867324828,\n",
       " 0.2390731549263001,\n",
       " 0.07939521074295042,\n",
       " 0.3817870020866394,\n",
       " 0.3028310942649841,\n",
       " 0.30636258363723756,\n",
       " 0.18004513502120967,\n",
       " 0.0353929471969604,\n",
       " 0.25504418134689333,\n",
       " 0.04310636997222905,\n",
       " 0.1667861747741699,\n",
       " 0.14146703481674194,\n",
       " 0.38860874176025395,\n",
       " 0.05463651180267334,\n",
       " 0.02661294460296626,\n",
       " 0.1373024916648865,\n",
       " 0.16165977954864497,\n",
       " 0.06255086660385134,\n",
       " 0.08814168691635127,\n",
       " 0.17574043512344362,\n",
       " 0.01217857122421262,\n",
       " 0.08497734069824214,\n",
       " 0.010301785469055191,\n",
       " 0.026820588111877486,\n",
       " 0.04012539148330685,\n",
       " 0.09648466110229492,\n",
       " 0.5150240230560302,\n",
       " 0.09136118412017824,\n",
       " 0.2462230920791626,\n",
       " 0.06398982763290406,\n",
       " 0.255899453163147,\n",
       " 0.32059753179550166,\n",
       " 0.2997574043273926,\n",
       " 0.08168842792510989,\n",
       " 0.15828217029571534,\n",
       " 0.03627436399459838,\n",
       " 0.08162426233291631,\n",
       " 0.11784343242645268,\n",
       " 0.2827209258079528,\n",
       " 0.20990472078323363,\n",
       " 0.03333310127258299,\n",
       " 0.1361766934394837,\n",
       " 0.18531984090805054,\n",
       " 0.054663908481597856,\n",
       " 0.05344783782958984,\n",
       " 0.20668247461318967,\n",
       " 0.017035173177719104,\n",
       " 0.11060915946960448,\n",
       " 0.109911003112793,\n",
       " 0.020775446891784677,\n",
       " 0.005946340560913033,\n",
       " 0.27154300212860105,\n",
       " 0.03442394733428955,\n",
       " 0.2160910964012146,\n",
       " 0.16684034585952756,\n",
       " 0.10135805606842041,\n",
       " 0.060793788433074925,\n",
       " 0.11250224351882931,\n",
       " 0.05273397207260133,\n",
       " 0.14401605844497678,\n",
       " 0.10485599756240849,\n",
       " 0.06413708925247197,\n",
       " 0.0421614670753479,\n",
       " 0.17001473903656006,\n",
       " 0.05309746026992801,\n",
       " 0.08566951751708984,\n",
       " 0.5234721302986145,\n",
       " 0.19465978384017946,\n",
       " 0.18186642408370968,\n",
       " 0.004351749420166029,\n",
       " 0.2983489942550659,\n",
       " 0.33584970235824585,\n",
       " 0.04890675425529481,\n",
       " 0.15417419910430907,\n",
       " 0.1381663680076599,\n",
       " 0.032490499019622754,\n",
       " 0.03636837482452393,\n",
       " 0.017654628753662083,\n",
       " 0.20016280889511107,\n",
       " 0.018599627017974907,\n",
       " 0.06337072610855099,\n",
       " 0.16528354883193974,\n",
       " 0.13946683883666988,\n",
       " 0.05743628740310669,\n",
       " 0.3376261830329895,\n",
       " 0.09662068605422969,\n",
       " 0.0903665232658386,\n",
       " 0.16646342515945434,\n",
       " 0.10804349422454834,\n",
       " 0.06356558322906491,\n",
       " 0.09727653264999392,\n",
       " 0.0025798487663268865,\n",
       " 0.3342016935348511,\n",
       " 0.2493991589546204,\n",
       " 0.07284988164901735,\n",
       " 0.2093626689910889,\n",
       " 0.4067823886871338,\n",
       " 0.06025627970695496,\n",
       " 0.11533737182617188,\n",
       " 0.01362580537796021,\n",
       " 0.03381912946701049,\n",
       " 0.18527581930160525,\n",
       " 0.16710207223892215,\n",
       " 0.26841169595718384,\n",
       " 0.1429595398902893,\n",
       " 0.060574479103088386,\n",
       " 0.018978497982025166,\n",
       " 0.019056832790374734,\n",
       " 0.3272847318649292,\n",
       " 0.18527730226516725,\n",
       " 0.18857544660568237,\n",
       " 0.07585161924362183,\n",
       " 0.0007858419418335094,\n",
       " 0.14208791494369505,\n",
       " 0.13145990610122682,\n",
       " 0.17996683835983274,\n",
       " 0.006114237308502246,\n",
       " 0.17098069906234736,\n",
       " 0.06554007411003113,\n",
       " 0.04443484544754028,\n",
       " 0.07828205347061157,\n",
       " 0.10043059110641483,\n",
       " 0.09498212575912479,\n",
       " 0.017900931239128115,\n",
       " 0.0762357759475708,\n",
       " 0.4584530520439148,\n",
       " 0.15815114974975586,\n",
       " 0.0016092991828918768,\n",
       " 0.21947044372558588,\n",
       " 0.13744622468948364,\n",
       " 0.20347455263137815,\n",
       " 0.059343494772911065,\n",
       " 0.04662224531173709]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#err = y_test - y_pred\n",
    "err = [abs(e1 - e2) for e1, e2 in zip(y_test,y_pred)]\n",
    "err = np.concatenate(err).tolist()\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.73, 0.0806842255592346),\n",
       " (0.35, 0.2735284805297852),\n",
       " (0.42, 0.010934693813323959),\n",
       " (0.95, 0.1463717937469482),\n",
       " (0.04, 0.10755612611770629),\n",
       " (0.7, 0.11725078821182255),\n",
       " (0.51, 0.1612965965270996),\n",
       " (0.87, 0.11436640739440918),\n",
       " (0.64, 0.20529417753219603),\n",
       " (0.98, 0.7135003614425659),\n",
       " (0.75, 0.01655334234237671),\n",
       " (0.86, 0.1364817476272583),\n",
       " (0.5, 0.021780729293823242),\n",
       " (0.48, 0.27298821926116945),\n",
       " (0.41, 0.07628956079483035),\n",
       " (0.82, 0.002913050651550342),\n",
       " (1.0, 0.16778630018234253),\n",
       " (1.0, 0.5915975570678711),\n",
       " (1.0, 0.09047102928161621),\n",
       " (0.94, 0.12287914991378779),\n",
       " (0.8, 0.00016092061996464402),\n",
       " (0.02, 0.017189461290836334),\n",
       " (0.64, 0.0010921502113342418),\n",
       " (0.79, 0.09762511491775516),\n",
       " (1.0, 0.01088261604309082),\n",
       " (0.9, 0.06562813520431521),\n",
       " (0.78, 0.24329218149185183),\n",
       " (0.83, 0.05738816976547245),\n",
       " (1.0, 0.07284891605377197),\n",
       " (0.7, 0.07678948640823369),\n",
       " (0.88, 0.10242557048797607),\n",
       " (0.9, 0.20015856027603152),\n",
       " (0.95, 0.015300822257995561),\n",
       " (0.89, 0.06570292949676515),\n",
       " (0.86, 0.006302402019500719),\n",
       " (0.67, 0.032691495418548544),\n",
       " (0.62, 0.06114691019058227),\n",
       " (0.45, 0.08530913591384887),\n",
       " (0.14, 0.3188493704795837),\n",
       " (0.25, 0.49341869354248047),\n",
       " (0.91, 0.16146913528442386),\n",
       " (0.14, 0.4752995228767395),\n",
       " (0.96, 0.1723101949691772),\n",
       " (0.46, 0.1305715227127075),\n",
       " (0.92, 0.09024171352386479),\n",
       " (0.14, 0.05221009194850923),\n",
       " (0.08, 0.09258013397455216),\n",
       " (0.81, 0.004716815948486275),\n",
       " (0.44, 0.277842698097229),\n",
       " (0.92, 0.1326388549804688),\n",
       " (0.88, 0.008124952316284184),\n",
       " (1.0, 0.07465231418609619),\n",
       " (1.0, 0.2032511830329895),\n",
       " (0.28, 0.3759113264083862),\n",
       " (0.42, 0.19662203073501588),\n",
       " (0.74, 0.1811303400993347),\n",
       " (0.69, 0.023446795940399223),\n",
       " (0.83, 0.000502331256866495),\n",
       " (0.5, 0.08913969993591309),\n",
       " (0.52, 0.17562969446182253),\n",
       " (0.92, 0.19629068851470943),\n",
       " (0.16, 0.026097673773765567),\n",
       " (1.0, 0.06311476230621338),\n",
       " (0.8, 0.15282963514328007),\n",
       " (0.61, 0.13787527322769166),\n",
       " (0.6, 0.16557910442352297),\n",
       " (0.86, 0.06196503400802611),\n",
       " (0.72, 0.023608238697051975),\n",
       " (0.53, 0.279619128704071),\n",
       " (0.51, 0.21398602962493896),\n",
       " (0.81, 0.0933315777778625),\n",
       " (0.79, 0.14025197267532352),\n",
       " (0.81, 0.06245201587677007),\n",
       " (0.54, 0.08146556377410885),\n",
       " (0.29, 0.00017144441604616256),\n",
       " (0.46, 0.17694608211517332),\n",
       " (0.09, 0.06615728497505188),\n",
       " (0.58, 0.010998682975769003),\n",
       " (0.14, 0.011148319244384752),\n",
       " (0.67, 0.08678655147552494),\n",
       " (0.09, 0.06590210258960724),\n",
       " (0.92, 0.232950611114502),\n",
       " (0.21, 0.030009978413581856),\n",
       " (0.78, 0.0522646617889404),\n",
       " (1.0, 0.1746247410774231),\n",
       " (0.84, 0.42493727445602414),\n",
       " (0.71, 0.008847258090972865),\n",
       " (0.59, 0.16139254331588748),\n",
       " (0.62, 0.003142778873443608),\n",
       " (0.88, 0.17477381706237793),\n",
       " (0.8, 0.15795089006423946),\n",
       " (0.43, 0.18642837524414063),\n",
       " (0.63, 0.07670479536056518),\n",
       " (0.5, 0.19366246461868286),\n",
       " (0.49, 0.14664025068283082),\n",
       " (0.59, 0.07487156629562375),\n",
       " (0.54, 0.3259727889299393),\n",
       " (0.54, 0.18484441280364994),\n",
       " (0.81, 0.15344476461410528),\n",
       " (0.86, 0.04296343088150023),\n",
       " (0.33, 0.5312196445465087),\n",
       " (1.0, 0.07287818193435669),\n",
       " (0.87, 0.1513364863395691),\n",
       " (0.81, 0.08049905061721807),\n",
       " (0.58, 0.06271191120147701),\n",
       " (0.35, 0.07376476526260378),\n",
       " (0.15, 0.3288520336151123),\n",
       " (0.48, 0.03225855112075804),\n",
       " (1.0, 0.11669903993606567),\n",
       " (0.67, 0.27906642675399784),\n",
       " (1.0, 0.16742819547653198),\n",
       " (0.67, 0.2123323845863342),\n",
       " (0.01, 0.15863389730453492),\n",
       " (0.18, 0.34116042375564576),\n",
       " (0.5, 0.05532413721084595),\n",
       " (0.51, 0.09389401435852052),\n",
       " (0.64, 0.13629169702529909),\n",
       " (1.0, 0.0848231315612793),\n",
       " (0.79, 0.15519406557083126),\n",
       " (0.07, 0.04833145409822465),\n",
       " (0.78, 0.008470146656036404),\n",
       " (0.61, 0.11205477952957155),\n",
       " (0.12, 0.04506363451480866),\n",
       " (0.32, 0.3066719698905945),\n",
       " (0.47, 0.2721913743019104),\n",
       " (0.49, 0.13197846174240113),\n",
       " (0.5, 0.06365680694580078),\n",
       " (0.89, 0.0770477509498596),\n",
       " (0.9, 0.03703960180282595),\n",
       " (0.28, 4.99796867370339e-05),\n",
       " (0.85, 0.1364557981491089),\n",
       " (0.83, 0.06292809724807735),\n",
       " (0.21, 0.009367846846580513),\n",
       " (1.0, 0.3086434602737427),\n",
       " (0.79, 0.09647049665451046),\n",
       " (0.05, 0.10121480822563171),\n",
       " (0.58, 0.14710722684860233),\n",
       " (0.12, 0.02702042937278748),\n",
       " (0.22, 0.09381142139434814),\n",
       " (0.4, 0.30873421430587766),\n",
       " (0.65, 0.014624798297882102),\n",
       " (0.84, 0.0035462117195129705),\n",
       " (0.29, 0.009335198402404765),\n",
       " (0.93, 0.16021346330642705),\n",
       " (0.74, 0.054167797565460196),\n",
       " (0.76, 0.13430238723754884),\n",
       " (0.36, 0.028667702674865736),\n",
       " (0.28, 0.05160150051116941),\n",
       " (1.0, 0.1803666353225708),\n",
       " (0.64, 0.06693138122558595),\n",
       " (0.68, 0.09330158710479741),\n",
       " (0.47, 0.03404364109039304),\n",
       " (0.3, 0.019175124168396007),\n",
       " (0.77, 0.01138762712478636),\n",
       " (0.9, 0.15862967967987063),\n",
       " (0.74, 0.07339345455169677),\n",
       " (0.92, 0.13933561563491825),\n",
       " (0.83, 0.01310370683670048),\n",
       " (0.53, 0.26560810327529905),\n",
       " (0.73, 0.020503838062286395),\n",
       " (0.86, 0.10796411275863649),\n",
       " (1.0, 0.2440001368522644),\n",
       " (0.41, 0.07158503293991086),\n",
       " (0.88, 0.10597664594650269),\n",
       " (0.5, 0.12323766946792603),\n",
       " (0.63, 0.2974361824989319),\n",
       " (0.2, 0.030677956342697155),\n",
       " (0.09, 0.09707945942878723),\n",
       " (0.62, 0.03313774347305298),\n",
       " (0.09, 0.07254162788391114),\n",
       " (0.19, 0.14284503698349),\n",
       " (0.97, 0.24987778186798093),\n",
       " (1.0, 0.22538882493972778),\n",
       " (0.37, 0.10325575351715088),\n",
       " (0.33, 0.5312196445465087),\n",
       " (1.0, 0.19241005182266235),\n",
       " (0.2, 0.011153894662857045),\n",
       " (1.0, 0.10552483797073364),\n",
       " (0.27, 0.16000083506107332),\n",
       " (1.0, 0.24474292993545532),\n",
       " (0.77, 0.05754937648773195),\n",
       " (0.05, 0.01609273701906204),\n",
       " (0.67, 0.08276154279708858),\n",
       " (1.0, 0.25350242853164673),\n",
       " (0.9, 0.23119589090347292),\n",
       " (0.54, 0.010738275051116908),\n",
       " (0.56, 0.09985065698623652),\n",
       " (0.78, 0.15344467639923098),\n",
       " (0.82, 0.11153710603713984),\n",
       " (0.97, 0.2214248490333557),\n",
       " (0.91, 0.03015073776245114),\n",
       " (0.17, 0.5547113585472107),\n",
       " (0.75, 0.17520052194595337),\n",
       " (0.45, 0.439018714427948),\n",
       " (0.55, 0.055417788028717),\n",
       " (0.92, 0.1175454688072205),\n",
       " (0.48, 0.2556259822845459),\n",
       " (0.04, 0.022124371528625487),\n",
       " (0.73, 0.10439370155334471),\n",
       " (0.3, 0.1098585158586502),\n",
       " (0.35, 0.01745661497116091),\n",
       " (0.8, 0.08179705142974858),\n",
       " (0.05, 0.14451853632926942),\n",
       " (0.33, 0.24299005985260008),\n",
       " (0.64, 0.11435119867324828),\n",
       " (0.81, 0.2390731549263001),\n",
       " (0.85, 0.07939521074295042),\n",
       " (1.0, 0.3817870020866394),\n",
       " (0.98, 0.3028310942649841),\n",
       " (0.61, 0.30636258363723756),\n",
       " (0.82, 0.18004513502120967),\n",
       " (0.82, 0.0353929471969604),\n",
       " (0.48, 0.25504418134689333),\n",
       " (0.82, 0.04310636997222905),\n",
       " (0.52, 0.1667861747741699),\n",
       " (1.0, 0.14146703481674194),\n",
       " (0.8, 0.38860874176025395),\n",
       " (0.09, 0.05463651180267334),\n",
       " (0.57, 0.02661294460296626),\n",
       " (0.59, 0.1373024916648865),\n",
       " (0.94, 0.16165977954864497),\n",
       " (0.6, 0.06255086660385134),\n",
       " (0.82, 0.08814168691635127),\n",
       " (0.89, 0.17574043512344362),\n",
       " (0.72, 0.01217857122421262),\n",
       " (0.7, 0.08497734069824214),\n",
       " (0.33, 0.010301785469055191),\n",
       " (0.7, 0.026820588111877486),\n",
       " (0.71, 0.04012539148330685),\n",
       " (1.0, 0.09648466110229492),\n",
       " (0.83, 0.5150240230560302),\n",
       " (0.52, 0.09136118412017824),\n",
       " (1.0, 0.2462230920791626),\n",
       " (0.49, 0.06398982763290406),\n",
       " (0.9, 0.255899453163147),\n",
       " (0.94, 0.32059753179550166),\n",
       " (0.67, 0.2997574043273926),\n",
       " (0.35, 0.08168842792510989),\n",
       " (0.49, 0.15828217029571534),\n",
       " (0.76, 0.03627436399459838),\n",
       " (0.57, 0.08162426233291631),\n",
       " (0.58, 0.11784343242645268),\n",
       " (0.94, 0.2827209258079528),\n",
       " (0.74, 0.20990472078323363),\n",
       " (0.46, 0.03333310127258299),\n",
       " (0.7, 0.1361766934394837),\n",
       " (1.0, 0.18531984090805054),\n",
       " (0.8, 0.054663908481597856),\n",
       " (0.38, 0.05344783782958984),\n",
       " (0.97, 0.20668247461318967),\n",
       " (0.17, 0.017035173177719104),\n",
       " (0.86, 0.11060915946960448),\n",
       " (0.54, 0.109911003112793),\n",
       " (0.24, 0.020775446891784677),\n",
       " (0.81, 0.005946340560913033),\n",
       " (0.15, 0.27154300212860105),\n",
       " (1.0, 0.03442394733428955),\n",
       " (1.0, 0.2160910964012146),\n",
       " (0.97, 0.16684034585952756),\n",
       " (1.0, 0.10135805606842041),\n",
       " (0.47, 0.060793788433074925),\n",
       " (0.66, 0.11250224351882931),\n",
       " (0.89, 0.05273397207260133),\n",
       " (0.97, 0.14401605844497678),\n",
       " (0.92, 0.10485599756240849),\n",
       " (0.8, 0.06413708925247197),\n",
       " (0.06, 0.0421614670753479),\n",
       " (1.0, 0.17001473903656006),\n",
       " (0.59, 0.05309746026992801),\n",
       " (1.0, 0.08566951751708984),\n",
       " (0.0, 0.5234721302986145),\n",
       " (0.64, 0.19465978384017946),\n",
       " (0.67, 0.18186642408370968),\n",
       " (0.36, 0.004351749420166029),\n",
       " (0.53, 0.2983489942550659),\n",
       " (1.0, 0.33584970235824585),\n",
       " (0.23, 0.04890675425529481),\n",
       " (0.76, 0.15417419910430907),\n",
       " (1.0, 0.1381663680076599),\n",
       " (0.82, 0.032490499019622754),\n",
       " (0.37, 0.03636837482452393),\n",
       " (0.97, 0.017654628753662083),\n",
       " (0.73, 0.20016280889511107),\n",
       " (0.56, 0.018599627017974907),\n",
       " (0.79, 0.06337072610855099),\n",
       " (0.8, 0.16528354883193974),\n",
       " (0.83, 0.13946683883666988),\n",
       " (1.0, 0.05743628740310669),\n",
       " (0.9, 0.3376261830329895),\n",
       " (0.82, 0.09662068605422969),\n",
       " (0.52, 0.0903665232658386),\n",
       " (0.51, 0.16646342515945434),\n",
       " (0.44, 0.10804349422454834),\n",
       " (0.47, 0.06356558322906491),\n",
       " (0.9, 0.09727653264999392),\n",
       " (0.48, 0.0025798487663268865),\n",
       " (1.0, 0.3342016935348511),\n",
       " (0.91, 0.2493991589546204),\n",
       " (0.45, 0.07284988164901735),\n",
       " (0.92, 0.2093626689910889),\n",
       " (1.0, 0.4067823886871338),\n",
       " (0.32, 0.06025627970695496),\n",
       " (1.0, 0.11533737182617188),\n",
       " (0.87, 0.01362580537796021),\n",
       " (0.51, 0.03381912946701049),\n",
       " (0.41, 0.18527581930160525),\n",
       " (0.66, 0.16710207223892215),\n",
       " (1.0, 0.26841169595718384),\n",
       " (0.88, 0.1429595398902893),\n",
       " (0.43, 0.060574479103088386),\n",
       " (0.46, 0.018978497982025166),\n",
       " (0.9, 0.019056832790374734),\n",
       " (0.46, 0.3272847318649292),\n",
       " (0.77, 0.18527730226516725),\n",
       " (1.0, 0.18857544660568237),\n",
       " (0.75, 0.07585161924362183),\n",
       " (0.36, 0.0007858419418335094),\n",
       " (0.46, 0.14208791494369505),\n",
       " (0.86, 0.13145990610122682),\n",
       " (0.72, 0.17996683835983274),\n",
       " (0.82, 0.006114237308502246),\n",
       " (0.82, 0.17098069906234736),\n",
       " (0.03, 0.06554007411003113),\n",
       " (1.0, 0.04443484544754028),\n",
       " (0.88, 0.07828205347061157),\n",
       " (0.83, 0.10043059110641483),\n",
       " (0.54, 0.09498212575912479),\n",
       " (0.19, 0.017900931239128115),\n",
       " (0.88, 0.0762357759475708),\n",
       " (0.32, 0.4584530520439148),\n",
       " (0.75, 0.15815114974975586),\n",
       " (0.66, 0.0016092991828918768),\n",
       " (0.94, 0.21947044372558588),\n",
       " (1.0, 0.13744622468948364),\n",
       " (0.53, 0.20347455263137815),\n",
       " (0.07, 0.059343494772911065),\n",
       " (0.72, 0.04662224531173709)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(y_testi,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2de7gdZXXwfysnGzhR5ASJVQ4JSW2EitFEjqDip+AFqDyECGiIpV9R27RWasGaPqHl41Yskdhiq9Qa/UArCuEag9IG+wW1RYM5MYEYIG3kmkRrhASrOcjJYX1/zJ4wZ868M+/sPTP7tn7Pkydnz549s2b27He977qKqmIYhmH0LpNaLYBhGIbRWkwRGIZh9DimCAzDMHocUwSGYRg9jikCwzCMHmdyqwXIy2GHHaYzZ85stRiGYRgdxYYNG36uqtOS3us4RTBz5kyGh4dbLYZhGEZHISKPu94z05BhGEaPY4rAMAyjxzFFYBiG0eOYIjAMw+hxTBEYhmH0OKYIDMMwehxTBIZhGD2OKQLDMIwexxSBYRhGj9NxmcWGYRjdxKqNO1i+Zis794xw+EA/S045igXzBiuVwRSBYRhGi1i1cQcX3b6ZkdExAHbsGeGi2zcDVKoMzDRkGIbRIpav2bpfCYSMjI6xfM3WSuUoVRGIyKkislVEtonI0oT3rxGRTfV//ykie8qUxzAMo53YuWck1/ayKM00JCJ9wLXAu4DtwHoRWa2qD4b7qOqFkf3/FJhXljyGYRjtxuED/exIGPQPH+ivVI4yVwTHAdtU9RFVfQ64CTgjZf9FwI0lymMYhtFWLDnlKPprfeO29df6WHLKUZXKUaazeBB4MvJ6O3B80o4iciQwC1hbojyGYRhtRegQ7uaoIUnYpo59zwFuVdWxpDdFZDGwGGDGjBnFSGcYhtEGLJg3WPnAH6dM09B2YHrk9RHATse+55BiFlLVFao6pKpD06YldlozDMMwGqRMRbAemC0is0TkAILBfnV8JxE5CpgKfL9EWQzDMAwHpSkCVd0HnA+sAR4CblbVLSJyhYjMj+y6CLhJVV1mI8MwDKNESs0sVtW7gLti2y6Jvb6sTBkMwzCMdCyz2DAMo8cxRWAYhtHjmCIwDMPocUwRGIZh9DimCAzDMHocUwSGYRg9jikCwzCMHscUgWEYRo9jisAwDKPHMUVgGIbR45giMAzD6HFMERiGYfQ4pggMwzB6HFMEhmEYPY4pAsMwjB7HFIFhGEaPY4rAMAyjxzFFYBiG0eOUqghE5FQR2Soi20RkqWOf94nIgyKyRUS+VqY8hmEYxkRK61ksIn3AtcC7gO3AehFZraoPRvaZDVwEnKCqu0XkZWXJYxiGYSRTZvP644BtqvoIgIjcBJwBPBjZ5w+Ba1V1N4Cq/qxEeQzDMDqGVRt3sHzNVnbuGeHwgX6WnHIUC+YNlnKuMk1Dg8CTkdfb69uivAp4lYjcKyLrROTUpAOJyGIRGRaR4V27dpUkrmEYRnuwauMOLrp9Mzv2jKDAjj0jXHT7ZlZt3FHK+cpUBJKwTWOvJwOzgROBRcAXRWRgwodUV6jqkKoOTZs2rXBBDcMw2onla7YyMjo2btvI6BjL12wt5XxlKoLtwPTI6yOAnQn7fF1VR1X1UWArgWIwDMPoWXbuGcm1vVnKVATrgdkiMktEDgDOAVbH9lkFnAQgIocRmIoeKVEmwzCMtufwgf5c25slVRGISJ+ILG/kwKq6DzgfWAM8BNysqltE5AoRmV/fbQ3wlIg8CNwDLFHVpxo5n2EYRrew5JSj6K/1jdvWX+tjySlHlXI+UY2b7WM7iKwF3qFZO1bE0NCQDg8Pt1oMwzCMUik6akhENqjqUNJ7PuGjG4Gvi8gtwK/Cjap6e8MSGYZhGKksmDdYWrhoHB9FcCjwFPD2yDYFTBEYhmF0AZmKQFU/UIUghmEYRmvIVAQicgTwGeAEgpXAfwB/pqrbS5bNyKDKzEPDMLoXn/DR6wnCPg8nyAy+s77NaCFVZx4ahtG9+CiCaap6varuq//7EmDpvS2m6sxDwzC6Fx9F8HMRObeeU9AnIucSOI+NFlJ15qFhGN2LjyL4IPA+4KfAT4Cz69uMFlJ15qFhGN1LZmYxcJaqzlfVaar6MlVdoKqPVySf4aDqzEPDMLqXVEWgqmMEPQSMNmPBvEGuOnMOgwP9CDA40M9VZ86xqCHDMHLjk1B2r4h8FljJ+MziH5YmleFFlZmHZXDxqs3ceN+TjKnSJ8Ki46dz5YI5rRbLMHoOH0Xw5vr/V0S2KeMzjQ0jFxev2swN657Y/3pMdf9rUwaGUS2pikBEJgGfU9WbK5KndCwJqz248b4nndtNERhGtWT5CJ4nKCXdFVgSVvsw5ihm69puGEZ5+ISPfktEPi4i00Xk0PBf6ZKVgCVhtQ99ktTJ1L3dMHxYtXEHJyxby6yl3+SEZWttkueJj48gzBn4SGSbAr9ZvDjlYklY7cOi46eP8xFEtxtGI4Qr/nCyF674ATP/ZpC5IlDVWQn/Ok4JgCVhtRNXLpjDuW+csX8F0CfCuW+cYf4Bo2Fsxd84zhWBiPyFql5d//u9qnpL5L2/UdW/rELAIllyylHjZgxgSVit5MoFc2zgNwrDVvyNk7YiOCfy90Wx9071ObiInCoiW0Vkm4gsTXj/PBHZJSKb6v/+wOe4jWJJWIbRvdiKv3HSfATi+Dvp9cQPB+UprgXeBWwH1ovIalV9MLbrSlWtLDKp05OwDMNIxlb8jZOmCNTxd9LrJI4DtqnqIwAichNBuYq4IjDaFMu5MDqJ8Nm0ZzY/aYrgdSLyC4LZf3/9b+qvD/I49iAQzRraDhyfsN9ZIvJW4D+BC1V1QqaRiCwGFgPMmDHD49RGs1gEhtGJ2Iq/MZw+AlXtU9WXqOrBqjq5/nf4uuZx7CTzUXwlcScwU1VfC/wb8GWHLCtUdUhVh6ZNs544VWARGIbRO/gklDXKdiAaFH4EsDO6g6o+paq/rr/8AnBsifIYObAIDMPoHXwSyhplPTBbRGYBOwiikN4f3UFEXqGqP6m/nA88VKI8HUWr7fOHD/SzI2HQtwgMw+g+SlsRqOo+gjpFawgG+JtVdYuIXCEi8+u7fVREtojI/cBHgfPKkqeTaIeaSNb4xjB6B9EOK/I1NDSkw8PDrRajVE5YtjZxNj440M+9S6ur/t3qVYlhGMUhIhtUdSjpvUzTkIicCXwSeBmBA1gAVdWXFCqlsZ92sc9bBIZh9AY+pqGrgfmqekgkasiUQIlYhqRhGFXiowj+W1XNiVsh3WyftzLBhtF++EQNDYvISmAVEIZ6oqq3lyZVj9OtGZKWpGYY7YmPIngJsBc4ObJNAVMEJdKN9vm0JLVuu1bD6CQyFYGqfqAKQYzup12c4IZhjCfTRyAiR4jIHSLyMxH5bxG5TUSOqEI4o7swJ7hhtCc+zuLrgdXA4QSF5O6sbzOMXLS7E9wc2Uav4uMjmKaq0YH/SyJyQVkCGd1LOzvBzZFt9DI+iuDnInIucGP99SLgqfJEMrqZdnWCmyPb6GV8TEMfBN4H/BT4CXB2fZthdA3myDZ6GZ+ooScIKoMaRtdi1Va7D6uV5Y9TEYjIX6jq1SLyGRJaU6rqR0uVzDAqxPrddhfm88lH2oogLCvR3aU+DYP2dmQb+TGfTz6cikBV76z/uVdVb4m+JyLvLVUqw2gB7erINvJjPp98+DiLL/LcZhiG0RZY8mI+0nwEvwO8GxgUkX+IvPUSYF/ZghmGYTRKN/h8qnR2p60IdhL4B54FNkT+rQZO8Tm4iJwqIltFZJuILE3Z72wRURFJ7J5jGIaRhwXzBrnqzDkMDvQjBN39rjpzTseY/qpuV5vZqlJEXgL8SlXH6q/7gANVdW/G5/qA/wTeBWwnaGa/SFUfjO13MPBN4ADgfFVNdU73QqtKwzB6mzLa1aa1qvTxEdwNRA1r/cC/eXzuOGCbqj6iqs8BNwFnJOz31wRd0J71OKZhGEbXU7Wz20cRHKSqvwxf1P+e4vG5QeDJyOvt9W37EZF5wHRV/YbH8QzDMHqCqp3dPorgVyLy+vCFiBwL+KglSdi23w4lIpOAa4A/zzyQyGIRGRaR4V27dnmc2jCMbqOXqsNWXanXp+jcBcAtIrKz/voVwEKPz20HpkdeH0HggA45GHgN8G0RAXg5sFpE5sf9BKq6AlgBgY/A49yGYXQRvZYpvGDeIMOPP82N9z3JmCp9Ipx1bHl5LpkrAlVdDxwNfBj4E+C3VXWDx7HXA7NFZJaIHACcQxBxFB73GVU9TFVnqupMYB0wQQkYhmGkZQp3I6s27uC2DTsYqwfzjKly24Ydpa2C0vII3q6qa0XkzNhbs0Uks3m9qu4TkfOBNUAfcJ2qbhGRK4BhVV2d9nmjM7DCXkYV9FqmcNUlMtJMQ28D1gKnJ7zn1bxeVe8C7optu8Sx74lZxzPai15brhuto9eqw1at+NJqDV1a/9+a1xuJWGEvoyq6IVM4D1UrvjTT0MfSPqiqf1e8OEYn0WvLdaN19Fp12KoVX5pp6OD6/0cBb+AFR+/pwHdLkcboKHptuW60ll6qDlu14kszDV0OICJ3A69X1f+pv74MuMX1OaN36LXlumFUSZWKzyePYAbwXOT1c8DMUqQxOopeW64bRrfiowi+AvxARO4giBZ6D/DPpUpldAy9tFw3jG7Fp3n9J0TkX4D/Vd/0AVXdWK5YhmEYRlX4rAggKDL3C1W9XkSmicgsVX20TMGMfFhil2EYjZKpCETkUmCIIHroeqAG3ACcUK5ohi+W2GUY5dELkyyfFcF7gHnADwFUdWe9mYzRJlhil2GUQysnWRev2jyu6Nyi46dz5YI5pZzLpwz1cxq0MVMAEXlRKZIYDWOJXYZRDq0qdnfxqs3csO6JcUXnblj3BBev2lzK+XxWBDeLyOeBARH5Q+CDwBdKkcZIJGtpaoldxVOmOaAXTA3dQqsmWTfe96RzexmrAp8y1J8CbgVuI/ATXKKqnylcEiMRnybWVTex6HbKbBxedVNyozmq7hQWMuboJe/a3iypikBE+kTk31T1W6q6RFU/rqrfKkUSYxxhN6YLVm7KXJoumDfIVWfOYXCgHyFocH3VmXNsltkgZZoDeq2ufqfTyCSriE5qfZLU4NG9vVlSTUOqOiYie0XkEFV9phQJjAnEHVRJxJemlthVHGWaA8yf01nkzZ4vyrm86Pjp3LDuicTtZeDjI3gW2Cwi3wJ+FW5U1Y+WIpGROGuMY/b/8ijT52L+nM4jzySrqAi+0A/QTlFD3wT+D0HF0Q2Rf0ZJZM0OO9n+X3YD8iKOX6bPxfw53U2RK76hIw/l5YcchAAvP+Qgho48tEnp3KSuCERkHsEqYIuqPlSaFMY4XLNGCOz/nRplUnZMdlHHL7OYXlnHtkik9qCRFV/SdwdUmr8g6vBCi8glwLkEs//jgatUNVfYqIicCvw9Qc/iL6rqstj7fwx8BBgDfgksVtUH0445NDSkw8Pd3d8+yUfQX+vreAfwCcvWJv5IBgf6uXfp29v++O1Kkc+LKZTmyPtduPY/cPIk9oyMTti/mWdZRDao6lDSe2krgoXAXFXdKyIvBf6VHPkDItIHXAu8C9gOrBeR1bGB/muq+k/1/ecDfwec6nuObqVbyzuX7ShtZ0dsmQNsViRS1Y7OXibvb9f13bl8hJX3LAaeVdW9AKr6lIj4+BOiHAdsU9VHAETkJuAMYL8iUNVfRPZ/EfXsZaM7o4DKdpS2qyO27AHWNTiE5/E9r5UqKYY8v928A3tZz3La4P5KEVld/3dn7PXqlM+FDALR9Ljt9W3jEJGPiMiPgauBxEgkEVksIsMiMrxr1y6PUxvtSNmO0nZ1xJadO+AaHPpEcp23nVdU3Yrru5s6pVbps5y2Ijgj9vpTOY+dlPkwYcavqtcC14rI+4GLgd9P2GcFsAICH0FOOYw2oWyTV7ua1MoeYF0tQ/OaF9p1RdXNuL67S08/BmiPnsXfafLY24Fo9sMRwM6U/W8CPtfkOY02p2yTVzua1MoeYF0KcPmarbnOaz2oiyGPPyhr8tJOPYsbZT0wW0RmATuAc4D3R3cQkdmq+l/1l6cB/4VhVEgVUTJLTjmKJbfez+jYC4vZWp8UOsC6FGCegX3BvEGGH396XBLTWce2n2JtZxrxB7XD5KU0RaCq+0TkfGANQfjodaq6RUSuAIZVdTVwvoi8ExgFdpNgFjJ6j6pCGPP+aJuSK27QrMDA2Uh5hNs27BhX+vi2DTsYOvLQlg9UnUKnOtydeQQTdhR5kar+KnvPcumFPII4vRTbXWUOhW/ewaqNO7j8zi3s3js+rttXrk7Jb+gUOduZWUu/majjBXh02WlVizNehpQ8gsyQUBF5s4g8CDxUf/06EfnHgmU0HPRa2eIqq3P6OHHD+x9XAnnk6pRonE6Rs50psmx12eVYovjkBlwDnAI8BaCq9wNvLU0iYxy9Vra4ysHI50ebVQDQR65D+mu5treKVtXe7yaKCmGuegLolSSmqvF2OemlMY3C6LVZWpWDkc+PNus++8jlKiFfUmn5hmnXPIxOoqjeIFVPAH2cxU+KyJsBFZEDCJK+rABdRfRabHeVIYw+ztS0AoCT6vJmsSfBrJS2vVW0ax5Gq8nroysiCqjqCaCPIvhjgsJxgwS5AXcDf1KKNMYEei22u+rBKOtHm3T/Q54Hhh9/OlO2TlLm7RDK2E60qv5S1c+MjyI4SlV/N7pBRE4A7i1FImMcvThLa6fBKJTjgpWbEt/3aSbea8q8m3CZaC5YuYnla7Y2/FvMWmWcdPS0xA5lJx09Lf9FeOCjCD4DvN5jm1ES7TQwFkmnhMUumDfoVAQ+zcR7UZk3Qzs9F2mmmEZXBz6rjHseTq6p5treLE5FICJvAt4MTBORj0XeeglBgphhNEynlTzuE0kc9H2biXerMi8a3+eiKmWR5iOCxpLFfJLOXOdMk6UZ0qKGDgBeTKAsDo78+wVwdinSGD1Dp4XFupqGl9VMvFfxeS6qDK1MiqSKk9eB246RgFlF574jIl9S1ccrlMnoAdrxx5BG1c3EexWf56LKMg5Rs55rNq4EWdm+q5J2DB7w8RF8SUSSykdbzrnRMO34Y8jiygVzbOAvGZ/nopWTCCG5TFQe02Y7Bg/4JJR9HFhS//d/gE1AbxX7MQrHkpeMJHyeiyqTDqNmKAiUgMsr5Gva9Ek6O+GVhyZ+1rW9WTIVgapuiPy7V1U/RtDM3jAapqgMTKO78HkuqpxEJJmh0uLEilqVvHdoRq7tzZJpGhKRqAqaBBwLvLwUaYyewiJpjCSynosqw3Eb6SmcFdHkExnlWlmUVc7ax0ewgRdWRPuAR4EPFS6JYXQB7RQD381UNYlw+SymTqnx7OjzE+z8Jx09zWuQz3J2t12JCVWdVcqZDaPL6LTciCTKUGSdrBzTegondXO75+FdhQzyh/TX2DMysRZVWRVr0xLKzkz7oKreXrw4htG5tHt3qiJMFo2cs5OVo8sMBSR2c3OVLI8O8j6RUVVXrE1bEZye8p4CpggMI0LaTK/Vs2Jfu3TRiqzdlaMPSWaoE5atTbwuVwZ6dJD3qSNUdcXatISyDzR7cBE5laByaR/wRVVdFnv/Y8AfEPgedgEftOQ1o1NxzfQO6a+1fFZctV06VHyuJKx2TRxMIkmJu+QfU6W/1peaI+BTR2hgSi2xK97AlHJMQz6tKg8Rkb8TkeH6v78VkUM8PtcHXAv8DvBqYJGIvDq220ZgSFVfC9wKXJ3/Egwjmyra/rnCGkVoeTkNn0G+qPj8eOx9EcdsFa5yFq4BOQx3TQt/9fkuXLUMPVvM58Ynaug64EfA++qvfw+4Hkj1IQDHAdtU9REAEbkJOAN4MNxBVe+J7L8OONdPbMPwp2w7dXTGODClxoGTJ/HMyOj+2eOFjsqlVc6KfZyPRWW8ZrX37KTEQddKypVNcNLR0zIjmnx8BM8kfFdp25vFJ7P4lap6qao+Uv93OfCbHp8bBKItLrfXt7n4EPAvSW+IyOJwRbJrVzllWI3upcwCd/EZ4+69o/x63/Ncs3Au9y59OwvmDbZFL2Af52NRSX5pCq7TEgdd1zIy+nzidp8y0T4JcVX3ufZZEYyIyFtU9T9gf1Man6lM0qOXqEZF5FxgCHhb0vuqugJYATA0NFTS4sjoNrLs1Dv2jOQqFpaEj+09aaYtBZ3fF1/nYxHx+a4Z7+BAP/cuTS5R1mpnuousMtRxfFZ5Pglxz+1LXlG5tjeLjyL4MPDlul9AgKeB8zw+tx2I1ug9AtgZ30lE3gn8FfA2Vf21x3ENI5O4OchFs2YiH3tvvIJltHBZu7U+bGZAjireeHG2NMXXrOmuTCXiMpcdVJuU6Mz1XeVlKdy9jhWHa3uz+NQa2qSqrwNeC8xR1Xmqer/HsdcDs0VkVr3p/TnA6ugOIjIP+DwwX1V/ll98w0gmy04dJclM5OtcTjP7RI8RtjUcHOifsCyuwnHsY45ops5/WnG2JMUXPWYzpruyehOE392FKzdxUG0SA/21ceayS08/pquKJvrUGvozAufw/wBfEJHXA0tV9e60z6nqPhE5H1hDED56napuEZErgGFVXQ0sJ2h+c4sExsonVHV+U1dkGDTXLCTPDNU1Y3SVGvBJOCoDH3NEMzH/ruJsSXH18Z6/zYStFpmn4FrR7N47Sn+tj2sWzp1wzDwrkXY1f4GfaeiDqvr3InIK8DLgAwSKIVURAKjqXcBdsW2XRP5+Zz5xDcMPlynEJ+Enz+DiGmBdx/A5f1lkmSOaGZDT4updREMxGzWzFJX7EFf+rlVb9P7l8ae0e4a1jyIIV3jvBq5X1ftFykp0NoxicM3Uzzp2cEIpgPiSPu/gEh0QshzUPglHvhQ1wwyP4xqyfQbkvIo3ZGR0jAMnT/K6J0nXW1RNHh9TYjOrNtfEILoyaqVC8Ko+KiJ3A7OAi0TkYKAcj4VhFESaKWToyENTB9BGu6et2riDJbfez+iYe+Ab6K9x2fxjCnfINjrDzHKq+yqpPIo3zjMjo1yzcG5DdZAmFVSTx2eQ9ykxHcrqm4kM7bE6EM1IVRORScBc4BFV3SMiLwUGVfWBKgSMMzQ0pMPDxTRIa2ebXTvT7fctaXDsr/WNi39PugeX37kl0cQRZeqUGhsvObkwueKkhWgmccKytc7Vy2CDSir+XGStknxkTpMzCQEeXXaa9/5Zxw+VbjwaKnwuIDkqLNznwMmTElcuUaL3YebSbzr3eyzHdUURkQ2qOpT0nk8Z6udFZCZwbr138X+o6h0NSdJGtLvNrl3phfuW5Vh13QOfKKVmioaVYb5w7S+QS6GA22Yebr941ebEYmvxsNJ4prYqmYNonLw+F1euR3zwT/IdXLZ6C7/e93yqf+Gg2kTzV5xW1l/yiRr6R+C3gBvrm/5IRN6pqh8pVbKS6YaqiK2gV+5bmiPQXXYgm2acwr7mizw0agZrhLSs21CZDj/+9DhTUtYKK4k8Ppes8iBpK5kQHyW1Z+8L5i/X8aL3fEptUmLOwJSaTzGI/Pgc9W3AKap6vapeT+A0PrEUaSqk6g5A3YLdt8avtRmn8AnL1qb2ym30+FX2/826byOjY9x435PeSjWJPCUsfMqDFPVcK+x3Cn964dzMe/43Z752gv9jkgTby8DHWbwVmAGE5aGnAy3xDxRJIzOhTrGNlylnlTPIVpJ2D13hji86oI/n9j3P6PMTh+y89vaoHGlmp9Bs0ejxq+z/61OuIS3CKIu85iyfSJ4smdOyjOOEq56rzpzDWccOTuhuFg9NDWWsYrxJ61B2J8EzdgjwkIj8oP76eOB7pUhTIXkrLXaKbbxsOYuqUNnOZN1D11hV65vEJ94zp9Afb5pfYDDBIXvhyk25z1tV/9+kZydOVrhpGr5RPSE+kTxJUU+h8h3oryESrCSSSmokXUXUpxDvbjZ05KHjZB1+/Gl++syzKPDTZ55l+PGnq1cEwKdS3uv4wm95NW6n2MbLlrPqmUoryLqHaSWCix5UfUwTvsq/6LyDvMeJ11yKU+sTFr5hema4aRJZjePD80Yd0FmD2MjoGPc8vIurzpyo3IEJCWjx1dmFKzclniPJpxD/jcYd62Oq+19fuWCO1z3JQ1qHsu8kba9XH30/8N3CpamYPD/aTrGNVyFnVTPIVpF1D6s0j6WZJsKB7sDJkzKVf1ErxWaPEz47qzbuYMkt9483oykMHXno/jyPNJNMOBvfs3e8YzfpPlx+5xaeHX0hqiePA3rnnhHvVpWhEgjNUz6O5vi5Qr6aEF0Vbi9DEXi5oEVkrohcLSKPAVcCDxUuSZvTDjXlfegUOdsZV/epcHu0t2wU1/ZmSHLmRhkZHXNGrUQHlqJ6MhR5nLgvZfR53a+87l369sQ69hDMvDddejIbLzmZR5edlunY3b13tGEHtOt3k6acQ1yO+KmO5yt6LtdqpSxTjFMRiMirROQSEXkI+CxBkxlR1ZNU9bMlydO2VBld0QydImc7k9Um0BUGecO6JwpvgxltFpOX6MBS1EqxyuPkndQ0O9nJ87vpc6Qu94l0ZOXStBXBw8A7gNNV9S2q+hmgnK4IHUBR3ZvKplPkbGey2gT6OBmLVgb3Ln27UxlMnVLLHFiKGjyrPE7eSY1r/wHPukN5fjcuh/aYamZIqs9v9EUHJK8CXdubJc1ZfBZBD4F7RORfgZtI7jrWM3SKbbxT5GxXsnwAWSGFRQcRpDV86a/1cenpxwDpDvyior2WnHLUhHpKtT5p6DhZ8uQNTHDtD2RGK02dUsv1uxlMKbLnE6yRda5PvGfOBGez1LeXQZqz+A7gDhF5EbAAuBD4DRH5HHBHVj8Cw+hUsgYpnzDIopzzSeWRk3IHslYgC+YNMvz404mx67mjgOKT4QYM176DvM/g7Ct/mvN2997RXG1DTzp6WmK5DNdKIe/zMPz404m3uawQUp9aQ78Cvgp8VUQOBd4LLMWjH4HRu3RD8p1re1YYJBSXmOhq+BKNTvGJ5Fm1cQe3bdgxIXYdGBeumRUFlOXkzUMRK1ffKKbouX73C9/n3h8/PeFYeSKgXH6iovpN3Hjfkw/97TcAABeISURBVM7tlYaPJqGqTxO0lvx84ZK0KZ0yoLUT3ZJ8lyZrNAyyzMREH6eqT+6Ia59whZD22bzyhNdZxe8mb97Mxas2JyqB6Gd9egSkNeIpot9Emg+iDMqpYFRHRE4Vka0isk1Elia8/1YR+aGI7BORs8uUpRGS+qFeuHITMzP62PY6RYUYxvHtI+xLEXLmdc7nPaePU9VncM7bQcy1v488ZfURziNnWNE0fk7XTDvp80tuvd8ps6vxzUB/rZBgDVc/hbJaguVaEeRBRPqAa4F3AduB9SKyWlUfjOz2BHAe8PGy5GgG17Ic/Ga5vbqaKCOprYxVRlFyFpWYmPS8+DhVfRLcmmndGcVHnjL6COdtIgTJz0ieGfXomHL5nVv2X1NUhrSBugiT12SB0QRRJ5ekCMpcERwHbFPVR1T1OYKoozOiO6jqY/UGN23Z8cynWqJrJlflrKgRip5dRykjzruMVUYZcmbdV9exB6bUEp8XyA5r9AmzdO2z6Pjpzs8mXYvPCqjIPsJLbrl/3D1ZckswSw9ly8rcjT8jrvh/F7v3jiZ+L67s5Gb6TURJqECdur1ZSlsRAIMESWgh2wkK1nUMPtUSXbbRpM+1S22iTixMV8Yqoyg509pHXrByExes3ESfCIuOn+6MNnl2dIyR2K88fF7C+HMXPhE4afskte4EGvafFFWC47LVWxId0xfd/gAwMUzTRfQZWXT89MT7n0bSBKQop3C7UKYiSFK9DXk6RGQxsBhgxowZzciUC58wwSTbaLt2IQrpxMJ0ZdT3icsZFiO7MEdD8aTwziTComGuhKC4EgjxfV58zBGufaLbi5jIFKVgXaUzXPfKRfQZCSNuomG0i46fzjfu/0muLmguE1NRZUZc1UvLSuQqUxFsJ+hdEHIEsLORA6nqCmAFBD2LmxfNj3iYYFIyT5ZtNE47zBjavTBdo7byZuRsdJXk851H+dVz+ZLzFXLFtzdDYiG4GC5fhu/qo2qSnpErF8yZEII5dOShE669Nkl48UGTE81ArhVBWhe2PLRNraECWA/MFpFZInIAQZby6hLPVwphev9jy07jmoVzG7KNhrS6nkhIOxemc/lWIF8JgLw06oMoSnkmlYkIqcq/lGSKiXNIf7IvowzZXMXZfIk+I1m+mwXzBll43PT9PoQ+ERYeN91ZF6ioxLF2obQVgaruE5HzgTVAH3Cdqm4RkSuAYVVdLSJvAO4ApgKni8jlqnpMWTI1S6O2UWi8g1QZtHNzmbQBOctWHqeIJiVZP2wfP5IPp732Fanll6NKKc9MO889yDKN9Nf6EEm2mfuWvM4j/6WnHzOhnIUv0W5lvgl3K3/w5LiEuxvWPcEN655goL/GQbVJE0ped1OnvlLzCFT1LlV9laq+UlU/Ud92iaqurv+9XlWPUNUXqepL21kJ+JAUmdGOxZnauTBdFREnSTS6Ssr7nfc7mo/f8/CuzPLL4QDmOxsvMnItfEZc0TLRQTGtBeSFKzd5y7Ng3iDLz35d05VXfVZ7aauhPSOjPDs6vnBct1X5LVUR9BrxksHxCJJ2Ch8NB51oPfd2IKsXgC+uiJPLVm9J3L/RHgNJSvWahXN5bNlpnPvGGeNMDee+cQbPejiFXconraBZEmkDcpJ5xGWKmTqltv8ZSSu/nHQtceJDbZb5LavyahLxAdlncpG1GorL2c6TqUYo01ncsTSTCBaaj5JinNslfLSdyeoF4Ivrh+3a7nLy3fPwrsTnASZGG8VJckre8/CuTJOCy3Tncko3sopKMo8kmWJqfbK/uin4lT7Iay5LkzMtiimJqQmRX0VFnIXZyuF4UETiWLtgK4IYjSynkxxRPh2MepEsp11WL4CyZEgrVbDk1vEmpj+/5f5xZqfde0fZMzLq9bz4mBTis82pU2ocONn9Uw0ji8Jzhtfn05M3PssNTTHhLHf52a8bN9i5ZuaDMUWW1lUtjmtAjv4Ws6j1vbDiin8XJx09LdF8Fy1D4euYbreVfVGIllTEqCyGhoZ0eHi4tOO7shWj1R6juIqOpYUUPrbstGKE7TBc9yq6pM57/13Mu+LuRHv2lNokNJaM1F/r48DJkxJXC5MEMgJpEkmTN7rCOCSh92504PXJTYlex1nHDuZq/i7AozmeR5/vMHqNWYN4+FmY6ETOsxJIC/UcjB0vKQz8rGMHWbn+SW/HdN7nMS8zl37T+V6j44eIbFDVoaT3bEUQI+8y22WHbZYyS0AUSR45fZx2vk64rPNeevox1PrG27NrfcKBCUp6ZHQMkeRWhY0oAUhf+YV272sWzuXX+55n9173aiJPnkJYTTTP85fXPOJrGw+v8dML5074HkLCzwKJxR3zrJ5Hn9dUR3bU15Dkp7jn4V25HNOdGibqwnwEMfLaE/M+ED5RRFWXcW7UJ1JGSWWfZCSf87qOc+HKTYky7Nk7yjUL507Y/wLH/ln41LTxyfDO+3ylFVUrojwyNJAsGBOpNklY/t7XjVsFuoo7FoGPI3tnXVksmDfI3MvvznQelx0m6kpYy1sryRdTBDHyxtgPTKk5ZyJJTImVGEgahMsuARE/f6NKJ6+cvko2a6DxPW/ScdLiv5P2v2z1llylB0J8qlz6NnDPMzN2DSBR80hZ2b6uZzmrkU3Zs2sfR3b0Gcwaa6sIE+2qfgSdiO/SNzRN5FECAHsjJQZcjmnXD7+MH0wzVT3zmtGKir1uJtcgrwyXzT+G2qSYiWlS4JhMMyP4mBgabeDuIquaaJkhw808y0XMrtPG7rgjO+n7jH7/aRVEqwoTHUjpd1AGtiJIIGtGmseBF8cn0aXKyobNDKp5zWhF1aBpJhwwrwxZ++ftUBYlbwP3tJVBNHM9qZpoWQNXVpE6n2fZp7ij6zjgLtAGju8irjVir4vKFm+GrmlM0834OPAG+mv8et/zqT/ystvd+dDMoNpIqYoiitH5FABMw0eGqJnDlScQHgteUBRhFJBPBVNfpeQqjBded6sSmXwmRGnPcvweh5FbrqieeDRUWrQXJJd1Wb5m64TIoNGx8aaqNMVUVdtV16qkqH4HcSx8tAFmLf1mqjMrLSQu+vCkhUoWZc9NGtCeGXkhVBFoanCpqgtb0qATDhhF13HyGeAG+mtcNv+YzFDPogfqrPvdrAx5vk+fxjCuZxnczx0k/26SZEtz5ieFWbp+u/EwWp/w1zLrh7nCn6dOqbHxkpMbOmZa+KgpggZI+wHkeTiSyv7GIyqSPuP7Q80a0HwVVivJ+kGWEc/tM8BBeTkQ0LiCbUaGsnIEkuQu6l75DuyNntd30lf078UVuTTQX2PTpcUrAjMNZZCnNn5DD0SGvTIuS54InywTVqNVPauiVY1+fI8ZrwhalJO/mUiuZnw+WYEDLrNcEn0inHWs2wRXVHHBvHX785ozs/wFZUXzVZFhH8WihlIouzZ+mr3Stb9PhI9vP1dIbrVZRCJbEcdpVaOfPMfMio7Jc7zwnl2wclPDkVzN9JpIK7MRvUYfG8KYKl9d9wQzc/ZuLjs+P2+xuKQoozhlTEaqvj+2IqiTJ57/gpWbCrEP5p0Vpf1Qw2JYMNH2mkZaq81GHWPNHieP6aEMB7qrr3ASSRVBo/jKWNTqx3fGm/S8u2a/WdfoIl55N6QIh3/IVEceT1rtoNwBCxmROmmDs8vEl2X6q7pniPkIaKxeULhPM/bBvM7irMGxv9bHQbVJ3rkNZdm4i7ZTJ5FXERftBIXsZ8Qlo2vS4XNOn+tu1KHsiswpomQKBIPzs6PjI+lCZTA1IZDB28/maF5TxL3KehbSxoC89zl+nItXbZ7QWzlezTYP5izOwPVlp8UuhzTjrMz7oOQtKJZG0oOV1/EWJ2sm74rMyKvs8irfvJE0aQ7CwYF+L3mLLFIYp90mID4+gzSSQq3zRIRlhRXnGajj571w5abUZ6GRCUVa9rerq1rWtfhgzuIM8sTz+37WB1ccucskdc/Du7jqzDm5qjK6GFPltg07GDry0P1yNJNT4DOT9zFDZSUVpTkgXRRVCsM1uOdZwudNIkwiy0GZNcv1qbcTxzVgNqsEILlHRJJZyTVbDuWJfi4kNOVGczrSJizx87pKyPhMANPGlaz9qywzAyUrAhE5Ffh7gp7FX1TVZbH3DwT+GTgWeApYqKqPlSlTEmk//KwZUbPOm6QfnqswWvSH6pptJM2uXMQfrEbsko3a8xsZEMdUWfmDJ8cpLx8aKYWRdB9OOnoaJyxbmzjA+pqd8iYR5m1G49MvOC3SJtp4JSRtwuI7IclK/krDJzorSxmF92H48ae9V9Ujo2McOHlSw8mdaT6XrGzroqKqfCktakhE+oBrgd8BXg0sEpFXx3b7ELBbVX8LuAb4ZFnypJFWfyasz/LphXMr61HaaA2a/lofl80/Zly7zCzilT/zRFT4Ng5JOk7WgOgird2ki7wRGEn3ITTLJTUsylPDx3XO8B7F773re3Qdx6VgL79zi9d35Wq8knSNvoNSeC2XzT8m8Zn1aQrjE52VRSNlup8ZGW04QtD1G02rBxXSTVFDxwHbVPURABG5CTgDeDCyzxnAZfW/bwU+KyKiFTsufGZ1RdXJ8SFvDZokeVwlCeLkrfwZxSe8s08kcQndzCos76yyiFIYSaWSG1mqp8nia5ZJk901OOcpjuh7XVkx9i6btk+WcZxGI5fi5K3e6apK60PabzSrHlTVUUNlKoJB4MnI6+3A8a59VHWfiDwDvBT4eXQnEVkMLAaYMWNGKcL6fNnN1MnJKwv416DxOU5R4XpRfGaErh+ez4CY1qUpD0Uo8aKW6nllybt/UQXTGg1VzXLypj2zac9pUZFLeYrXFTHwuq43z2+3ioz/MhVBUvRt/Bvw2QdVXQGsgCBqqHnR2p+ilE70OEXXBfIZdFymDZ8HvZEYcRfN3s+iGqA3Ikue/V0K1mWfb6bSbZGDVdZzWkSARFboZlHXUhRVTTyhXEWwHZgeeX0EsNOxz3YRmQwcAjxdokw9TdEPVlb54GYrkV56+jETYsRrfcKlpx/TuNANUvVSvVFcgzMkm5hcA2ORlVzz4msii+KKXkpaoaSZZVo58LeSMhXBemC2iMwCdgDnAO+P7bMa+H3g+8DZwNqq/QNG48QHnaTqps38sKpeHneKLFlkmV/y2qvbAd/777PqrXKm3SmUmlAmIu8GPk0QPnqdqn5CRK4AhlV1tYgcBHwFmEewEjgndC67aIfqo4ZhGJ1GyxLKVPUu4K7Ytksifz8LvLdMGQzDMIx0rPqoYRhGj2OKwDAMo8cxRWAYhtHjmCIwDMPocUwRGIZh9DimCAzDMHocUwSGYRg9jikCwzCMHqfjWlWKyC7g8QIOdRixKqddjl1v99JL1wp2vY1ypKpOS3qj4xRBUYjIsCvduhux6+1eeulawa63DMw0ZBiG0eOYIjAMw+hxelkRrGi1ABVj19u99NK1gl1v4fSsj8AwDMMI6OUVgWEYhoEpAsMwjJ6n6xWBiJwqIltFZJuILE14/0ARWVl//z4RmVm9lMXgca0fE5EHReQBEfl/InJkK+Qsiqzrjex3toioiHR0yKHP9YrI++rf8RYR+VrVMhaJx/M8Q0TuEZGN9Wf63a2QswhE5DoR+ZmI/MjxvojIP9TvxQMi8vpCBVDVrv1H0CLzx8BvAgcA9wOvju3zJ8A/1f8+B1jZarlLvNaTgCn1vz/cqdfqe731/Q4GvgusA4ZaLXfJ3+9sYCMwtf76Za2Wu+TrXQF8uP73q4HHWi13E9f7VuD1wI8c778b+BdAgDcC9xV5/m5fERwHbFPVR1T1OeAm4IzYPmcAX67/fSvwDhGRCmUsisxrVdV7VHVv/eU64IiKZSwSn+8W4K+Bq4FnqxSuBHyu9w+Ba1V1N4Cq/qxiGYvE53oVeEn970OAnRXKVyiq+l2Cvu0uzgD+WQPWAQMi8oqizt/timAQeDLyent9W+I+qroPeAZ4aSXSFYvPtUb5EMEMo1PJvF4RmQdMV9VvVClYSfh8v68CXiUi94rIOhE5tTLpisfnei8DzhWR7QS90f+0GtFaQt7fdy5KbV7fBiTN7OPxsj77dALe1yEi5wJDwNtKlahcUq9XRCYB1wDnVSVQyfh8v5MJzEMnEqz2/l1EXqOqe0qWrQx8rncR8CVV/VsReRPwlfr1Pl++eJVT6jjV7SuC7cD0yOsjmLh83L+PiEwmWGKmLdHaFZ9rRUTeCfwVMF9Vf12RbGWQdb0HA68Bvi0ijxHYVVd3sMPY91n+uqqOquqjwFYCxdCJ+Fzvh4CbAVT1+8BBBAXauhGv33ejdLsiWA/MFpFZInIAgTN4dWyf1cDv1/8+G1irde9Mh5F5rXVTyecJlEAn248h43pV9RlVPUxVZ6rqTAKfyHxVHW6NuE3j8yyvIggIQEQOIzAVPVKplMXhc71PAO8AEJHfJlAEuyqVsjpWA/+7Hj30RuAZVf1JUQfvatOQqu4TkfOBNQRRCNep6hYRuQIYVtXVwP8lWFJuI1gJnNM6iRvH81qXAy8Gbqn7w59Q1fktE7oJPK+3a/C83jXAySLyIDAGLFHVp1ondeN4Xu+fA18QkQsJzCTndegkDhG5kcCkd1jd53EpUANQ1X8i8IG8G9gG7AU+UOj5O/S+GYZhGAXR7aYhwzAMIwNTBIZhGD2OKQLDMIwexxSBYRhGj2OKwDAMo8cxRWC0BSIyJiKbRORHInKniAw0cazH6nH0Sdv/PbZtk6viY2y/v/Q891/GXn/P53Mex31jvTruJhF5SEQuK+K4hgGmCIz2YURV56rqawjyOT5S0nkOFpEwk/y3c3zOSxHE91PVN+c4RxpfBhar6lyCjOmbmz2giPQ1LZXRFZgiMNqR7xMpqCUiS0Rkfb0O++WR7atEZEO99v5iz2PfDCys/70IuDFyvPNE5LOR198QkRNFZBnQX5+Nf9V1bsd+v6z/LyKyvL7i2SwiC+vbTxSRb4vIrSLysIh81VH99mXATwBUdUxVH6x//sUicn39mA+IyFn17Yvq234kIp+MXNMvReQKEbkPeJOIHCsi36lfyxopsKKl0UG0ug63/bN/qgrwy/r/fcAtwKn11ycT1J0XgonLN4C31t87tP5/P/Aj4KX1148BhyWc4zGCsgvfq7/eSFDH/kf11+cBn43s/w3gxKh8kfdc547vF17XWcC36tf3GwTlEV5BkE36DEHtmEkESvAtCbJfAuwG7gD+CDiovv2TwKcj+00FDq8ffxpB9YC1wIL6+wq8r/53DfgeMK3+eiFBBm/Lnwf7V+0/WxEY7UK/iGwCngIOJRg0IVAEJxMM2j8EjuaFQmofFZH7CeoITcevwNrTwG4ROQd4iCBdvxHynvstwI0azOb/G/gO8Ib6ez9Q1e0aVM3cBMyMf1hVryCoGHs38H7gX+tvvRO4NrLf7vpxv62quzQorf5VgsYnEJSeuK3+91EEZqZv1e/9xXR2jwqjQbq61pDRUYyo6lwROYRgJv4R4B8IVgJXqernozuLyIkEg+CbVHWviHyboOiYDysJBs/zYtv3Md5cmni8Bs+d1uwoWgV2DMfvUlV/DHxORL4A7BKRl9aP61NaPeRZVR2L7LdFVd+UKrnR9diKwGgrVPUZ4KPAx0WkRlB07IMi8mIAERkUkZcRlAvfXR+IjyYoM+3LHQRdy9bEtj8GzBWRSXWH8nGR90br8pBx7uh+Ub4LLBSRPhGZRjBD/4GvwCJyWsR3MJtAYewhWCGcH9lvKnAf8DYROazuEF5EsAKJsxWYJkEtf0SkJiLH+MpkdA+mCIy2Q1U3EvSoPUdV7wa+BnxfRDYTtBM9mMA0MllEHiBoR7kux/H/R1U/qUELxCj3Ao8Cm4FPEZiiQlYAD9SdwGnnju4X5Q7ggfp1rQX+QlV/6isz8HvA1roJ5yvA79Zn9lcCU+tO4fuBkzQoT3wRcE/9fD9U1a8n3IfnCEqvf7L+2U1AUVFORgdh1UcNwzB6HFsRGIZh9DimCAzDMHocUwSGYRg9jikCwzCMHscUgWEYRo9jisAwDKPHMUVgGIbR4/x/O82k8pWEz34AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*zip(*list(zip(y_testi,err))))\n",
    "plt.ylabel('Absolute Prediction Error')\n",
    "plt.xlabel('Real Mutation Score')\n",
    "#plt.show()\n",
    "#plt.show()\n",
    "plt.savefig(\"foo.pdf\", bbox_inches='tight')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert type 'ndarray' to numerator/denominator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-5f3c482d8cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mStatisticsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean requires at least one data point'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(data, start)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_coerce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or raise TypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_exact_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mpartials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartials_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/statistics.py\u001b[0m in \u001b[0;36m_exact_ratio\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"can't convert type '{}' to numerator/denominator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert type 'ndarray' to numerator/denominator"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "err = [abs(e1 - e2) for e1, e2 in zip(y_test,y_pred)]\n",
    "statistics.mean(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "#frame.Assrtions, frame.Conditions,frame.TryCatch, frame.Loop,frame.Hamcrest,frame.Mockito,\n",
    "#           frame.BadApi,frame.LOC,frame.Expressions, frame.Depth, frame.Vocabulary,\n",
    "#           frame.Understandability,frame.BodySize, frame.Dexterity, frame.NonWhiteCharacters]\n",
    "    \n",
    "frame = load_frame()\n",
    "\n",
    "\n",
    "x1 = frame['mutation']#[12, 2, 1, 12, 2]\n",
    "x2 = frame['TryCatch'] #[1, 4, 7, 1, 0]\n",
    "\n",
    "#x1 = [12, 1, 2, 12, 2]\n",
    "#x2 = [1, 7, 4, 1, 0]\n",
    "\n",
    "tau, p_value = stats.kendalltau(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08356339374825926"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3212083462945562e-07"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = load_frame_with_projects()\n",
    "#pd.unique(frame['project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>isAssertionRoulette</th>\n",
       "      <th>isEagerTest</th>\n",
       "      <th>isLazyTest</th>\n",
       "      <th>isMysteryGuest</th>\n",
       "      <th>isSensitiveEquality</th>\n",
       "      <th>isResourceOptimism</th>\n",
       "      <th>...</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>55</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>javapoet</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>220</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>21364.0</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RxJava</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>6181.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>111</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41722.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>RxJava</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5005.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>104</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6814.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>junit4</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>31</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>93</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9227.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>commons-math</td>\n",
       "      <td>0.698462</td>\n",
       "      <td>325</td>\n",
       "      <td>0.941748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>3017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11829.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           project  mutation  no_mutations  line_coverage  \\\n",
       "0     commons-math  0.690909            55       0.875000   \n",
       "1         javapoet  0.372727           220       0.500000   \n",
       "2     commons-math  0.830769           130       0.900000   \n",
       "3           RxJava  1.000000             5       1.000000   \n",
       "4     commons-math  0.738739           111       0.750000   \n",
       "...            ...       ...           ...            ...   \n",
       "2240        RxJava  0.875000             8       1.000000   \n",
       "2241  commons-math  0.865385           104       0.954545   \n",
       "2242        junit4  0.806452            31       0.695652   \n",
       "2243  commons-math  0.827957            93       0.903226   \n",
       "2244  commons-math  0.698462           325       0.941748   \n",
       "\n",
       "      isAssertionRoulette  isEagerTest  isLazyTest  isMysteryGuest  \\\n",
       "0                       1            0           0               0   \n",
       "1                       0            1           0               0   \n",
       "2                       0            0           0               0   \n",
       "3                       1            0           0               0   \n",
       "4                       1            1           0               0   \n",
       "...                   ...          ...         ...             ...   \n",
       "2240                    1            0           0               0   \n",
       "2241                    1            0           0               0   \n",
       "2242                    1            0           0               0   \n",
       "2243                    1            1           0               1   \n",
       "2244                    0            0           0               0   \n",
       "\n",
       "      isSensitiveEquality  isResourceOptimism  ...  Mockito  BadApi    LOC  \\\n",
       "0                       0                   0  ...      0.0     0.0   41.0   \n",
       "1                       1                   0  ...      0.0     0.0  222.0   \n",
       "2                       0                   0  ...      0.0     0.0  249.0   \n",
       "3                       0                   0  ...      0.0     0.0  164.0   \n",
       "4                       0                   0  ...      0.0     0.0  176.0   \n",
       "...                   ...                 ...  ...      ...     ...    ...   \n",
       "2240                    1                   0  ...      0.0     0.0  180.0   \n",
       "2241                    0                   0  ...      0.0     0.0  205.0   \n",
       "2242                    0                   0  ...      0.0     0.0  185.0   \n",
       "2243                    0                   0  ...      0.0     0.0  319.0   \n",
       "2244                    0                   0  ...      0.0     0.0  769.0   \n",
       "\n",
       "      Expressions  Depth  Vocabulary  Understandability  BodySize  Dexterity  \\\n",
       "0           121.0    9.0        17.0             1118.0     156.0        2.0   \n",
       "1          1807.0   26.0       101.0            21364.0    1889.0        2.0   \n",
       "2           587.0   21.0        36.0             7056.0     847.0        3.0   \n",
       "3           440.0   16.0        63.0             6181.0     673.0        2.0   \n",
       "4           850.0   12.0        53.0            41722.0     593.0        2.0   \n",
       "...           ...    ...         ...                ...       ...        ...   \n",
       "2240        451.0   15.0        58.0             5005.0     577.0        2.0   \n",
       "2241        540.0   13.0        44.0             6814.0     867.0        2.0   \n",
       "2242        514.0   13.0        77.0             5271.0     470.0        3.0   \n",
       "2243        858.0   15.0        72.0             9227.0    1107.0        3.0   \n",
       "2244       2061.0   16.0        56.0            28605.0    3017.0        3.0   \n",
       "\n",
       "      NonWhiteCharacters  \n",
       "0                  937.0  \n",
       "1                16425.0  \n",
       "2                 3503.0  \n",
       "3                 3149.0  \n",
       "4                 6628.0  \n",
       "...                  ...  \n",
       "2240              2807.0  \n",
       "2241              3430.0  \n",
       "2242              4317.0  \n",
       "2243              5789.0  \n",
       "2244             11829.0  \n",
       "\n",
       "[2243 rows x 85 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>isAssertionRoulette</th>\n",
       "      <th>isEagerTest</th>\n",
       "      <th>isLazyTest</th>\n",
       "      <th>isMysteryGuest</th>\n",
       "      <th>isSensitiveEquality</th>\n",
       "      <th>isResourceOptimism</th>\n",
       "      <th>...</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>55</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>220</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>21364.0</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7056.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>6181.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>111</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>41722.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6628.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>8</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5005.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>4</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>104</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6814.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>13</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>31</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5271.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>4</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>93</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9227.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>4</td>\n",
       "      <td>0.698462</td>\n",
       "      <td>325</td>\n",
       "      <td>0.941748</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>769.0</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>28605.0</td>\n",
       "      <td>3017.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11829.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      project  mutation  no_mutations  line_coverage  isAssertionRoulette  \\\n",
       "0           4  0.690909            55       0.875000                    1   \n",
       "1           1  0.372727           220       0.500000                    0   \n",
       "2           4  0.830769           130       0.900000                    0   \n",
       "3           8  1.000000             5       1.000000                    1   \n",
       "4           4  0.738739           111       0.750000                    1   \n",
       "...       ...       ...           ...            ...                  ...   \n",
       "2240        8  0.875000             8       1.000000                    1   \n",
       "2241        4  0.865385           104       0.954545                    1   \n",
       "2242       13  0.806452            31       0.695652                    1   \n",
       "2243        4  0.827957            93       0.903226                    1   \n",
       "2244        4  0.698462           325       0.941748                    0   \n",
       "\n",
       "      isEagerTest  isLazyTest  isMysteryGuest  isSensitiveEquality  \\\n",
       "0               0           0               0                    0   \n",
       "1               1           0               0                    1   \n",
       "2               0           0               0                    0   \n",
       "3               0           0               0                    0   \n",
       "4               1           0               0                    0   \n",
       "...           ...         ...             ...                  ...   \n",
       "2240            0           0               0                    1   \n",
       "2241            0           0               0                    0   \n",
       "2242            0           0               0                    0   \n",
       "2243            1           0               1                    0   \n",
       "2244            0           0               0                    0   \n",
       "\n",
       "      isResourceOptimism  ...  Mockito  BadApi    LOC  Expressions  Depth  \\\n",
       "0                      0  ...      0.0     0.0   41.0        121.0    9.0   \n",
       "1                      0  ...      0.0     0.0  222.0       1807.0   26.0   \n",
       "2                      0  ...      0.0     0.0  249.0        587.0   21.0   \n",
       "3                      0  ...      0.0     0.0  164.0        440.0   16.0   \n",
       "4                      0  ...      0.0     0.0  176.0        850.0   12.0   \n",
       "...                  ...  ...      ...     ...    ...          ...    ...   \n",
       "2240                   0  ...      0.0     0.0  180.0        451.0   15.0   \n",
       "2241                   0  ...      0.0     0.0  205.0        540.0   13.0   \n",
       "2242                   0  ...      0.0     0.0  185.0        514.0   13.0   \n",
       "2243                   0  ...      0.0     0.0  319.0        858.0   15.0   \n",
       "2244                   0  ...      0.0     0.0  769.0       2061.0   16.0   \n",
       "\n",
       "      Vocabulary  Understandability  BodySize  Dexterity  NonWhiteCharacters  \n",
       "0           17.0             1118.0     156.0        2.0               937.0  \n",
       "1          101.0            21364.0    1889.0        2.0             16425.0  \n",
       "2           36.0             7056.0     847.0        3.0              3503.0  \n",
       "3           63.0             6181.0     673.0        2.0              3149.0  \n",
       "4           53.0            41722.0     593.0        2.0              6628.0  \n",
       "...          ...                ...       ...        ...                 ...  \n",
       "2240        58.0             5005.0     577.0        2.0              2807.0  \n",
       "2241        44.0             6814.0     867.0        2.0              3430.0  \n",
       "2242        77.0             5271.0     470.0        3.0              4317.0  \n",
       "2243        72.0             9227.0    1107.0        3.0              5789.0  \n",
       "2244        56.0            28605.0    3017.0        3.0             11829.0  \n",
       "\n",
       "[2243 rows x 85 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project commons-lang, tau: 0.01742127854436364, p: 0.7873241936854231\n",
      "project javapoet, tau: 0.19767441860465115, p: 0.3439053279944314\n",
      "project commons-io, tau: -0.031482587918892135, p: 0.7301967773648329\n",
      "project jfreechart, tau: 0.2338729936316801, p: 9.901299820714473e-10\n",
      "project commons-math, tau: 0.009994365652805792, p: 0.7750782099346211\n",
      "project opengrok, tau: 0.09178657790594506, p: 0.14587429514478942\n",
      "project checkstyle, tau: -0.030041674231558454, p: 0.5174776973820301\n",
      "project closure-compiler, tau: -0.281426531784519, p: 6.407863679488957e-10\n",
      "project RxJava, tau: -0.05124484535057572, p: 0.14118719938920765\n",
      "project fastjson, tau: -0.12980352659818656, p: 0.1630773518413966\n",
      "project cat, tau: 0.20029544520870746, p: 0.030626784063831295\n",
      "project joda-beans, tau: 0.4112329130008325, p: 0.08393637367791504\n",
      "project commons-collections, tau: -0.027573590260267383, p: 0.7904296853624124\n",
      "project junit4, tau: 0.046554553461689845, p: 0.6683826688503443\n",
      "project gson, tau: 0.912870929175277, p: 0.07095149242730567\n",
      "project jsoup, tau: 0.27106408958257083, p: 0.06592890401949764\n",
      "project guice, tau: 0.2222222222222222, p: 0.6027418291290805\n"
     ]
    }
   ],
   "source": [
    "frame = load_frame_with_projects()\n",
    "projects = ['commons-lang', 'javapoet', 'commons-io', 'jfreechart',\n",
    "       'commons-math', 'opengrok', 'checkstyle', 'closure-compiler',\n",
    "       'RxJava', 'fastjson', 'cat', 'joda-beans', 'commons-collections',\n",
    "       'junit4', 'gson', 'jsoup', 'guice']\n",
    "for project in projects:\n",
    "    small_frame = frame[frame['project']==project]\n",
    "    x1 = small_frame['mutation']\n",
    "    x2= small_frame['Assrtions']\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    print('project {}, tau: {}, p: {}'.format(project,tau,p_value))\n",
    "    \n",
    "#['commons-lang', 'javapoet', 'commons-io', 'jfreechart',\n",
    "#       'commons-math', 'opengrok', 'checkstyle', 'closure-compiler',\n",
    "#       'RxJava', 'fastjson', 'cat', 'joda-beans', 'commons-collections',\n",
    "#       'junit4', 'gson', 'jsoup', 'guice']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric Assrtions, tau: -0.069, p: 0.000\n",
      "metric Conditions, tau: 0.081, p: 0.000\n",
      "metric TryCatch, tau: 0.089, p: 0.000\n",
      "metric Loop, tau: 0.153, p: 0.000\n",
      "metric Hamcrest, tau: -0.031, p: 0.084\n",
      "metric Mockito, tau: -0.169, p: 0.000\n",
      "metric BadApi, tau: -0.203, p: 0.000\n",
      "metric LOC, tau: 0.102, p: 0.000\n",
      "metric Expressions, tau: 0.125, p: 0.000\n",
      "metric Depth, tau: 0.224, p: 0.000\n",
      "metric Vocabulary, tau: 0.140, p: 0.000\n",
      "metric Understandability, tau: 0.141, p: 0.000\n",
      "metric BodySize, tau: 0.123, p: 0.000\n",
      "metric Dexterity, tau: -0.136, p: 0.000\n",
      "metric NonWhiteCharacters, tau: 0.116, p: 0.000\n"
     ]
    }
   ],
   "source": [
    "frame = load_frame_with_projects()\n",
    "me = ['Assrtions', 'Conditions', 'TryCatch', 'Loop', 'Hamcrest' , 'Mockito',\n",
    "           'BadApi', 'LOC' ,'Expressions', 'Depth', 'Vocabulary', 'Understandability', 'BodySize'\n",
    "     , 'Dexterity', 'NonWhiteCharacters']\n",
    "\n",
    "labels = [1,2]\n",
    "bins = [0,frame.mutation.median(),1]\n",
    "frame['mutation_bins'] = pd.cut(frame.mutation, bins=bins, labels = labels, include_lowest=True)\n",
    "    \n",
    "for m in me:\n",
    "    x1 = frame['mutation'].round(1) #['mutation_bins']\n",
    "    x2 = frame[m] #/frame['LOC_test']\n",
    "    tau, p_value = stats.kendalltau(x1, x2)\n",
    "    print('metric {}, tau: {:.3f}, p: {:.3f}'.format(m,tau,p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-b3bd6789edde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutation_bins'\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutation_bins'\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "import scipy.stats as s\n",
    "\n",
    "s.ranksums(frame.Loop['mutation_bins'==1],frame.Loop['mutation_bins'==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2240    2\n",
       "2241    1\n",
       "2242    2\n",
       "2243    1\n",
       "2244    2\n",
       "Name: mutation_bins, Length: 2243, dtype: category\n",
       "Categories (2, int64): [1 < 2]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "frame['mutation_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        332\n",
       "1         85\n",
       "2       1168\n",
       "3        132\n",
       "4        120\n",
       "        ... \n",
       "2240      99\n",
       "2241      88\n",
       "2242    1519\n",
       "2243      67\n",
       "2244      38\n",
       "Name: LOC_test, Length: 2243, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['LOC_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.038961\n",
       "1       0.076923\n",
       "2       0.065789\n",
       "3       0.065789\n",
       "4       0.136628\n",
       "          ...   \n",
       "2240    0.144330\n",
       "2241    0.112500\n",
       "2242    0.000000\n",
       "2243    0.000000\n",
       "2244    0.009539\n",
       "Length: 2243, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = load_frame_with_projects()\n",
    "frame['Assrtions']/frame['LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        181.0\n",
       "1        215.0\n",
       "2        398.0\n",
       "3        560.0\n",
       "4        140.0\n",
       "         ...  \n",
       "2240      73.0\n",
       "2241    1077.0\n",
       "2242      70.0\n",
       "2243      50.0\n",
       "2244      27.0\n",
       "Name: LOC, Length: 2243, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = is_checkstyle['mutation']#[12, 2, 1, 12, 2]\n",
    "x2 = is_checkstyle['TryCatch'] #[1, 4, 7, 1, 0]\n",
    "\n",
    "#x1 = [12, 1, 2, 12, 2]\n",
    "#x2 = [1, 7, 4, 1, 0]\n",
    "\n",
    "tau, p_value = stats.kendalltau(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07391248429554852"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05580407629140783"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = pd.read_csv('good_tests.csv', sep=\",\")\n",
    "frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "#frame\n",
    "frame.to_csv ('good_tests_extended.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>module</th>\n",
       "      <th>commit</th>\n",
       "      <th>path_test</th>\n",
       "      <th>test_name</th>\n",
       "      <th>path_src</th>\n",
       "      <th>class_name</th>\n",
       "      <th>mutation</th>\n",
       "      <th>no_mutations</th>\n",
       "      <th>line_coverage</th>\n",
       "      <th>...</th>\n",
       "      <th>csm_CC</th>\n",
       "      <th>csm_FD</th>\n",
       "      <th>csm_Blob</th>\n",
       "      <th>csm_SC</th>\n",
       "      <th>csm_MC</th>\n",
       "      <th>csm_LM</th>\n",
       "      <th>csm_FE</th>\n",
       "      <th>prod_readability</th>\n",
       "      <th>test_readability</th>\n",
       "      <th>TestClassName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commons-collections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483cbbbf5a8b3c14edf3e6563e8f8ba380966d47</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.ClosureUtilsTest</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.ClosureUtils</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.447056</td>\n",
       "      <td>ClosureUtilsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commons-collections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483cbbbf5a8b3c14edf3e6563e8f8ba380966d47</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.QueueUtilsTest</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.QueueUtils</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.721790</td>\n",
       "      <td>0.629857</td>\n",
       "      <td>QueueUtilsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commons-collections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483cbbbf5a8b3c14edf3e6563e8f8ba380966d47</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.FactoryUtilsTest</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.FactoryUtils</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.748377</td>\n",
       "      <td>0.751692</td>\n",
       "      <td>FactoryUtilsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commons-collections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483cbbbf5a8b3c14edf3e6563e8f8ba380966d47</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.FluentIterable...</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.FluentIterable</td>\n",
       "      <td>0.978873</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.861148</td>\n",
       "      <td>0.569284</td>\n",
       "      <td>FluentIterableTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commons-collections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>483cbbbf5a8b3c14edf3e6563e8f8ba380966d47</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.MultiSetUtilsTest</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.collections4.MultiSetUtils</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.729573</td>\n",
       "      <td>0.849755</td>\n",
       "      <td>MultiSetUtilsTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>commons-io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2ae025fe5c4a7d2046c53072b0898e37a079fe62</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.ChunkedOutputStre...</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.ChunkedOutputStream</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.848355</td>\n",
       "      <td>0.813373</td>\n",
       "      <td>ChunkedOutputStreamTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>commons-io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2ae025fe5c4a7d2046c53072b0898e37a079fe62</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.StringBuilderWrit...</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.StringBuilderWriter</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952415</td>\n",
       "      <td>0.746426</td>\n",
       "      <td>StringBuilderWriterTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>commons-io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2ae025fe5c4a7d2046c53072b0898e37a079fe62</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.AppendableOutputS...</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.AppendableOutputS...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.961975</td>\n",
       "      <td>0.782097</td>\n",
       "      <td>AppendableOutputStreamTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>commons-io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2ae025fe5c4a7d2046c53072b0898e37a079fe62</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.ChunkedWriterTest</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.ChunkedWriter</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.887467</td>\n",
       "      <td>0.791455</td>\n",
       "      <td>ChunkedWriterTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>commons-io</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2ae025fe5c4a7d2046c53072b0898e37a079fe62</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.CloseShieldOutput...</td>\n",
       "      <td>/home/ubuntu/scripts_mutation/projects/commons...</td>\n",
       "      <td>org.apache.commons.io.output.CloseShieldOutput...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958096</td>\n",
       "      <td>0.864106</td>\n",
       "      <td>CloseShieldOutputStreamTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 project module                                    commit  \\\n",
       "0    commons-collections    NaN  483cbbbf5a8b3c14edf3e6563e8f8ba380966d47   \n",
       "1    commons-collections    NaN  483cbbbf5a8b3c14edf3e6563e8f8ba380966d47   \n",
       "2    commons-collections    NaN  483cbbbf5a8b3c14edf3e6563e8f8ba380966d47   \n",
       "3    commons-collections    NaN  483cbbbf5a8b3c14edf3e6563e8f8ba380966d47   \n",
       "4    commons-collections    NaN  483cbbbf5a8b3c14edf3e6563e8f8ba380966d47   \n",
       "..                   ...    ...                                       ...   \n",
       "598           commons-io    NaN  2ae025fe5c4a7d2046c53072b0898e37a079fe62   \n",
       "599           commons-io    NaN  2ae025fe5c4a7d2046c53072b0898e37a079fe62   \n",
       "600           commons-io    NaN  2ae025fe5c4a7d2046c53072b0898e37a079fe62   \n",
       "601           commons-io    NaN  2ae025fe5c4a7d2046c53072b0898e37a079fe62   \n",
       "602           commons-io    NaN  2ae025fe5c4a7d2046c53072b0898e37a079fe62   \n",
       "\n",
       "                                             path_test  \\\n",
       "0    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "1    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "2    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "3    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "4    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "..                                                 ...   \n",
       "598  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "599  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "600  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "601  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "602  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "\n",
       "                                             test_name  \\\n",
       "0     org.apache.commons.collections4.ClosureUtilsTest   \n",
       "1       org.apache.commons.collections4.QueueUtilsTest   \n",
       "2     org.apache.commons.collections4.FactoryUtilsTest   \n",
       "3    org.apache.commons.collections4.FluentIterable...   \n",
       "4    org.apache.commons.collections4.MultiSetUtilsTest   \n",
       "..                                                 ...   \n",
       "598  org.apache.commons.io.output.ChunkedOutputStre...   \n",
       "599  org.apache.commons.io.output.StringBuilderWrit...   \n",
       "600  org.apache.commons.io.output.AppendableOutputS...   \n",
       "601     org.apache.commons.io.output.ChunkedWriterTest   \n",
       "602  org.apache.commons.io.output.CloseShieldOutput...   \n",
       "\n",
       "                                              path_src  \\\n",
       "0    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "1    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "2    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "3    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "4    /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "..                                                 ...   \n",
       "598  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "599  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "600  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "601  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "602  /home/ubuntu/scripts_mutation/projects/commons...   \n",
       "\n",
       "                                            class_name  mutation  \\\n",
       "0         org.apache.commons.collections4.ClosureUtils  0.972973   \n",
       "1           org.apache.commons.collections4.QueueUtils  1.000000   \n",
       "2         org.apache.commons.collections4.FactoryUtils  1.000000   \n",
       "3       org.apache.commons.collections4.FluentIterable  0.978873   \n",
       "4        org.apache.commons.collections4.MultiSetUtils  1.000000   \n",
       "..                                                 ...       ...   \n",
       "598   org.apache.commons.io.output.ChunkedOutputStream  1.000000   \n",
       "599   org.apache.commons.io.output.StringBuilderWriter  0.944444   \n",
       "600  org.apache.commons.io.output.AppendableOutputS...  1.000000   \n",
       "601         org.apache.commons.io.output.ChunkedWriter  1.000000   \n",
       "602  org.apache.commons.io.output.CloseShieldOutput...  1.000000   \n",
       "\n",
       "     no_mutations  line_coverage  ...  csm_CC  csm_FD  csm_Blob  csm_SC  \\\n",
       "0            74.0       0.965517  ...       0       0         0       0   \n",
       "1            15.0       0.857143  ...       0       0         0       0   \n",
       "2            18.0       0.857143  ...       0       0         0       0   \n",
       "3           142.0       1.000000  ...       0       0         0       0   \n",
       "4            12.0       0.857143  ...       0       0         0       0   \n",
       "..            ...            ...  ...     ...     ...       ...     ...   \n",
       "598          16.0       1.000000  ...       0       0         0       0   \n",
       "599          36.0       0.960000  ...       0       0         0       0   \n",
       "600           5.0       1.000000  ...       0       0         0       0   \n",
       "601          16.0       1.000000  ...       0       0         0       0   \n",
       "602           2.0       1.000000  ...       0       0         0       0   \n",
       "\n",
       "     csm_MC  csm_LM  csm_FE  prod_readability  test_readability  \\\n",
       "0         1       0       0          0.654370          0.447056   \n",
       "1         0       0       0          0.721790          0.629857   \n",
       "2         0       0       0          0.748377          0.751692   \n",
       "3         0       0       0          0.861148          0.569284   \n",
       "4         0       0       0          0.729573          0.849755   \n",
       "..      ...     ...     ...               ...               ...   \n",
       "598       0       0       0          0.848355          0.813373   \n",
       "599       0       0       0          0.952415          0.746426   \n",
       "600       0       0       0          0.961975          0.782097   \n",
       "601       0       0       0          0.887467          0.791455   \n",
       "602       0       0       0          0.958096          0.864106   \n",
       "\n",
       "                   TestClassName  \n",
       "0               ClosureUtilsTest  \n",
       "1                 QueueUtilsTest  \n",
       "2               FactoryUtilsTest  \n",
       "3             FluentIterableTest  \n",
       "4              MultiSetUtilsTest  \n",
       "..                           ...  \n",
       "598      ChunkedOutputStreamTest  \n",
       "599      StringBuilderWriterTest  \n",
       "600   AppendableOutputStreamTest  \n",
       "601            ChunkedWriterTest  \n",
       "602  CloseShieldOutputStreamTest  \n",
       "\n",
       "[603 rows x 77 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nº</th>\n",
       "      <th>Project</th>\n",
       "      <th>TestClassName</th>\n",
       "      <th>Assrtions</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>TryCatch</th>\n",
       "      <th>Loop</th>\n",
       "      <th>Hamcrest</th>\n",
       "      <th>Mockito</th>\n",
       "      <th>BadApi</th>\n",
       "      <th>LOC</th>\n",
       "      <th>Expressions</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Understandability</th>\n",
       "      <th>BodySize</th>\n",
       "      <th>Dexterity</th>\n",
       "      <th>NonWhiteCharacters</th>\n",
       "      <th>AllTestMethods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>RxJava</td>\n",
       "      <td>TestSingleThreadedObservable</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1333.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>RxJava</td>\n",
       "      <td>ConvertToCylonDetector</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>RxJava</td>\n",
       "      <td>ParallelFilterTest</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>RxJava</td>\n",
       "      <td>AllSubscriber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>RxJava</td>\n",
       "      <td>SingleNullTests</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>33297.0</td>\n",
       "      <td>2679.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18487.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6181</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>max</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>10643.0</td>\n",
       "      <td>27386.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>1864449.0</td>\n",
       "      <td>27964.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>439388.0</td>\n",
       "      <td>1783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>range</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>10643.0</td>\n",
       "      <td>27385.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2067.0</td>\n",
       "      <td>1864448.0</td>\n",
       "      <td>27964.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>439376.0</td>\n",
       "      <td>1783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6183</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6184</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3974.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Σ</td>\n",
       "      <td>74394.0</td>\n",
       "      <td>4597.0</td>\n",
       "      <td>6557.0</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>5949.0</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>7026.0</td>\n",
       "      <td>1046975.0</td>\n",
       "      <td>2667829.0</td>\n",
       "      <td>63781.0</td>\n",
       "      <td>268906.0</td>\n",
       "      <td>37554046.0</td>\n",
       "      <td>3278108.0</td>\n",
       "      <td>17017.0</td>\n",
       "      <td>22744641.0</td>\n",
       "      <td>43422.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6186 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nº Project                 TestClassName  Assrtions  Conditions  \\\n",
       "0     1.0  RxJava  TestSingleThreadedObservable        0.0         0.0   \n",
       "1     2.0  RxJava        ConvertToCylonDetector        0.0         0.0   \n",
       "2     3.0  RxJava            ParallelFilterTest        4.0         0.0   \n",
       "3     4.0  RxJava                 AllSubscriber        0.0         0.0   \n",
       "4     5.0  RxJava               SingleNullTests       11.0         5.0   \n",
       "...   ...     ...                           ...        ...         ...   \n",
       "6181  NaN     NaN                           max     2137.0       127.0   \n",
       "6182  NaN     NaN                         range     2137.0       127.0   \n",
       "6183  NaN     NaN                            Q1        0.0         0.0   \n",
       "6184  NaN     NaN                            Q3        8.0         0.0   \n",
       "6185  NaN     NaN                             Σ    74394.0      4597.0   \n",
       "\n",
       "      TryCatch    Loop  Hamcrest  Mockito  BadApi        LOC  Expressions  \\\n",
       "0          2.0     1.0       0.0      0.0     0.0       55.0         95.0   \n",
       "1          0.0     0.0       0.0      0.0     0.0        3.0         19.0   \n",
       "2          2.0     2.0       0.0      0.0     0.0       69.0        221.0   \n",
       "3          0.0     0.0       0.0      0.0     0.0        0.0         34.0   \n",
       "4          6.0     6.0       0.0      0.0     0.0      652.0       2224.0   \n",
       "...        ...     ...       ...      ...     ...        ...          ...   \n",
       "6181     113.0   167.0     786.0    186.0   293.0    10643.0      27386.0   \n",
       "6182     113.0   167.0     786.0    186.0   293.0    10643.0      27385.0   \n",
       "6183       0.0     0.0       0.0      0.0     0.0       16.0         40.0   \n",
       "6184       0.0     0.0       0.0      0.0     0.0      158.0        379.0   \n",
       "6185    6557.0  6303.0    5949.0   1705.0  7026.0  1046975.0    2667829.0   \n",
       "\n",
       "        Depth  Vocabulary  Understandability   BodySize  Dexterity  \\\n",
       "0        18.0        28.0             1333.0      120.0        3.0   \n",
       "1         6.0         9.0              166.0        6.0        2.0   \n",
       "2        17.0        41.0             2848.0      290.0        2.0   \n",
       "3         5.0        20.0              316.0        6.0        2.0   \n",
       "4        17.0       235.0            33297.0     2679.0        2.0   \n",
       "...       ...         ...                ...        ...        ...   \n",
       "6181     35.0      2068.0          1864449.0    27964.0        5.0   \n",
       "6182     33.0      2067.0          1864448.0    27964.0        3.0   \n",
       "6183      8.0        15.0              371.0       38.0        2.0   \n",
       "6184     12.0        52.0             3974.0      473.0        3.0   \n",
       "6185  63781.0    268906.0         37554046.0  3278108.0    17017.0   \n",
       "\n",
       "      NonWhiteCharacters  AllTestMethods  \n",
       "0                  952.0             0.0  \n",
       "1                  231.0             0.0  \n",
       "2                 1604.0             6.0  \n",
       "3                  342.0             0.0  \n",
       "4                18487.0           104.0  \n",
       "...                  ...             ...  \n",
       "6181            439388.0          1783.0  \n",
       "6182            439376.0          1783.0  \n",
       "6183               380.0             0.0  \n",
       "6184              3200.0             6.0  \n",
       "6185          22744641.0         43422.0  \n",
       "\n",
       "[6186 rows x 19 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = pd.read_csv('bad_tests.csv', sep=\",\")\n",
    "frame1['TestClassName'] = frame1.apply(lambda row: label_rename(row), axis=1)\n",
    "frame2 = pd.read_csv(CSV_MINER_PATH, sep=',')\n",
    "frame = pd.merge(frame1, frame2, on='TestClassName')\n",
    "frame.to_csv ('bad_tests_extended.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
